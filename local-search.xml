<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Swin-transformer论文阅读</title>
    <link href="/2022/04/22/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2022/04/22/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="Swin-transformer"><a href="#Swin-transformer" class="headerlink" title="Swin-transformer"></a>Swin-transformer</h1><h1 id="论文注意点"><a href="#论文注意点" class="headerlink" title="论文注意点"></a>论文注意点</h1><h2 id="Image-Classification-on-ImageNet-1K"><a href="#Image-Classification-on-ImageNet-1K" class="headerlink" title="Image Classification on ImageNet-1K"></a>Image Classification on ImageNet-1K</h2><h3 id="Regular-ImageNet-1K-training"><a href="#Regular-ImageNet-1K-training" class="headerlink" title="Regular ImageNet-1K training"></a>Regular ImageNet-1K training</h3><ul><li>AdamW [37] optimizer</li><li>300 epochs</li><li>cosine decay learning rate scheduler</li><li>20 epochs of linear warm-up</li><li>batch size of 1024</li><li>an initial learning rate of 0.001</li><li>weight decay of 0.05</li><li>most of the augmentation and regularization strategies</li></ul><h3 id="Pre-training-on-ImageNet-22K"><a href="#Pre-training-on-ImageNet-22K" class="headerlink" title="Pre-training on ImageNet-22K"></a>Pre-training on ImageNet-22K</h3><ul><li>pre-trainonthelarger ImageNet-22Kdataset</li><li>AdamW optimizer for <strong>90 epochs</strong> using a linear decay learning rate scheduler with a 5-epoch linear warm-up</li><li>A batch size of 4096</li><li>initial learning rate of <strong>0.001</strong></li><li>aweight decayof<strong>0.01</strong></li><li>we train the models for <strong>30 epochs</strong> with a batch size of <strong>1024</strong>,a constant learning rate of 10−5,and a weight decay of <strong>10−8</strong>.</li></ul><h1 id="swin-transformer视频"><a href="#swin-transformer视频" class="headerlink" title="swin-transformer视频"></a>swin-transformer视频</h1><p><a href="https://www.bilibili.com/video/BV15r4y1W7RY">https://www.bilibili.com/video/BV15r4y1W7RY</a></p><p><img src="/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.assets/image-20220324201353129.png" alt="image-20220324201353129"></p><p><img src="/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.assets/image-20220324201631131.png" alt="image-20220324201631131"></p><h1 id="3-31："><a href="#3-31：" class="headerlink" title="3.31："></a>3.31：</h1><p>换成tiny</p><p><img src="/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.assets/image-20220331234829685.png" alt="image-20220331234829685"></p><p>不对的keys少了些</p><p>small：</p><p>不对的key很多</p><p><img src="/Swin-transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.assets/image-20220331235000295.png" alt="image-20220331235000295"></p><p>换成2元钱的不会outofmemory了</p><p>而且好快</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>paper阅读记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>跟李沐学AI</title>
    <link href="/2022/04/22/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI/"/>
    <url>/2022/04/22/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI/</url>
    
    <content type="html"><![CDATA[<h1 id="Dive-Into-DL"><a href="#Dive-Into-DL" class="headerlink" title="Dive Into DL"></a>Dive Into DL</h1><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><h2 id="1-1-日常生活中的机器学习"><a href="#1-1-日常生活中的机器学习" class="headerlink" title="1.1. 日常生活中的机器学习"></a>1.1. 日常生活中的机器学习</h2><h2 id="1-2-关键组件"><a href="#1-2-关键组件" class="headerlink" title="1.2. 关键组件"></a>1.2. 关键组件</h2><h3 id="1-2-1-数据"><a href="#1-2-1-数据" class="headerlink" title="1.2.1. 数据"></a>1.2.1. 数据</h3><p>每个数据集由一个个<em><strong>样本</strong></em>（example, sample）组成</p><p>通常每个样本由一组称为<em><strong>特征</strong></em>（features，或<em>协变量</em>（covariates））的属性组成</p><p>要预测的是一个特殊的属性，它被称为<em><strong>标签</strong></em></p><h1 id="2-1-数据操作"><a href="#2-1-数据操作" class="headerlink" title="2.1. 数据操作"></a>2.1. 数据操作</h1><p>（1）获取数据；（2）将数据读入计算机后对其进行处理</p><p> <strong>n维数组</strong>，也称为<em><strong>张量</strong></em>（tensor）</p><p>与Numpy的<code>ndarray</code>类似</p><ul><li>GPU很好地支持加速计算，而NumPy仅支持CPU计算</li><li>张量类支持自动微分</li><li>tensor比ndarray更适合深度学习</li></ul><h2 id="2-1-1-入门"><a href="#2-1-1-入门" class="headerlink" title="2.1.1. 入门"></a>2.1.1. 入门</h2><p>导入<code>torch</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br></code></pre></td></tr></table></figure><p><strong>Tensor张量</strong>表示由一个数值组成的<strong>数组</strong></p><p><code>Tensor</code>提供<strong>GPU计算和自动求梯度</strong>等更多功能</p><blockquote><p>A<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>torch.Tensor</code></a>是包含单一数据类型元素的<strong>多维矩阵</strong></p></blockquote><p>这个数组可能有<strong>多个维度</strong></p><blockquote><p>具有<strong>一个轴的张量</strong>对应数学上的***&#x3D;&#x3D;向量&#x3D;&#x3D;***</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/ee2ab83add03b31bb076140fb6780c38.svg" alt="img"></p><p>具有<strong>两个轴的张量</strong>对应数学上的***&#x3D;&#x3D;矩阵&#x3D;&#x3D;***</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/a25495da6d0d038829fed794c8378d2d.svg" alt="img"></p><p>具有<strong>两个轴以上的张量</strong>没有特殊的数学名称</p></blockquote><h3 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h3><p>x&#x3D;torch.empty(4,2) 未初始化</p><p>x&#x3D;torch.rand(3,1)</p><p>x&#x3D;torch.zeros(5,3,dtype&#x3D;torch.int)</p><p>x&#x3D;torch.tensor([5,3])</p><p>x&#x3D;x.new_ones(5,3,dtype&#x3D;torch.float64)</p><h3 id="创建行向量"><a href="#创建行向量" class="headerlink" title="创建行向量"></a>创建行向量</h3><p>使用 <code>arange</code> 创建一个行向量 <code>x</code></p><h5 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h5><ul><li><p><strong>start</strong> (<em>Number</em>) – the <strong>starting value</strong> for the set of points. Default: <code>0</code>.</p></li><li><p><strong>end</strong> (<em>Number</em>) – the <strong>ending value</strong> for the set of points</p></li><li><p><strong>step</strong> (<em>Number</em>) – the <strong>gap</strong> between each pair of adjacent points. Default: <code>1</code>.</p></li><li><blockquote><p>torch.arange(5) 0-4<br>torch.arange(1, 4) 1-3<br>torch.arange(1, 2.5, 0.5) 1-2.5</p></blockquote></li></ul><p>除非额外指定，<strong>新的张量将存储在内存中</strong>，并采用基于CPU的计算</p><h3 id="访问张量形状（一条轴上个数）"><a href="#访问张量形状（一条轴上个数）" class="headerlink" title="访问张量形状（一条轴上个数）"></a>访问张量形状（一条轴上个数）</h3><p>通过张量的<code>shape</code>属性来访问张量（沿每个轴的长度）的<em>形状</em> </p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">x</span> <span class="hljs-operator">=</span> torch.arange(<span class="hljs-number">12</span>)<br>x<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x.shape<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220131145533639.png" alt="image-20220131145533639"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(k.shape[<span class="hljs-number">0</span>])        <span class="hljs-comment"># shape[0]输出3，为矩阵的行数</span><br><span class="hljs-built_in">print</span>(k.shape[<span class="hljs-number">1</span>])        <span class="hljs-comment"># 同理shape[1]输出列数</span><br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;shape[0]:行数&#x3D;&#x3D;</p><p>&#x3D;&#x3D;shape[1]:列数&#x3D;&#x3D;</p><h3 id="张量中元素的总数（所有轴上个数）"><a href="#张量中元素的总数（所有轴上个数）" class="headerlink" title="张量中元素的总数（所有轴上个数）"></a>张量中元素的总数（所有轴上个数）</h3><p>因为这里在处理的是一个<strong>向量</strong>，所以它的<code>shape</code>与它的<code>size/numel</code>相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x.numel()<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220131145851232.png" alt="image-20220131145851232"></p><h3 id="改变一个张量的形状"><a href="#改变一个张量的形状" class="headerlink" title="改变一个张量的形状"></a>改变一个张量的形状</h3><p>改变一个张量的<strong>形状</strong>而不改变元素<strong>数量和元素值</strong></p><h4 id="用view-来改变Tensor的形状"><a href="#用view-来改变Tensor的形状" class="headerlink" title="用view()来改变Tensor的形状"></a>用<code>view()</code>来改变<code>Tensor</code>的形状</h4><p>y &#x3D; x.view(15)</p><p>z &#x3D; x.view(-1, 5) </p><p>【<strong>view仅仅是改变了对这个张量的观察角度，内部数据并未改变</strong>】</p><h4 id="先clone后view"><a href="#先clone后view" class="headerlink" title="先clone后view"></a>先clone后view</h4><p>先用<code>clone</code>创造一个副本然后再使用<code>view</code></p><blockquote><p>使用<code>clone</code>还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源<code>Tensor</code>。？？？？？？？</p></blockquote><h4 id="调用reshape函数"><a href="#调用reshape函数" class="headerlink" title="调用reshape函数"></a>调用<code>reshape</code>函数</h4><ul><li>把张量<code>x</code>从形状为（12,）的行向量转换为形状为（3,4）的矩阵</li></ul><blockquote><p>注意，通过改变张量的形状，张量的大小不会改变。（数量没变）</p></blockquote><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220131150140026.png" alt="image-20220131150140026"></p><ul><li>把1x4变成2x2矩阵</li></ul><h4 id="还可以通过-1来调用此自动计算出维度"><a href="#还可以通过-1来调用此自动计算出维度" class="headerlink" title="还可以通过-1来调用此自动计算出维度"></a>还可以通过<code>-1</code>来调用此自动计算出维度</h4><p>用<code>x.reshape(-1,4)</code>或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code></p><h3 id="使用全0、全1、其他常量"><a href="#使用全0、全1、其他常量" class="headerlink" title="使用全0、全1、其他常量"></a>使用全0、全1、其他常量</h3><h4 id="zero函数"><a href="#zero函数" class="headerlink" title="zero函数"></a>zero函数</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><h4 id="ones函数"><a href="#ones函数" class="headerlink" title="ones函数"></a>ones函数</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><h3 id="创建一个形状为（2-3-4）的张量"><a href="#创建一个形状为（2-3-4）的张量" class="headerlink" title="创建一个形状为（2,3,4）的张量"></a>创建一个形状为（2,3,4）的张量</h3><p>torch.ones&#x2F;zeros((x,x,x))   <strong>两个括号</strong></p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220131154920248.png" alt="image-20220131154920248"></p><h3 id="从某个特定的概率分布中随机采样来得到张量中每个元素的值"><a href="#从某个特定的概率分布中随机采样来得到张量中每个元素的值" class="headerlink" title="从某个特定的概率分布中随机采样来得到张量中每个元素的值"></a>从某个特定的概率分布中随机采样来得到张量中每个元素的值</h3><p>以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从<strong>均值为0、标准差为1的标准高斯分布</strong>（<strong>正态分布</strong>）中随机采样</p><blockquote><p>当我们构造数组来作为神经网络中的参数时，我们通常会<strong>随机初始化参数的值</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220131160420347.png" alt="image-20220131160420347"></p><p>2x4的矩阵</p><h2 id="2-1-2-运算符"><a href="#2-1-2-运算符" class="headerlink" title="2.1.2. 运算符"></a>2.1.2. 运算符</h2><p>最简单且最有用的操作是<em><strong>按元素</strong></em>（elementwise）运算</p><p>将标准标量运算符应用于<strong>数组的每个元素</strong></p><p>可以基于任何<strong>从标量到标量</strong>的函数来创建按元素函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br>y = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>x + y, x - y, x * y, x / y, x ** y  <span class="hljs-comment"># **运算符是求幂运算</span><br></code></pre></td></tr></table></figure><p>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算</p><p>可以在<strong>同一形状的任意两个张量上</strong>调用按元素操作</p><h3 id="求幂"><a href="#求幂" class="headerlink" title="求幂"></a>求幂</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">torch.<span class="hljs-built_in">exp</span>(x)<br></code></pre></td></tr></table></figure><h3 id="线性代数运算"><a href="#线性代数运算" class="headerlink" title="线性代数运算"></a>线性代数运算</h3><p>包括<strong>向量点积</strong>和<strong>矩阵乘法</strong></p><h3 id="张量连结"><a href="#张量连结" class="headerlink" title="张量连结"></a>张量连结</h3><p>可以把<strong>多个张量</strong><em>连结</em>（concatenate）在一起</p><p>把它们端对端地叠起来形成一个<strong>更大的张量</strong></p><p>只需要提供张量列表，并给出沿哪个轴连结</p><p>&#x3D;&#x3D;沿行（轴-0，形状的第一个元素）&#x3D;&#x3D;</p><p>&#x3D;&#x3D;按列（轴-1，形状的第二个元素）&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br>Y = torch.tensor([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]])<br>torch.cat((X, Y), dim=<span class="hljs-number">0</span>), torch.cat((X, Y), dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201115012265.png" alt="image-20220201115012265"></p><p>dim&#x3D;0：共有3+3&#x3D;6行，4列（不变）</p><p>dim&#x3D;-1：共有4+4&#x3D;8列，3行（不变）</p><h3 id="通过逻辑运算符构建二元张量"><a href="#通过逻辑运算符构建二元张量" class="headerlink" title="通过逻辑运算符构建二元张量"></a>通过<em>逻辑运算符</em>构建二元张量</h3><p> 以<code>X == Y</code>为例： 对于每个位置，如果<code>X</code>和<code>Y</code>在该位置相等，则新张量中相应项的值为1</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201115239453.png" alt="image-20220201115239453"></p><h3 id="元素求和"><a href="#元素求和" class="headerlink" title="元素求和"></a>元素求和</h3><p>对张量中的所有元素进行求和，会产生<strong>一个单元素张量</strong></p><ul><li><p>直接相加每个元素</p></li><li><pre><code class="python">X.sum()<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs mel"><br>  ![<span class="hljs-keyword">image</span><span class="hljs-number">-20220201115359787</span>](%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/<span class="hljs-keyword">image</span><span class="hljs-number">-20220201115359787.</span>png)<br><br>## <span class="hljs-number">2.1</span><span class="hljs-number">.3</span>. 广播机制<br><br>**形状不同**，我们仍然可以通过**触发 *广播机制***（broadcasting mechanism）来执行**按元素操作**<br><br>- 适当**复制元素**来扩展一个或两个数组<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">`python</span><br><span class="hljs-string">    a = torch.arange(3).reshape((3, 1))</span><br><span class="hljs-string">    b = torch.arange(2).reshape((1, 2))</span><br><span class="hljs-string">    a, b</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    (tensor([[0],</span><br><span class="hljs-string">             [1],</span><br><span class="hljs-string">             [2]]),</span><br><span class="hljs-string">     tensor([[0, 1]]))</span><br></code></pre></td></tr></table></figure>  - ```python  a + b  <figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>  - 矩阵`a`将复制列， 矩阵`b`将复制行<br><br>- 在转换之后，两个张量具有**相同的形状**<br><br>  - ```python<br>    tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br></code></pre></td></tr></table></figure>    </code></pre></li><li><p>对<strong>生成的数组</strong>执行按元素操作</p></li></ul><p><strong>做运算即可，会自动复制使得可以运算</strong></p><p><strong>a和b应该分别需要复制行或列！</strong></p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201123907161.png" alt="image-20220201123907161"></p><h2 id="2-1-4-索引和切片"><a href="#2-1-4-索引和切片" class="headerlink" title="2.1.4. 索引和切片"></a>2.1.4. 索引和切片</h2><h4 id="张量中的元素可以通过索引访问"><a href="#张量中的元素可以通过索引访问" class="headerlink" title="张量中的元素可以通过索引访问"></a>张量中的元素可以通过<strong>索引</strong>访问</h4><ul><li>just like 数组</li><li>&#x3D;&#x3D;第一个元素的索引是0，最后一个元素索引是-1&#x3D;&#x3D;<ul><li>与任何Python数组一样</li></ul></li><li>可以指定范围以包含<strong>第一个元素和最后一个之前</strong>的元素</li><li><strong>索引出来的结果与原数据&#x3D;&#x3D;共享内存&#x3D;&#x3D;，也即修改一个，另一个会跟着修改。</strong></li></ul><p><strong>x[-1]：</strong>最后一个元素</p><p><strong>x[1:3]：</strong>第二个到第三个元素（1、2）</p><p><strong>x[0:-1]：</strong>第一个到倒数第二个元素（0、1…）</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201120758704.png" alt="image-20220201120758704"></p><p>&#x3D;&#x3D;一个方框[]内为一个元素&#x3D;&#x3D;</p><h4 id="指定索引来将元素写入矩阵"><a href="#指定索引来将元素写入矩阵" class="headerlink" title="指定索引来将元素写入矩阵"></a>指定索引来将元素写入矩阵</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>] = <span class="hljs-number">9</span>   //x[行号，列号]=数据<br>X<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201121038926.png" alt="image-20220201121038926"></p><h4 id="想为多个元素赋值相同的值"><a href="#想为多个元素赋值相同的值" class="headerlink" title="想为多个元素赋值相同的值"></a>想为多个元素赋值相同的值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>, :] = <span class="hljs-number">12</span>//[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>, :]访问第<span class="hljs-number">1</span>行和第<span class="hljs-number">2</span>行<br>X<br></code></pre></td></tr></table></figure><ul><li>0：2—》0-1之间（2之前）</li><li>“:”代表沿轴1（列）的所有元素</li></ul><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201121413306.png" alt="image-20220201121413306"></p><h2 id="2-1-5-节省内存"><a href="#2-1-5-节省内存" class="headerlink" title="2.1.5. 节省内存"></a>2.1.5. 节省内存</h2><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs tp">before = id(<span class="hljs-keyword">Y</span>)<br><span class="hljs-keyword">Y</span> = <span class="hljs-keyword">Y</span> + <span class="hljs-keyword">X</span><br>id(<span class="hljs-keyword">Y</span>) == before<br></code></pre></td></tr></table></figure><ul><li>我们希望<strong>原地执行</strong>这些更新</li><li>如果我们不原地更新，<strong>其他引用仍然会指向旧的内存位置</strong><ul><li>某些代码可能会无意中引用旧的参数</li></ul></li></ul><h3 id="切片表示法"><a href="#切片表示法" class="headerlink" title="切片表示法"></a>切片表示法</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Z = torch<span class="hljs-selector-class">.zeros_like</span>(Y)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;id(Z):&#x27;</span>, id(Z)</span></span>)<br>Z<span class="hljs-selector-attr">[:]</span> = X + Y<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&#x27;id(Z):&#x27;</span>, id(Z)</span></span>)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;Y[:] &#x3D; <expression>&#x3D;&#x3D;</p><ul><li>使用<strong>切片表示法</strong>将操作的结果<strong>分配给先前分配的数组</strong></li><li><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201122253477.png" alt="image-20220201122253477"></li><li>两次y&#x3D;不同的表达式，但是id都相同</li></ul><h3 id="x3D-法"><a href="#x3D-法" class="headerlink" title="+&#x3D;法"></a>+&#x3D;法</h3><p>如果在后续计算中<strong>没有重复</strong>使用<code>X</code></p><p>也可以使用<code>X += Y</code>来减少操作的内存开销</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-keyword">before</span> = <span class="hljs-built_in">id</span>(X)<br>X += Y<br><span class="hljs-built_in">id</span>(X) == <span class="hljs-keyword">before</span><br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201122349833.png" alt="image-20220201122349833"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>赋值的时候表达式：y+&#x3D;x或z[:]&#x3D;x+y</p><h2 id="2-1-6-转换为其他Python对象"><a href="#2-1-6-转换为其他Python对象" class="headerlink" title="2.1.6. 转换为其他Python对象"></a>2.1.6. 转换为其他Python对象</h2><h3 id="张量tensor-数组numpy"><a href="#张量tensor-数组numpy" class="headerlink" title="张量tensor 数组numpy"></a>张量tensor 数组numpy</h3><ul><li>torch张量和numpy数组共享底层内存</li><li><strong>就地操作</strong>更改一个张量也会同时更改另一个张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">A = X.numpy()<br>B = torch.tensor(A)<br><span class="hljs-built_in">type</span>(A), <span class="hljs-built_in">type</span>(B)<br></code></pre></td></tr></table></figure><h3 id="张量tensor-标量item"><a href="#张量tensor-标量item" class="headerlink" title="张量tensor 标量item"></a>张量tensor 标量item</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([<span class="hljs-number">3.5</span>])<br>a, a.item(), <span class="hljs-built_in">float</span>(a), <span class="hljs-built_in">int</span>(a)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(tensor([<span class="hljs-number">3.5000</span>]), <span class="hljs-number">3.5</span>, <span class="hljs-number">3.5</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><ul><li>a是自己，tensor</li><li><strong>a.item</strong>是标量，数字</li></ul><h2 id="2-1-7-小结"><a href="#2-1-7-小结" class="headerlink" title="2.1.7. 小结"></a>2.1.7. 小结</h2><p>张量 ：n维数组，不管几维都是张量！</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201124310314.png" alt="image-20220201124310314"></p><p>随便赋值：torch.arange(12)   是一个行向量</p><p>指定赋值：torch.tensor([1,2,3])</p><p>一条轴上个数：x.shape()</p><p>所有轴的总元素个数：x.numel()</p><p>求所有元素值：x.sum()</p><p>连接张量：torch.cat((x,y),dim&#x3D;0&#x2F;-1) 分别沿横轴、纵轴</p><p>索引访问张量：x[1:-1] x[2] x[0:2] x[-1]</p><p>改写元素：x[2,3]&#x3D;12</p><p>为多个元素赋值：x[0:1, :] 第1行所有列</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201125756294.png" alt="image-20220201125756294"></p><p>要更新一个值，不要写新的z&#x3D;x+y或y&#x3D;x+y，写y+&#x3D;x或切片表示z[:]&#x3D;x+y，不会开辟新的内存</p><p>tensor-&gt;numpy：y&#x3D;x.numpy()</p><p>numpy-&gt;tensor：&#x3D;torch.tensor(xxx)</p><p>tensor-&gt;item：a&#x3D;torch.tensor() ; a.item()</p><h1 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2. 数据预处理"></a>2.2. 数据预处理</h1><p>在Python中常用的数据分析工具中，我们通常使用<code>pandas</code>软件包</p><p><code>pandas</code>可以与张量兼容</p><ul><li>使用<code>pandas</code>预处理原始数据</li><li>将原始数据转换为张量格式</li></ul><h2 id="2-2-1-读取数据集"><a href="#2-2-1-读取数据集" class="headerlink" title="2.2.1. 读取数据集"></a>2.2.1. 读取数据集</h2><p>创建一个人工数据集：</p><p>存储在&#x3D;&#x3D;CSV（逗号分隔值）&#x3D;&#x3D;文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">os.makedirs(os.path.join(<span class="hljs-string">&#x27;..&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>), exist_ok=<span class="hljs-literal">True</span>)<br>data_file = os.path.join(<span class="hljs-string">&#x27;..&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;house_tiny.csv&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="os-makedirs-方法"><a href="#os-makedirs-方法" class="headerlink" title="os.makedirs() 方法"></a>os.makedirs() 方法</h3><p>用于递归创建目录</p><p>文件地址：data_file &#x3D; os.path.join(‘..’, ‘data’, ‘house_tiny.csv’)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_file,<span class="hljs-string">&#x27;w&#x27;</span>)<span class="hljs-keyword">as</span> f: <span class="hljs-comment">#open操作，打开上面的csv文件并写入</span><br>    f.write(<span class="hljs-string">&#x27;NumRooms,Alley,Price\n&#x27;</span>) <span class="hljs-comment">#写入第一行</span><br>    f.write(<span class="hljs-string">&#x27;NA,PAVE,127500\n&#x27;</span>)<br>    f.write(<span class="hljs-string">&#x27;2,NA,106000\n&#x27;</span>)<br>    f.write(<span class="hljs-string">&#x27;4,NA,178100\n&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data=pd.read_csv(data_file) <span class="hljs-comment">#使用pandas包中的readcsv函数读取这个文件</span><br><span class="hljs-built_in">print</span>(data) <span class="hljs-comment">#打印所有写入的数据</span><br></code></pre></td></tr></table></figure><h3 id="读取写入的csv文件"><a href="#读取写入的csv文件" class="headerlink" title="读取写入的csv文件"></a>读取写入的csv文件</h3><p>pandas.read_csv(地址)</p><h2 id="分割输入输出"><a href="#分割输入输出" class="headerlink" title="分割输入输出"></a>分割输入输出</h2><h3 id="位置索引iloc"><a href="#位置索引iloc" class="headerlink" title="位置索引iloc"></a>位置索引<code>iloc</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#分割出输入输出</span><br>inputs,outputs=data.iloc[:,<span class="hljs-number">0</span>:<span class="hljs-number">2</span>],data.iloc[:,-<span class="hljs-number">1</span>] <span class="hljs-comment">#inputs：所有行，0-1列，outputs：所有行，最后一行</span><br><br><span class="hljs-comment">#处理缺失值</span><br>inputs=inputs.fillna(inputs.mean()) <span class="hljs-comment">#fillna函数填充缺失值，mean函数求平均</span><br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201151237761.png" alt="image-20220201151237761"></p><h2 id="2-2-2-处理缺失值"><a href="#2-2-2-处理缺失值" class="headerlink" title="2.2.2. 处理缺失值"></a>2.2.2. 处理缺失值</h2><h3 id="插值法"><a href="#插值法" class="headerlink" title="插值法"></a>插值法</h3><p>用一个替代值弥补缺失值</p><h4 id="fillna函数"><a href="#fillna函数" class="headerlink" title="fillna函数"></a>fillna函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs,outputs=data.iloc[:,<span class="hljs-number">0</span>:<span class="hljs-number">2</span>],data.iloc[:,-<span class="hljs-number">1</span>] <span class="hljs-comment">#inputs：所有行，0-1列，outputs：所有行，最后一行</span><br><span class="hljs-comment">#处理缺失值</span><br>inputs=inputs.fillna(inputs.mean()) <span class="hljs-comment">#fillna函数填充缺失值，mean函数求平均</span><br><br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201151424300.png" alt="image-20220201151424300"></p><h4 id="对alley中NaN的处理"><a href="#对alley中NaN的处理" class="headerlink" title="对alley中NaN的处理"></a>对alley中NaN的处理</h4><p>对于<code>inputs</code>中的<strong>类别值或离散值</strong>，我们将“NaN”视为一个类别</p><h5 id="get-dummies函数"><a href="#get-dummies函数" class="headerlink" title="get_dummies函数"></a>get_dummies函数</h5><p> <code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">pd.get_dummies(s1, dummy_na=<span class="hljs-literal">True</span>)<br>   a  b  NaN<br><span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>    <span class="hljs-number">0</span><br><span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>    <span class="hljs-number">0</span><br><span class="hljs-number">2</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>    <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201152109456.png" alt="image-20220201152109456"></p><h3 id="删除法"><a href="#删除法" class="headerlink" title="删除法"></a>删除法</h3><p>直接<strong>忽略</strong>缺失值</p><h2 id="2-2-3-转换为张量格式"><a href="#2-2-3-转换为张量格式" class="headerlink" title="2.2.3. 转换为张量格式"></a>2.2.3. 转换为张量格式</h2><p>👆以上inputs、outputs都是数值类型</p><p>可以转换为<strong>张量格式</strong></p><p><strong>&#x3D;&#x3D;&#x3D;torch.tensor(x.values)&#x3D;&#x3D;</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#将输入输出由数值转换成张量</span><br><span class="hljs-keyword">import</span> torch<br>x,y=torch.tensor(inputs.values),torch.tensor(outputs.values)<br></code></pre></td></tr></table></figure><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220201152552575.png" alt="image-20220201152552575"></p><p>原数据按照格式变成矩阵（张量）</p><p>输入是3x3</p><p>输出是1x3</p><h2 id="2-2-4-小结"><a href="#2-2-4-小结" class="headerlink" title="2.2.4. 小结"></a>2.2.4. 小结</h2><p><code>pandas</code>可以与张量兼容</p><ul><li>自创数据集：<ul><li>import os</li><li><strong>os.makedirs</strong>(<strong>os.path.join</strong>(‘..’, ‘data’), exist_ok&#x3D;True)</li></ul></li><li>文件地址<ul><li>data_file &#x3D; <strong>os.path.join</strong>(‘..’, ‘data’, ‘house_tiny.csv’)   分级创建</li></ul></li><li>打开文件，准备写入<ul><li>with <strong>open</strong>(data_file,’w’)as f: #open操作，打开上面的csv文件并写入</li></ul></li><li>写入<ul><li>f.<strong>write</strong>(‘NumRooms,Alley,Price\n’) #写入第一行</li></ul></li><li>读取csv<ul><li>import <strong>pandas</strong> as pd</li><li>data&#x3D;<strong>pd.read_csv</strong>(data_file)&#x2F;&#x2F;参数：文件地址</li></ul></li><li>分割输入输出<ul><li>input&#x3D;data.<strong>iloc[:,0:-1]</strong></li><li>outputs&#x3D;data.<strong>iloc[:,-1]</strong></li></ul></li><li>求列的平均值<ul><li>inputs**.mean()**</li></ul></li><li>替换NaN<ul><li>inputs.<strong>fllna(替代值)</strong></li></ul></li><li>将csv数值转换成张量<ul><li>import <strong>torch</strong></li><li>inputs&#x3D;<strong>torch.tensor</strong>(inputs.values)<ul><li>不要忘了values！</li></ul></li></ul></li></ul><h1 id="2-3-线性代数"><a href="#2-3-线性代数" class="headerlink" title="2.3. 线性代数"></a>2.3. 线性代数</h1><table><thead><tr><th>函数</th><th>功能</th></tr></thead><tbody><tr><td>trace</td><td><strong>对角线元素之和</strong>(矩阵的迹)</td></tr><tr><td>diag</td><td><strong>对角线元素</strong></td></tr><tr><td>triu&#x2F;tril</td><td>矩阵的<strong>上三角&#x2F;下三角</strong>，可指定偏移量</td></tr><tr><td>mm&#x2F;bmm</td><td>矩阵<strong>乘法</strong>，batch的矩阵乘法</td></tr><tr><td>addmm&#x2F;addbmm&#x2F;addmv&#x2F;addr&#x2F;baddbmm..</td><td>矩阵运算</td></tr><tr><td>t</td><td><strong>转置</strong></td></tr><tr><td>dot&#x2F;cross</td><td><strong>内积&#x2F;外积</strong></td></tr><tr><td>inverse</td><td>求<strong>逆矩阵</strong></td></tr><tr><td>svd</td><td>奇异值分解</td></tr></tbody></table><h1 id="2-3-自动求梯度"><a href="#2-3-自动求梯度" class="headerlink" title="2.3 自动求梯度"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter02_prerequisite/2.3_autograd?id=_23-%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6">2.3 自动求梯度</a></h1><blockquote><p><a href="https://pytorch.org/docs/stable/autograd.html">autograd</a>包能够根据<strong>输入</strong>和<strong>前向传播</strong>过程&#x3D;&#x3D;自动构建计算图&#x3D;&#x3D;，并执行&#x3D;&#x3D;反向传播&#x3D;&#x3D;</p></blockquote><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title=".requires_grad"></a>.requires_grad</h3><p>追踪(track)在Tensor上的所有操作</p><ul><li>将其属性<code>.requires_grad</code>设置为<code>True</code><ul><li><strong>a.requires_grad_(True)</strong></li><li><strong>x&#x3D;torch.ones(2,2,requires_grad&#x3D;True)</strong></li></ul></li><li>a &#x3D; torch.randn(2, 2) <strong>#</strong> <strong>缺失情况下默认 requires_grad &#x3D; False</strong></li></ul><h3 id="梯度计算-backward"><a href="#梯度计算-backward" class="headerlink" title="梯度计算.backward()"></a>梯度计算.backward()</h3><ul><li><p>.backward()</p></li><li><p>标量：</p><ul><li><p>计算梯度不需要指定对谁求导（不需要传参）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">out.backward() <span class="hljs-comment"># 等价于 out.backward(torch.tensor(1.))</span><br></code></pre></td></tr></table></figure></li><li><p><code>out</code>关于<code>x</code>的梯度<strong>d(out)&#x2F;dx</strong></p></li></ul></li><li><p>否则，需要传入一个与<code>out</code>同形的<code>Tensor</code></p></li><li><p>此<code>Tensor</code>的梯度将累积到<code>.grad</code>属性中</p><ul><li>grad在反向传播过程中是<strong>累加的</strong>(accumulated)</li><li>每一次运行反向传播，梯度都会<strong>累加之前的梯度</strong></li><li>所以一般<strong>在反向传播之前需把梯度清零</strong></li></ul></li></ul><h3 id="不想计算梯度"><a href="#不想计算梯度" class="headerlink" title="不想计算梯度"></a>不想计算梯度</h3><ul><li>with torch.no_grad()</li><li>评估模型的时候很常用（val、test）<ul><li>因为并不需要计算<strong>可训练参数</strong>（<code>requires_grad=True</code>）<strong>的梯度</strong></li></ul></li></ul><h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><blockquote><p><code>Tensor</code>和<code>Function</code>互相结合：</p><ul><li>构建一个<strong>记录有整个计算过程</strong>的&#x3D;&#x3D;有向无环图（DAG）&#x3D;&#x3D;？？？？</li></ul></blockquote><p>每个<code>Tensor</code>都有一个<code>.grad_fn</code><strong>属性</strong></p><ul><li><p>该<code>Tensor</code>是不是通过某些运算得到的</p></li><li><p>是：</p><ul><li><p><code>grad_fn</code>返回一个与这些运算相关的对象</p></li><li><p>&#96;&#96;&#96;python<br>y &#x3D; x + 2<br>print(y)<br>print(y.grad_fn)&#x2F;&#x2F;grad_fn&#x3D;<AddBackward>)<br>&lt;AddBackward object at 0x1100477b8&gt;</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><br>- 否：<br><br>  - 返回<span class="hljs-attribute">none</span><br><br>  - ```python<br>    x = torch<span class="hljs-selector-class">.ones</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, requires_grad=True)<br>    <span class="hljs-built_in">print</span>(x)<br>    <span class="hljs-built_in">print</span>(x.grad_fn)<span class="hljs-comment">//none</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h2 id="雅可比矩阵"><a href="#雅可比矩阵" class="headerlink" title="雅可比矩阵"></a>雅可比矩阵</h2><p>函数值y和自变量x都为向量</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220207162814512.png" alt="image-20220207162814512"></p><p>【<strong>y关于x的梯度</strong>：雅阁比矩阵】</p><h3 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h3><blockquote><p>计算<strong>雅克比矩阵的乘积</strong></p></blockquote><h1 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1. 线性回归"></a>3.1. 线性回归</h1><p>回归经常用来表示输入和输出之间的关系</p><ul><li>1……N个自变量</li><li>因变量</li></ul><p>机器学习中大多数任务通常都与<em>预测</em>（prediction）有关</p><h3 id="预测类问题"><a href="#预测类问题" class="headerlink" title="预测类问题"></a>预测类问题</h3><ul><li>预测价格（房屋、股票等）</li><li>预测住院时间（针对住院病人等）</li><li>预测需求（零售销量等）</li></ul><h3 id="预测类常用办法："><a href="#预测类常用办法：" class="headerlink" title="预测类常用办法："></a>预测类常用办法：</h3><ul><li>回归问题<ul><li>线性回归：最简单而且最流行</li></ul></li><li>分类问题<ul><li>分类问题的目标是<strong>预测</strong>数据<strong>属于一组类别中的哪一个</strong></li></ul></li></ul><h2 id="3-1-1-线性回归"><a href="#3-1-1-线性回归" class="headerlink" title="3.1.1. 线性回归"></a>3.1.1. 线性回归</h2><ul><li>自变量xx和因变量yy之间的关系是线性的</li><li>yy可以表示为xx中元素的加权和</li><li>允许包含观测值(已知的x和y??)的一些噪声<ul><li>假设任何噪声都比较正常，如噪声遵循正态分布</li></ul></li></ul><h2 id="预测房价的模型"><a href="#预测房价的模型" class="headerlink" title="预测房价的模型"></a>预测房价的模型</h2><h4 id="1-训练（数据）集"><a href="#1-训练（数据）集" class="headerlink" title="1 训练（数据）集"></a>1 训练（数据）集</h4><p>数据集包括了房屋的<strong>销售价格、面积和房龄</strong></p><h4 id="2-样本-数据点-数据样本"><a href="#2-样本-数据点-数据样本" class="headerlink" title="2 样本  数据点  数据样本"></a>2 样本  数据点  数据样本</h4><p><strong>每行数据</strong>（比如一次房屋交易相对应的数据）称为<em>样本</em></p><h4 id="3-特征-x2F-协变量-x3D-自变量"><a href="#3-特征-x2F-协变量-x3D-自变量" class="headerlink" title="3 特征&#x2F;协变量&#x3D;自变量"></a>3 特征&#x2F;协变量&#x3D;自变量</h4><p>预测所依据的<strong>自变量</strong>（面积和房龄）</p><p>用来预测标签的多个因素叫作特征</p><p>特征用来表征样本的特点</p><h4 id="4-标签-目标-x3D-预测值"><a href="#4-标签-目标-x3D-预测值" class="headerlink" title="4 标签 目标&#x3D;预测值"></a>4 标签 目标&#x3D;预测值</h4><p>试图预测的<strong>目标</strong>（比如预测房屋价格）</p><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><blockquote><p>假设目标（房屋价格）可以表示为特征（面积和房龄）的<strong>加权和</strong></p></blockquote><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220207172206110.png" alt="image-20220207172206110"></p><h4 id="权重："><a href="#权重：" class="headerlink" title="权重："></a><strong>权重：</strong></h4><p>决定了<strong>每个特征对我们预测值的影响</strong></p><h4 id="偏置（bias）："><a href="#偏置（bias）：" class="headerlink" title="偏置（bias）："></a><strong>偏置（bias）：</strong></h4><p>当所有特征都取值为0时，预测值应该为多少</p><blockquote><p>如果没有偏置项，我们模型的表达能力将受到限制</p></blockquote><p>给定一个数据集，我们的目标是<strong>寻找模型的权重w 和偏置b</strong> </p><ul><li><p>在机器学习领域，我们通常使用的是<strong>高维数据集</strong></p><ul><li><strong>X∈Rn×d：整个数据集的 n个样本</strong><ul><li>&#x3D;&#x3D;X的每一行&#x3D;&#x3D;：一个样本（一次房屋交易相对应的数据）</li><li>&#x3D;&#x3D;X的每一列&#x3D;&#x3D;：一种特征（面积和房龄）</li></ul></li></ul></li><li><p>建模时采用<strong>线性代数表示法</strong>会比较方便</p></li><li><p>输入包含d个特征时：</p><ul><li><p>预测结果y：<img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220209164247749.png" alt="image-20220209164247749"></p></li><li><p>不同的特征（自变量）分配不同的权重，相乘</p></li></ul></li><li><p>可以用<strong>点积形式</strong>来简洁地表达模型：<img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220209164343783.png" alt="image-20220209164343783"></p></li><li><p>无论我们使用什么手段来观察特征 X和标签 y， 都可能会出现少量的观测误差。</p><ul><li>会加入一个<strong>噪声项</strong>来考虑观测误差带来的影响</li></ul></li></ul><h3 id="3-1-1-2-损失函数"><a href="#3-1-1-2-损失函数" class="headerlink" title="3.1.1.2. 损失函数"></a>3.1.1.2. 损失函数</h3><blockquote><p>拟合程度的度量</p></blockquote><p>通常我们会选取一个<strong>非负数</strong>&gt;&#x3D;0作为误差</p><p>数值越小表示误差越小</p><p><strong>回归问题</strong>中最常用的损失函数是<strong>平方误差函数</strong>（平方损失（square loss））</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220209165545414.png" alt="image-20220209165545414"></p><ul><li>常数 1&#x2F;2 使对平方项<strong>求导后的常数系数为1</strong></li><li>形式上稍微简单一些</li></ul><p>👆每个样本训练都有一个误差</p><p>👇</p><p>用训练数据集中<strong>所有样本误差的平均</strong>来衡量模型预测的质量</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220209170001796.png" alt="image-20220209170001796"></p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">优化算法</a></h3><blockquote><p>无法得到解析解的情况下</p><p>只能<strong>通过优化算法</strong>有限次<strong>迭代模型参数</strong>来尽可能<strong>降低损失函数的值</strong>。这类解叫作数值解</p></blockquote><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><ul><li><p>几乎可以优化所有深度学习模型</p></li><li><p>不断地<strong>在损失函数递减的方向</strong>上<strong>更新参数</strong>来降低误差</p></li><li><p>用法：</p><ul><li><p>计算<strong>损失函数</strong>（数据集中<u>所有样本的损失均值</u>） 关于模型<strong>参数</strong>的导数—&gt;【梯度】</p><ul><li><u>所有样本</u>：导致执行可能会非常慢，在每一次更新参数之前，我们必须<strong>遍历整个数据集</strong>（计算所有样本的loss）</li></ul></li></ul></li></ul><p>👇<em>每次需要计算更新的时候随机抽取一小批样本：</em></p><h4 id="小批量随机梯度下降（mini-batch-stochastic-gradient-descent）"><a href="#小批量随机梯度下降（mini-batch-stochastic-gradient-descent）" class="headerlink" title="小批量随机梯度下降（mini-batch stochastic gradient descent）"></a>小批量随机梯度下降（mini-batch stochastic gradient descent）</h4><p>由固定数量的训练样本组成</p><p>对参数进行多次迭代，使<strong>每次迭代都可能降低损失函数的值</strong></p><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><blockquote><p>人为设定的，并不是通过模型训练学出的</p></blockquote><p>批量大小，batch size</p><p>学习率（learning rate）</p><h4 id="“调参”"><a href="#“调参”" class="headerlink" title="“调参”"></a>“调参”</h4><p>调节超参数👆</p><p>反复试错来找到超参数合适的值</p><ul><li>少数情况下，超参数也可以通过模型训练学出</li></ul><h3 id="3-1-1-2-模型训练"><a href="#3-1-1-2-模型训练" class="headerlink" title="3.1.1.2 模型训练"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3112-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">3.1.1.2 模型训练</a></h3><p>模型训练：</p><ul><li><p>通过数据来寻找特定的模型参数值</p></li><li><p>模型在数据上的误差尽可能小</p></li></ul><h3 id="3-1-1-3-模型预测-模型测试"><a href="#3-1-1-3-模型预测-模型测试" class="headerlink" title="3.1.1.3 模型预测 模型测试"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3113-%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B">3.1.1.3 模型预测 </a>模型测试</h3><p><strong>优化算法停止时的值</strong>–》新的w、b</p><p>用此wb计算训练集以外任一样本的目标值</p><h2 id="3-1-2-线性回归与神经网络的联系"><a href="#3-1-2-线性回归与神经网络的联系" class="headerlink" title="3.1.2 线性回归与神经网络的联系"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_312-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95">3.1.2 </a>线性回归与神经网络的联系</h2><h4 id="3-1-2-1-神经网络图"><a href="#3-1-2-1-神经网络图" class="headerlink" title="3.1.2.1 神经网络图"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3121-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE">3.1.2.1 神经网络图</a></h4><p>使用神经网络图直观地<strong>表现模型结构</strong></p><h5 id="线性回归是一个单层神经网络"><a href="#线性回归是一个单层神经网络" class="headerlink" title="线性回归是一个单层神经网络"></a>线性回归是一个单层神经网络</h5><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter03/3.1_linreg.svg" alt="img"></p><ul><li>神经网络图<strong>隐去了模型参数权重w和偏差b</strong></li><li>输入：<ul><li>特征、x</li></ul></li><li>输出<ul><li>预测值、y</li></ul></li><li>o的计算<strong>依赖于</strong> x1和 x2<ul><li>o和每个输入<strong>完全连接</strong></li><li>【输出层：&#x3D;&#x3D;全连接层&#x3D;&#x3D;或稠密层】</li></ul></li></ul><h4 id="3-1-2-2-矢量计算表达式（向量计算）"><a href="#3-1-2-2-矢量计算表达式（向量计算）" class="headerlink" title="3.1.2.2 矢量计算表达式（向量计算）"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3122-%E7%9F%A2%E9%87%8F%E8%AE%A1%E7%AE%97%E8%A1%A8%E8%BE%BE%E5%BC%8F">3.1.2.2 矢量计算表达式</a>（向量计算）</h4><p><strong>向量相加</strong></p><ol><li><p>按元素逐一做标量加法</p><ol><li><p>&#96;&#96;&#96;python<br>import torch<br>from time import time</p><p>a &#x3D; torch.ones(1000)<br>b &#x3D; torch.ones(1000)</p><p>start &#x3D; time()<br>c &#x3D; torch.zeros(1000)<br>for i in range(1000):<br>c[i] &#x3D; a[i] + b[i]<br>print(time() - start)</p><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs isbl"><br><span class="hljs-number">2</span>. 直接做矢量加法<br><br>   <span class="hljs-number">1</span>. ```<span class="hljs-variable">python</span><br>      <span class="hljs-variable">start</span> = <span class="hljs-function"><span class="hljs-title"><span class="hljs-built_in">time</span></span>()</span><br>      <span class="hljs-variable">d</span> = <span class="hljs-variable">a</span> + <span class="hljs-variable">b</span><br>      <span class="hljs-function"><span class="hljs-title">print</span>(<span class="hljs-title"><span class="hljs-built_in">time</span></span>() - <span class="hljs-variable">start</span>)</span><br></code></pre></td></tr></table></figure></li></ol></li></ol><p><strong>后者比前者更省时</strong>—&gt;尽可能采用矢量计算</p><h1 id="3-4-softmax回归"><a href="#3-4-softmax回归" class="headerlink" title="3.4. softmax回归"></a>3.4. softmax回归</h1><blockquote><p>softmax回归也是一个<strong>单层神经网络</strong></p><p>softmax回归的<strong>输出层也是全连接层</strong></p></blockquote><h2 id="3-4-1-分类问题"><a href="#3-4-1-分类问题" class="headerlink" title="3.4.1. 分类问题"></a>3.4.1. 分类问题</h2><p>2x2灰度图像：2x2&#x3D;4，即&#x3D;&#x3D;4个特征&#x3D;&#x3D;</p><h3 id="表示分类数据的简单方法：独热编码（one-hot-encoding）"><a href="#表示分类数据的简单方法：独热编码（one-hot-encoding）" class="headerlink" title="表示分类数据的简单方法：独热编码（one-hot encoding）"></a>表示分类数据的简单方法：<em>独热编码</em>（one-hot encoding）</h3><ul><li>是一个<strong>向量</strong></li><li><strong>分量和类别一样多</strong></li><li>分别对应的分量设置为1</li></ul><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220302233039947.png" alt="image-20220302233039947"></p><ul><li>&#x3D;&#x3D;三维向量（x,y,z）&#x3D;&#x3D;<ul><li>每一维表示一个种类</li></ul></li><li>3个可能的输出类别</li></ul><blockquote><p>一共4个特征+3个可能的输出类别</p></blockquote><h2 id="3-4-2-网络架构"><a href="#3-4-2-网络架构" class="headerlink" title="3.4.2. 网络架构"></a>3.4.2. 网络架构</h2><ul><li><p>估计<strong>所有可能类别</strong>的条件概率</p></li><li><p>需要一个有<strong>多个输出</strong>的模型</p></li><li><p>每个类别对应一个输出—》3个输出o1 o2和o3</p><ul><li><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220302233621062.png" alt="image-20220302233621062"></p></li><li><p>12个权重w，分别对每个输出o和每个特征x起作用</p></li></ul></li></ul><h2 id="3-4-3-全连接层的参数开销"><a href="#3-4-3-全连接层的参数开销" class="headerlink" title="3.4.3. 全连接层的参数开销"></a>3.4.3. 全连接层的参数开销</h2><p>有d 个输入和q 个输出的全连接层</p><p>参数开销为<img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220302234005970.png" alt="image-20220302234005970"></p><h3 id="平衡参数节约和模型有效性："><a href="#平衡参数节约和模型有效性：" class="headerlink" title="平衡参数节约和模型有效性："></a>平衡参数节约和模型有效性：</h3><p>将d 个输入转换为q 个输出的成本可以减少到O(dq&#x2F;n)   （参数变少？？）</p><ul><li>超参数nn可以由我们灵活指定</li></ul><h2 id="3-4-4-softmax运算"><a href="#3-4-4-softmax运算" class="headerlink" title="3.4.4. softmax运算"></a>3.4.4. softmax运算</h2><blockquote><p>每个<strong>求幂</strong>后的结果除以它们的<strong>总和</strong></p></blockquote><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220303093030092.png" alt="image-20220303093030092"></p><p>softmax运算<strong>不会改变未规范化的预测o 之间的顺序</strong>，只会确定<strong>分配给每个类别的概率</strong></p><p>在预测过程中，我们仍然可以用下式来选择最有可能的类别:</p><p><img src="/%E8%B7%9F%E6%9D%8E%E6%B2%90%E5%AD%A6AI.assets/image-20220303093127549.png" alt="image-20220303093127549"></p><ul><li>softmax回归是一个<em>线性模型</em><ul><li>softmax回归的<strong>输出</strong>仍然由<strong>输入特征的仿射变换</strong>决定</li><li>仿射变换是<strong>线性的</strong></li></ul></li><li>但softmax是一个非线性函数</li></ul><h2 id="3-4-5-小批量样本的矢量化"><a href="#3-4-5-小批量样本的矢量化" class="headerlink" title="3.4.5. 小批量样本的矢量化"></a>3.4.5. 小批量样本的矢量化</h2><p>小批量样本的矢量化<strong>加快了X和WX和W的矩阵-向量乘法</strong></p><p>提高计算效率并且充分利用GPU</p><h2 id="3-4-6-损失函数"><a href="#3-4-6-损失函数" class="headerlink" title="3.4.6. 损失函数"></a>3.4.6. 损失函数</h2><p>度量预测的效果</p><h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>对给定任意输入xx的每个类的条件概率</p><p>数据集中可能存在<strong>标签噪声</strong>（比如某些样本可能被误标）</p><h1 id="3-5-图像分类数据集（Fashion-MNIST）"><a href="#3-5-图像分类数据集（Fashion-MNIST）" class="headerlink" title="3.5 图像分类数据集（Fashion-MNIST）"></a><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.5_fashion-mnist?id=_35-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88fashion-mnist%EF%BC%89">3.5 图像分类数据集（Fashion-MNIST）</a></h1><h3 id="torchvision包"><a href="#torchvision包" class="headerlink" title="torchvision包"></a>torchvision包</h3><p>主要用来构建计算机视觉模型</p><ol><li><code>torchvision.datasets</code>: 一些<strong>加载数据</strong>的函数及常用的<strong>数据集</strong>接口；</li><li><code>torchvision.models</code>: 包含常用的<strong>模型结构</strong>（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li><code>torchvision.transforms</code>: 常用的图片变换，例如裁剪、旋转等；</li><li><code>torchvision.utils</code>: 其他的一些<strong>有用的方法</strong>。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CV入门-街道字符识别</title>
    <link href="/2022/04/22/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/"/>
    <url>/2022/04/22/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="CV之街道字符识别"><a href="#CV之街道字符识别" class="headerlink" title="CV之街道字符识别"></a>CV之街道字符识别</h1><h1 id="Task1-赛题理解"><a href="#Task1-赛题理解" class="headerlink" title="Task1 赛题理解"></a>Task1 赛题理解</h1><p><strong>赛题任务：</strong>赛题以计算机视觉中字符识别为背景，要求选手预测街道字符编码，这是一个典型的字符识别问题。<br>为了简化赛题难度，赛题数据采用公开数据集<a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a>，因此大家可以选择很多相应的paper作为思路参考</p><h3 id="1-2-赛题数据"><a href="#1-2-赛题数据" class="headerlink" title="1.2 赛题数据"></a>1.2 赛题数据</h3><ul><li>以街道字符为为赛题数据</li><li>该数据来自收集的SVHN街道字符，并进行了匿名采样处理。</li><li>只能使用比赛给定的数据集完成训练，不能使用SVHN原始数据集进行训练。</li><li><strong>训练集</strong>数据包括3W张照片，<strong>验证集</strong>数据包括1W张照片<ul><li>每张照片包括<strong>颜色图像</strong>和对应的<strong>编码类别</strong>和<strong>具体位置</strong></li></ul></li><li><strong>测试集A</strong>包括4W张照片，<strong>测试集B</strong>包括4W张照片</li><li>需要选手**&#x3D;&#x3D;识别图片中所有的字符&#x3D;&#x3D;**</li></ul><h3 id="1-3-数据标签"><a href="#1-3-数据标签" class="headerlink" title="1.3 数据标签"></a>1.3 数据标签</h3><ul><li>在比赛数据（训练集和验证集）中，同一张图片中可能<strong>包括一个或者多个字符</strong></li><li>在比赛数据的JSON标注中，会有<strong>两个字符的边框信息：</strong></li></ul><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590046243876_M7q1ta5vwQ.jpg" alt="Image"></p><h3 id="1-5-读取数据"><a href="#1-5-读取数据" class="headerlink" title="1.5 读取数据"></a>1.5 读取数据</h3><p>JSON中标签的读取方式：</p><p><strong>import json</strong><br>train_json &#x3D; <strong>json.load</strong>(open(‘..&#x2F;input&#x2F;train.json’))</p><h1 id="数据标注处理"><a href="#数据标注处理" class="headerlink" title="数据标注处理"></a>数据标注处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_json</span>(<span class="hljs-params">d</span>):<br>    arr = np.array([<br>        d[<span class="hljs-string">&#x27;top&#x27;</span>], d[<span class="hljs-string">&#x27;height&#x27;</span>], d[<span class="hljs-string">&#x27;left&#x27;</span>],  d[<span class="hljs-string">&#x27;width&#x27;</span>], d[<span class="hljs-string">&#x27;label&#x27;</span>]<br>    ])<br>    arr = arr.astype(<span class="hljs-built_in">int</span>)<br>    <span class="hljs-keyword">return</span> arr<br><br>img = cv2.imread(<span class="hljs-string">&#x27;../input/train/000000.png&#x27;</span>)<br>arr = parse_json(train_json[<span class="hljs-string">&#x27;000000.png&#x27;</span>])<br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br>plt.subplot(<span class="hljs-number">1</span>, arr.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>plt.imshow(img)<br>plt.xticks([]); plt.yticks([])<br><br><span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(arr.shape[<span class="hljs-number">1</span>]):<br>    plt.subplot(<span class="hljs-number">1</span>, arr.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>, idx+<span class="hljs-number">2</span>)<br>    plt.imshow(img[arr[<span class="hljs-number">0</span>, idx]:arr[<span class="hljs-number">0</span>, idx]+arr[<span class="hljs-number">1</span>, idx],arr[<span class="hljs-number">2</span>, idx]:arr[<span class="hljs-number">2</span>, idx]+arr[<span class="hljs-number">3</span>, idx]])<br>    plt.title(arr[<span class="hljs-number">4</span>, idx])<br>    plt.xticks([]); plt.yticks([])<br></code></pre></td></tr></table></figure><h3 id="1-6-解题思路"><a href="#1-6-解题思路" class="headerlink" title="1.6 解题思路"></a>1.6 解题思路</h3><ul><li><p>赛题本质是分类问题，需要对图片的字符进行识别</p></li><li><p>本次赛题的难点是需要对<strong>不定长的字符</strong>进行识别</p></li><li><h4 id="1-简单入门思路：定长字符识别"><a href="#1-简单入门思路：定长字符识别" class="headerlink" title="1 简单入门思路：定长字符识别"></a>1 简单入门思路：定长字符识别</h4><ul><li>抽象为一个<strong>定长字符识别</strong>问题</li><li>大部分图像中字符个数为<strong>2-4个</strong>，最多的字符 个数为<strong>6个</strong></li><li>因此可以对于所有的图像都抽<strong>象为6个字符的识别问题</strong>，字符23填充为<strong>23XXXX</strong>，字符231填充为<strong>231XXX</strong></li><li>每个字符的分类中会进行11个类别的分类<ul><li>0123456789X</li></ul></li></ul></li><li><h4 id="2-专业字符识别思路：不定长字符识别"><a href="#2-专业字符识别思路：不定长字符识别" class="headerlink" title="2 专业字符识别思路：不定长字符识别"></a>2 专业字符识别思路：不定长字符识别</h4><ul><li>在<strong>字符识别研究</strong>中，有特定的方法来解决此种<strong>不定长的字符识别</strong>问题，比较典型的有<strong>CRNN字符识别模型</strong></li><li>图像可以视为一个<strong>单词</strong>或者一个<strong>句子</strong></li></ul></li><li><h4 id="3-专业分类思路：检测再识别"><a href="#3-专业分类思路：检测再识别" class="headerlink" title="3 专业分类思路：检测再识别"></a>3 专业分类思路：检测再识别</h4><ul><li>首先将<strong>字符的位置</strong>进行<strong>识别</strong>，利用<strong>物体检测</strong>的思路完成</li><li>需要参赛选手构建<strong>字符检测模型</strong></li><li>对<strong>测试集中的字符</strong>进行识别</li><li>参考<strong>物体检测模型SSD</strong>或者<strong>YOLO</strong>来完成</li></ul></li></ul><h3 id="1-7-本章小节"><a href="#1-7-本章小节" class="headerlink" title="1.7 本章小节"></a>1.7 本章小节</h3><p>有多种解法可以使用到计算机视觉领域中的各个模型</p><h1 id="Task2-数据读取与数据扩增"><a href="#Task2-数据读取与数据扩增" class="headerlink" title="Task2 数据读取与数据扩增"></a>Task2 数据读取与数据扩增</h1><p>使用【定长字符识别】思路来构建模型</p><h2 id="2-数据读取与数据扩增"><a href="#2-数据读取与数据扩增" class="headerlink" title="2 数据读取与数据扩增"></a>2 数据读取与数据扩增</h2><ul><li>数据读取</li><li>数据扩增方法</li><li>Pytorch读取赛题数据</li></ul><h3 id="2-1-学习目标"><a href="#2-1-学习目标" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><ul><li>Python和Pytorch中图像读取</li><li>扩增方法和Pytorch读取赛题数据</li></ul><h3 id="2-2-图像读取"><a href="#2-2-图像读取" class="headerlink" title="2.2 图像读取"></a>2.2 图像读取</h3><p>Python中有很多库可以完成<strong>数据读取</strong>的操作，比较常见的有<strong>Pillow和OpenCV</strong></p><p>在上一章节，我们给大家讲解了赛题的内容和三种不同的解决方案。从本章开始我们将逐渐的学习使用【定长字符识别】思路来构建模型，逐步讲解赛题的解决方案和相应知识点。</p><h2 id="2-数据读取与数据扩增-1"><a href="#2-数据读取与数据扩增-1" class="headerlink" title="2 数据读取与数据扩增"></a>2 数据读取与数据扩增</h2><p>本章主要内容为数据读取、数据扩增方法和Pytorch读取赛题数据三个部分组成。</p><h3 id="2-1-学习目标-1"><a href="#2-1-学习目标-1" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><ul><li>学习Python和Pytorch中图像读取</li><li>学会扩增方法和Pytorch读取赛题数据</li></ul><h3 id="2-2-图像读取-1"><a href="#2-2-图像读取-1" class="headerlink" title="2.2 图像读取"></a>2.2 图像读取</h3><p>由于赛题数据是图像数据，赛题的任务是识别图像中的字符。因此我们首先需要完成对数据的读取操作，在Python中有很多库可以完成数据读取的操作，比较常见的有Pillow和OpenCV。</p><h4 id="2-2-1-Pillow"><a href="#2-2-1-Pillow" class="headerlink" title="2.2.1 Pillow"></a>2.2.1 Pillow</h4><ul><li>Pillow是Python<strong>图像处理函式库(PIL）</strong>的一个分支</li><li>Pillow提供了常见的<strong>图像读取和处理</strong>的操作</li><li>可以与ipython notebook无缝集成</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-comment"># 导入Pillow库</span><br><span class="hljs-comment"># 读取图片</span><br>im =Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./cat.png&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageFilter<br>im = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./cat.png&#x27;</span>)<br><span class="hljs-comment"># 应用模糊滤镜</span><br>im2 = im.<span class="hljs-built_in">filter</span>(ImageFilter.BLUR)<br>im2.save(<span class="hljs-string">&#x27;blur.jpg&#x27;</span>, <span class="hljs-string">&#x27;jpeg&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-comment"># 打开一个jpg图像文件，注意是当前路径</span><br>im = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./cat.jpg&#x27;</span>)<br><span class="hljs-comment">#改变图像尺寸</span><br>im.thumbnail((w//<span class="hljs-number">2</span>, h//<span class="hljs-number">2</span>))<br>im.save(<span class="hljs-string">&#x27;thumbnail.jpg&#x27;</span>, <span class="hljs-string">&#x27;jpeg&#x27;</span>)<br></code></pre></td></tr></table></figure><h4 id="2-2-2-OpenCV"><a href="#2-2-2-OpenCV" class="headerlink" title="2.2.2 OpenCV"></a>2.2.2 OpenCV</h4><ul><li>一个跨平台的计算机视觉库，最早由Intel开源得来</li><li>拥有众多的计算机视觉、数字图像处理和机器视觉等功能</li><li><strong>功能上比Pillow更加强大很多</strong>，学习成本也高很多</li><li>OpenCV还内置了很多的图像特征处理算法，如<strong>关键点检测、边缘检测和直线检测</strong>等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-comment"># 导入Opencv库</span><br>img = cv2.imread(<span class="hljs-string">&#x27;./cat.jpg&#x27;</span>)<br><span class="hljs-comment"># Opencv默认颜色通道顺序是BRG，转换一下</span><br>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) <br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pytho">import cv2<br># 导入Opencv库<br>img = cv2.imread(&#x27;./cat.jpg&#x27;)<br>img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br># 转换为灰度图<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"> <span class="hljs-keyword">import</span> cv2<br><span class="hljs-comment"># 导入Opencv库</span><br>img = cv2.imread(<span class="hljs-string">&#x27;./cat.jpg&#x27;</span>)<br>img =cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br><span class="hljs-comment"># 转换为灰度图</span><br><span class="hljs-comment"># Canny边缘检测</span><br>edges = cv2.Canny(img, <span class="hljs-number">30</span>, <span class="hljs-number">70</span>)<br>cv2.imwrite(<span class="hljs-string">&#x27;canny.jpg&#x27;</span>, edges)<br></code></pre></td></tr></table></figure><h4 id="2-3-数据扩增方法"><a href="#2-3-数据扩增方法" class="headerlink" title="2.3 数据扩增方法"></a>2.3 数据扩增方法</h4><h5 id="2-3-1-数据扩增介绍"><a href="#2-3-1-数据扩增介绍" class="headerlink" title="2.3.1 数据扩增介绍"></a>2.3.1 数据扩增介绍</h5><ul><li>数据扩增可以<strong>增加训练集的样本</strong></li><li>同时也可以有效<strong>缓解模型过拟合</strong>的情况</li><li>也可以给模型带来的更强的<strong>泛化能力</strong></li></ul><h6 id="数据扩增为什么有用？"><a href="#数据扩增为什么有用？" class="headerlink" title="数据扩增为什么有用？"></a>数据扩增为什么有用？</h6><p>现有深度学习的<strong>参数</strong>非常多，一般的模型<strong>可训练的参数量</strong>基本上都是万到百万级别，而<strong>训练集样本的数量很难有这么多</strong></p><ul><li>数据扩增可以扩展样本空间</li></ul><h5 id="有哪些数据扩增方法？"><a href="#有哪些数据扩增方法？" class="headerlink" title="有哪些数据扩增方法？"></a>有哪些数据扩增方法？</h5><p>对于图像分类:</p><ul><li>数据扩增一般不会改变标签</li></ul><p>对于物体检测:</p><ul><li>数据扩增会改变物体坐标位置</li></ul><p>对于图像分割:</p><ul><li>数据扩增会改变像素标签</li></ul><h4 id="2-3-2-常见的数据扩增方法"><a href="#2-3-2-常见的数据扩增方法" class="headerlink" title="2.3.2 常见的数据扩增方法"></a>2.3.2 常见的数据扩增方法</h4><ul><li>一般会从图像<strong>颜色、尺寸、形态、空间</strong>和<strong>像素</strong>等角度进行变换</li><li>不同的数据扩增方法可以<strong>自由进行组合</strong>，得到更加丰富的数据扩增方法</li></ul><p>以torchvision为例，</p><h5 id="常见的数据扩增方法包括："><a href="#常见的数据扩增方法包括：" class="headerlink" title="常见的数据扩增方法包括："></a>常见的数据扩增方法包括：</h5><ul><li>transforms.CenterCrop 对图片<strong>中心进行裁剪</strong></li><li>transforms.ColorJitter 对图像<strong>颜色的对比度、饱和度和零度</strong>进行变换</li><li>transforms.FiveCrop 对图像<strong>四个角和中心进行裁剪</strong>得到五分图像</li><li>transforms.Grayscale 对图像进行<strong>灰度变换</strong></li><li>transforms.Pad 使用固定值进行<strong>像素填充</strong></li><li>transforms.RandomAffine <strong>随机仿射变换</strong></li><li>transforms.RandomCrop <strong>随机区域裁剪</strong></li><li>transforms.RandomHorizontalFlip <strong>随机水平翻转</strong></li><li>transforms.RandomRotation 随机<strong>旋转</strong></li><li>transforms.RandomVerticalFlip 随机<strong>垂直翻转</strong></li></ul><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1589808907852_BVNUbF2z04.jpg" alt="Image"></p><blockquote><p><strong>赛题任务是</strong>需要对图像中的字符进行识别，因此对于字符图片并<strong>不能进行翻转操作</strong></p><p>比如字符6经过水平翻转就变成了字符9，会改变字符原本的含义。</p></blockquote><h4 id="2-3-3-常用的数据扩增库"><a href="#2-3-3-常用的数据扩增库" class="headerlink" title="2.3.3 常用的数据扩增库"></a>2.3.3 常用的数据扩增库</h4><ul><li><h4 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h4><p><a href="https://github.com/pytorch/vision">https://github.com/pytorch/vision</a><br>pytorch官方提供的数据扩增库，提供了<strong>基本的数据数据扩增方法</strong>，可以无缝与torch进行集成；但数据扩增方法<strong>种类较少，且速度中等</strong>；</p></li><li><h4 id="imgaug"><a href="#imgaug" class="headerlink" title="imgaug"></a>imgaug</h4><p><a href="https://github.com/aleju/imgaug">https://github.com/aleju/imgaug</a><br>imgaug是常用的第三方数据扩增库，提供了<strong>多样的数据扩增方法</strong>，且组合起来非常方便，<strong>速度较快</strong>；</p></li><li><h4 id="albumentations"><a href="#albumentations" class="headerlink" title="albumentations"></a>albumentations</h4><p><a href="https://albumentations.readthedocs.io/">https://albumentations.readthedocs.io</a><br>是常用的第三方数据扩增库，提供了<strong>多样的数据扩增方法</strong>，对<strong>图像分类、语义分割、物体检测和关键点检测</strong>都支持，<strong>速度较快。</strong></p></li></ul><h2 id="2-4-Pytorch读取数据"><a href="#2-4-Pytorch读取数据" class="headerlink" title="2.4 Pytorch读取数据"></a>2.4 Pytorch读取数据</h2><blockquote><p>Pytorch中数据是通过Dataset进行封装，并通过DataLoder进行并行读取</p></blockquote><p>只需要&#x3D;&#x3D;重载一下数据读取的逻辑&#x3D;&#x3D;</p><h5 id="1-将赛题的图像数据和对应标签进行读取，在读取过程中的进行数据扩增："><a href="#1-将赛题的图像数据和对应标签进行读取，在读取过程中的进行数据扩增：" class="headerlink" title="1 将赛题的图像数据和对应标签进行读取，在读取过程中的进行数据扩增："></a>1 将赛题的<strong>图像数据</strong>和<strong>对应标签</strong>进行读取，在读取过程中的进行<strong>数据扩增</strong>：</h5><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/image-20220125160547861.png" alt="image-20220125160547861"></p><h5 id="2-将在定义好的Dataset基础上构建DataLoder"><a href="#2-将在定义好的Dataset基础上构建DataLoder" class="headerlink" title="2 将在定义好的Dataset基础上构建DataLoder"></a>2 将在定义好的Dataset基础上构建DataLoder</h5><ul><li>Dataset：对数据集的封装，提供<strong>索引方式</strong>的对数据样本进行<strong>读取</strong></li><li>DataLoder：对Dataset进行封装，提供<strong>批量读取的迭代读取</strong></li></ul><h3 id="基于公开的CIFAR10数据集"><a href="#基于公开的CIFAR10数据集" class="headerlink" title="基于公开的CIFAR10数据集"></a>基于公开的CIFAR10数据集</h3><p>直接调用&#x3D;&#x3D;torchvision.datasets.CIFAR10&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision <br><span class="hljs-keyword">from</span> torch.utils.data.dataset <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms         <br><br><span class="hljs-comment"># 读取训练集</span><br>train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&#x27;../../../dataset&#x27;</span>, <br>                                                      train=<span class="hljs-literal">True</span>, <br>                                                      transform=<span class="hljs-literal">None</span>,  <br>                                                      target_transform=<span class="hljs-literal">None</span>, <br>                                                      download=<span class="hljs-literal">True</span>)          <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">torchvision.datasets.CIFAR10(dataset_dir, train=True, transform=None, target_transform=None, download=False) </span><br><span class="hljs-string"></span><br><span class="hljs-string">dataset_dir：存放数据集的路径。</span><br><span class="hljs-string">train（bool，可选）–如果为True，则构建训练集，否则构建测试集。</span><br><span class="hljs-string">transform：定义数据预处理，数据增强方案都是在这里指定。</span><br><span class="hljs-string">target_transform：标注的预处理，分类任务不常用。</span><br><span class="hljs-string">download：是否下载，若为True则从互联网下载，如果已经在dataset_dir下存在，就不会再次下载</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><p>如果<strong>非官方数据集</strong>，还是会对样本进行一定的<strong>数据增强</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取训练集</span><br>custom_transform=transforms.transforms.Compose([<br>              transforms.Resize((<span class="hljs-number">64</span>, <span class="hljs-number">64</span>)),    <span class="hljs-comment"># 缩放到指定大小 64*64</span><br>              transforms.ColorJitter(<span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>),    <span class="hljs-comment"># 随机颜色变换</span><br>              transforms.RandomRotation(<span class="hljs-number">5</span>),    <span class="hljs-comment"># 随机旋转</span><br>              transforms.Normalize([<span class="hljs-number">0.485</span>,<span class="hljs-number">0.456</span>,<span class="hljs-number">0.406</span>],    <span class="hljs-comment"># 对图像像素进行归一化</span><br>                                   [<span class="hljs-number">0.229</span>,<span class="hljs-number">0.224</span>,<span class="hljs-number">0.225</span>])])<br>train_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&#x27;../../../dataset&#x27;</span>, <br>                                        train=<span class="hljs-literal">True</span>,                                       <br>                                        transform=custom_transforms,<br>                                        target_transform=<span class="hljs-literal">None</span>, <br>                                        download=<span class="hljs-literal">False</span>)   <br><br></code></pre></td></tr></table></figure><p>数据集定义完成后，我们还需要进行<strong>数据加载</strong></p><p><strong>DataLoader</strong>来完成对于数据集的加载，并且支持多进程并行读取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集(官方数据集)</span><br>train_data=torchvision.datasets.CIFAR10(<span class="hljs-string">&#x27;../../../dataset&#x27;</span>, train=<span class="hljs-literal">True</span>, <br>                                                      transform=<span class="hljs-literal">None</span>,  <br>                                                      target_transform=<span class="hljs-literal">None</span>, <br>                                                      download=<span class="hljs-literal">True</span>)          <br><span class="hljs-comment"># 实现数据批量读取</span><br>train_loader = torch.utils.data.DataLoader(train_data,<br>                                           batch_size=<span class="hljs-number">2</span>,<br>                                           shuffle=<span class="hljs-literal">True</span>,<br>                                           num_workers=<span class="hljs-number">4</span>) <br><br><br></code></pre></td></tr></table></figure><p>num_workers&gt;&#x3D;1表示多进程读取数据，在Win下<strong>num_workers</strong>&#x3D;&#x3D;只能设置为0&#x3D;&#x3D;，否则会报错</p><blockquote><p><strong>DataLoader</strong>(<u>dataset</u>, <u>batch_size</u>&#x3D;1, <u>shuffle</u>&#x3D;False, sampler&#x3D;None, <u>num_workers</u>&#x3D;0, collate_fn&#x3D;default_collate, pin_memory&#x3D;False, drop_last&#x3D;False)</p></blockquote><ul><li>dataset：加载的数据集(Dataset对象)</li><li>batch_size：一个批量数目大小</li><li>shuffle:：是否打乱数据顺序</li><li>num_workers：使用多进程加载的进程数，0代表不使用多进程</li></ul><h3 id="自定义数据集及读取方法"><a href="#自定义数据集及读取方法" class="headerlink" title="自定义数据集及读取方法"></a>自定义数据集及读取方法</h3><ul><li>读取我们自己数据集中的数据</li><li>写一个Dataset的子类来定义我们的数据集</li></ul><h2 id="jupyter显示图片"><a href="#jupyter显示图片" class="headerlink" title="jupyter显示图片"></a>jupyter显示图片</h2><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1430038-20190510142709115-1866877626.png" alt="img"></p><h1 id="Task3-字符识别模型"><a href="#Task3-字符识别模型" class="headerlink" title="Task3 字符识别模型"></a>Task3 字符识别模型</h1><blockquote><p>构建一个<strong>定长多字符</strong>分类模型</p><p>卷积神经网络（Convolutional Neural Network, CNN）的常见层，并从头搭建一个<strong>字符识别模型</strong></p></blockquote><h2 id="CNN介绍"><a href="#CNN介绍" class="headerlink" title="CNN介绍"></a>CNN介绍</h2><ul><li>特殊的人工神经网络</li><li>精度和速度比传统计算学习算法高很多</li></ul><p>CNN是解决<strong>图像分类、图像检索、物体检测</strong>和<strong>语义分割</strong>的主流模型。</p><ul><li><p>每一层由众多的<strong>卷积核</strong>组成</p></li><li><p>每个卷积核对输入的像素进行卷积操作</p></li><li><p>得到下一次的输入</p></li><li><p>随着网络层的增加</p></li><li><p>卷积核会逐渐扩大感受野</p></li><li><p>并缩减图像的尺寸</p></li></ul><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055764551_AtogOEkSkl.jpg" alt="Image"></p><ul><li>CNN是一种<strong>层次模型</strong></li><li>输入的是<strong>原始的像素数据</strong></li><li>通过&#x3D;&#x3D;卷积（convolution）&#x3D;&#x3D;<ul><li>&#x3D;&#x3D;池化（pooling）&#x3D;&#x3D;</li><li>&#x3D;&#x3D;非线性激活函数（non-linear activation function）&#x3D;&#x3D;</li><li>&#x3D;&#x3D;全连接层（fully connected layer）&#x3D;&#x3D;构成</li></ul></li></ul><h3 id="LeNet网络结构"><a href="#LeNet网络结构" class="headerlink" title="LeNet网络结构"></a>LeNet网络结构</h3><ul><li>经典的<strong>字符识别模型</strong></li><li>两个卷积层，两个池化层，两个全连接层组成</li><li>卷积核都是5×5，stride&#x3D;1，池化层使用最大池化</li></ul><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055779865_mGmYsescOS.jpg" alt="Image"></p><p>多次卷积和池化，CNN的<strong>最后一层</strong>将输入的**&#x3D;&#x3D;图像像素&#x3D;&#x3D;<strong>映射为</strong>&#x3D;&#x3D;具体的输出&#x3D;&#x3D;**</p><ul><li>如在分类任务中会转换为不同类别的概率输出</li><li>计算真实标签与CNN模型的预测结果的<strong>差异</strong>loss</li><li>反向传播<strong>更新每层的参数</strong></li><li>更新完成后再次<strong>前向传播</strong></li><li>反复训练</li></ul><blockquote><p>与传统机器学习模型相比，CNN具有一种&#x3D;&#x3D;端到端（End to End）&#x3D;&#x3D;的思路</p></blockquote><p>训练的过程中是<strong>直接从图像像素到最终的输出</strong>，并不涉及到<strong>具体的特征提取</strong>和<strong>构建模型</strong>的过程，也<strong>不需要人工的参与</strong></p><h2 id="CNN发展"><a href="#CNN发展" class="headerlink" title="CNN发展"></a>CNN发展</h2><p><strong>网络模型结构越深、网络参数越多</strong>模型的精度更优。比较典型的是<strong>AlexNet、VGG、InceptionV3和ResNet</strong></p><p><em>LeNet-5(1998)</em></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055803003_31Dhamhy00.jpg" alt="Image"></p><p><strong>AlexNet(2012)</strong></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055813242_7nu5MzqZqj.jpg" alt="Image"></p><p><em>VGG-16(2014)</em></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055822332_4BXkZoGEJQ.jpg" alt="Image"></p><p><em>Inception-v1 (2014)</em></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055835546_7ue9xs5QG8.jpg" alt="Image"></p><p><em>ResNet-50 (2015)</em></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590055839093_rEeGjOcbcT.jpg" alt="Image"></p><h2 id="Pytorch构建CNN模型"><a href="#Pytorch构建CNN模型" class="headerlink" title="Pytorch构建CNN模型"></a>Pytorch构建CNN模型</h2><p>在Pytorch中构建CNN模型：</p><ul><li>只需要定义好<strong>模型的参数</strong>和<strong>正向传播</strong>即可</li><li>Pytorch会根据正向传播自动计算反向传播</li></ul><p>这个CNN模型包括<u><strong>两个卷积层</strong></u>，最后并联<u><strong>6个全连接层</strong></u>进行分类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># CNN提取特征模块</span><br>self.cnn = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<br>    nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<br>    nn.ReLU(),<br>    nn.MaxPool2d(<span class="hljs-number">2</span>),<br>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#6个全连接层</span><br>self.fc1 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br>self.fc2 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br>self.fc3 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br>self.fc4 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br>self.fc5 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br>self.fc6 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">7</span>, <span class="hljs-number">11</span>)<br></code></pre></td></tr></table></figure><p>分成11类（0-9，x）</p><h1 id="Task4-模型训练与验证"><a href="#Task4-模型训练与验证" class="headerlink" title="Task4 模型训练与验证"></a>Task4 模型训练与验证</h1><p>一个成熟合格的深度学习训练流程至少具备以下功能：</p><ul><li>在<strong>训练集上进行训练</strong>，并在<strong>验证集上进行验证</strong>；</li><li>模型可以保存<strong>最优的权重</strong>，并<strong>读取权重</strong>；</li><li>记录下<strong>训练集和验证集的精度</strong>，便于调<strong>参</strong>。</li></ul><p>使用Pytorch来完成CNN的<strong>训练和验证</strong>过程</p><ul><li>构造<strong>训练集</strong>和<strong>验证集</strong>；</li><li>每轮进行训练和验证，并根据<strong>最优验证集</strong>精度保存模型</li></ul><h2 id="1-构造验证集"><a href="#1-构造验证集" class="headerlink" title="1 构造验证集"></a>1 构造验证集</h2><blockquote><p>在机器学习模型（特别是深度学习模型）的训练过程中，模型是非常容易<strong>过拟合</strong></p></blockquote><p>深度学习模型在不断的训练过程中<strong>训练误差会逐渐降低</strong>，但<strong>测试误差的走势则不一定</strong></p><h3 id="过拟合（Overfitting）"><a href="#过拟合（Overfitting）" class="headerlink" title="过拟合（Overfitting）"></a>过拟合（Overfitting）</h3><p>模型如果将<strong>训练集学的过好</strong>，模型就会记住<strong>训练样本的细节</strong>，导致模型<strong>在测试集的泛化效果较差</strong></p><h4 id="导致模型过拟合的情况"><a href="#导致模型过拟合的情况" class="headerlink" title="导致模型过拟合的情况"></a>导致模型过拟合的情况</h4><p>模型复杂度（Model Complexity ）太高：</p><ul><li>导致模型学习到了<strong>训练数据的方方面面</strong>，学习到了一些<strong>细枝末节的规律</strong></li></ul><p>解决：</p><ul><li>构建一个与<strong>测试集</strong>尽可能分布一致的样本集（可称为<strong>验证集</strong>）</li><li>在训练过程中<strong>不断验证模型在验证集上的精度</strong></li><li>以此<strong>控制模型的训练</strong></li></ul><h3 id="比赛时："><a href="#比赛时：" class="headerlink" title="比赛时："></a>比赛时：</h3><ul><li><p>赛题方会给定&#x3D;&#x3D;训练集&#x3D;&#x3D;和&#x3D;&#x3D;测试集&#x3D;&#x3D;两部分数据</p></li><li><p>参赛者需要<strong>在<u>训练集</u>上面构建模型</strong>，并<strong>在<u>测试集</u>上面验证模型的泛化能力</strong></p></li><li><p>参赛者可以通过提交模型<strong>对测试集的预测结果</strong>，来验证自己模型的泛化能力</p></li><li><p>也可以自己在<strong>本地划分出一个验证集</strong>出来，进行本地验证</p></li><li><p>各数据作用：</p><ul><li><h4 id="训练集（Train-Set）：模型用于训练和调整模型参数；"><a href="#训练集（Train-Set）：模型用于训练和调整模型参数；" class="headerlink" title="训练集（Train Set）：模型用于训练和调整模型参数；"></a>训练集（Train Set）：模型用于训练和调整模型参数；</h4></li><li><h4 id="验证集（Validation-Set）：用来验证模型精度和调整模型超参数；"><a href="#验证集（Validation-Set）：用来验证模型精度和调整模型超参数；" class="headerlink" title="验证集（Validation Set）：用来验证模型精度和调整模型超参数；"></a>验证集（Validation Set）：用来验证模型精度和调整模型超参数；</h4></li><li><h4 id="测试集（Test-Set）：验证模型的泛化能力。"><a href="#测试集（Test-Set）：验证模型的泛化能力。" class="headerlink" title="测试集（Test Set）：验证模型的泛化能力。"></a>测试集（Test Set）：验证模型的泛化能力。</h4></li></ul></li></ul><h4 id="验证集的划分"><a href="#验证集的划分" class="headerlink" title="验证集的划分"></a>验证集的划分</h4><h5 id="留出法（Hold-Out）"><a href="#留出法（Hold-Out）" class="headerlink" title="留出法（Hold-Out）"></a>留出法（Hold-Out）</h5><ul><li><strong>数据量比较大</strong>的情况</li><li>&#x3D;&#x3D;直接将训练集划分成两部分，新的训练集和验证集&#x3D;&#x3D;</li><li>优点是最为直接简单</li><li>缺点是只得到了一份验证集，有可能导致模型在验证集上<strong>过拟合</strong></li></ul><h5 id="交叉验证法（Cross-Validation，CV）"><a href="#交叉验证法（Cross-Validation，CV）" class="headerlink" title="交叉验证法（Cross Validation，CV）"></a>交叉验证法（Cross Validation，CV）</h5><ul><li><p>将训练集划<strong>分成K份</strong>，将其中的<strong>K-1份作为训练集</strong>，剩余的<strong>1份作为验证集</strong>，循环K训练</p></li><li><p>&#x3D;&#x3D;所有的训练集都是验证集&#x3D;&#x3D;，最终模型验证精度是K份平均得到</p></li><li><p>优点是验证集精度比较可靠</p></li><li><p>缺点是需要训练K次，不适合数据量很大的情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loader = torch.utils.data.DataLoader(    train_dataset,    **batch_size=<span class="hljs-number">10</span>,**     **shuffle=<span class="hljs-literal">True</span>,**     num_workers=<span class="hljs-number">10</span>,  )     val_loader = torch.utils.data.DataLoader(    val_dataset,    **batch_size=<span class="hljs-number">10</span>,**     **shuffle=<span class="hljs-literal">False</span>,**     num_workers=<span class="hljs-number">10</span>,  )<br></code></pre></td></tr></table></figure></li></ul><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1964989-20200530231538613-926074198.png" alt="img"></p><h5 id="自助采样法（BootStrap）"><a href="#自助采样法（BootStrap）" class="headerlink" title="自助采样法（BootStrap）"></a>自助采样法（BootStrap）</h5><ul><li>有放回的采样方式得到新的训练集和验证集</li><li>适用于<strong>数据量较小的情况</strong></li></ul><p>划分验证集的时候，需要注意<strong>验证集的分布应该与测试集尽量保持一致</strong></p><p>任何的验证集的划分得到的验证集都是要保证<strong>训练集-验证集-测试集的分布</strong>是一致的</p><h3 id="欠拟合（Underfitting）"><a href="#欠拟合（Underfitting）" class="headerlink" title="欠拟合（Underfitting）"></a>欠拟合（Underfitting）</h3><p>模型在<strong>训练集上的拟合效果较差</strong></p><h2 id="2-模型训练与验证"><a href="#2-模型训练与验证" class="headerlink" title="2 模型训练与验证"></a>2 模型训练与验证</h2><h2 id="3-模型保存与加载"><a href="#3-模型保存与加载" class="headerlink" title="3 模型保存与加载"></a>3 模型保存与加载</h2><p>比较常见的做法是<strong>保存和加载</strong>模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model_object.state_dict(), <span class="hljs-string">&#x27;model.pt&#x27;</span>)<br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27; model.pt&#x27;</span>)) <br></code></pre></td></tr></table></figure><h2 id="4-模型调参流程"><a href="#4-模型调参流程" class="headerlink" title="4 模型调参流程"></a>4 模型调参流程</h2><p>训练深度学习模型需要GPU的硬件支持，也需要较多的训练时间</p><p>训练技巧：</p><ul><li><a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html">http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html</a></li><li><a href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a></li></ul><p>深度学习模型的精度与模型的复杂度、数据量、正则化、数据扩增等因素直接相关</p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjcxNjU3MA==,size_16,color_FFFFFF,t_70#pic_center.png" alt="调参流程"></p><h1 id="Task5-模型集成"><a href="#Task5-模型集成" class="headerlink" title="Task5 模型集成"></a>Task5 模型集成</h1><blockquote><p>使用集成学习<strong>提高预测精度</strong></p></blockquote><p>集成学习只能在一定程度上提高精度</p><p>并需要耗费较大的训练时间</p><p>建议先使用<strong>提高单个模型的精度</strong></p><p>再考虑集成学习过程</p><p>具体的集成学习方法需要<strong>与验证集划分方法</strong>结合</p><p>Dropout和TTA在所有场景都可以起作用</p><h2 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h2><ul><li>Stacking、Bagging和Boosting</li><li>这些集成学习方法与具体<strong>验证集划分</strong>联系紧密<ul><li>硬件设备不允许：建议选取<strong>留出法</strong></li><li>追求精度：可以使用<strong>交叉验证</strong>的方法</li></ul></li></ul><h3 id="10折交叉验证"><a href="#10折交叉验证" class="headerlink" title="10折交叉验证"></a>10折交叉验证</h3><p>训练得到10个CNN模型</p><blockquote><h5 id="交叉验证法（Cross-Validation，CV）-1"><a href="#交叉验证法（Cross-Validation，CV）-1" class="headerlink" title="交叉验证法（Cross Validation，CV）"></a>交叉验证法（Cross Validation，CV）</h5><ul><li>将训练集划<strong>分成K份</strong>，将其中的<strong>K-1份作为训练集</strong>，剩余的<strong>1份作为验证集</strong>，循环K训练</li></ul></blockquote><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/image-20220207182336934.png" alt="image-20220207182336934"></p><blockquote><ul><li>&#x3D;&#x3D;所有的训练集都是验证集&#x3D;&#x3D;，最终模型验证精度是K份<strong>平均</strong>得到</li><li>优点是验证集精度比较可靠</li><li>缺点是需要训练K次，不适合数据量很大的情况</li></ul></blockquote><p>求得10个E后：</p><ul><li>平均（如图）</li><li>投票</li></ul><blockquote><p>那么在10个CNN模型可以使用如下方式进行集成：</p><ul><li>对预测的结果的概率值进行平均，然后解码为具体字符；</li><li>对预测的字符进行投票，得到最终字符。</li></ul></blockquote><h2 id="深度学习中的集成学习"><a href="#深度学习中的集成学习" class="headerlink" title="深度学习中的集成学习"></a>深度学习中的集成学习</h2><h3 id="1-Dropout"><a href="#1-Dropout" class="headerlink" title="1 Dropout"></a>1 Dropout</h3><p>训练深度神经网络的一种技巧</p><ul><li><strong>可以有效的缓解模型过拟合的情况（训练），也可以在预测时增加模型的精度（预测）</strong></li><li>用于正则化和防止神经元共同适应的有效技术</li></ul><hr><ul><li>在每个<strong>训练批次</strong>中，通过<strong>随机让一部分的节点停止工作</strong><ul><li>&#x3D;&#x3D;<strong>有效的缓解模型过拟合</strong>&#x3D;&#x3D;（训练时不要学得太好）</li><li><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/image-20220207182655522.png" alt="image-20220207182655522"></li></ul></li><li>同时在<strong>预测</strong>的过程中让所有的节点都起作用<ul><li>可以在预测时<strong>增加模型的精度</strong></li></ul></li></ul><h4 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h4><p>在self.cnn &#x3D; nn.Sequential()中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),<br>nn.ReLU(),<br>nn.Dropout(<span class="hljs-number">0.25</span>),<span class="hljs-comment">#写在这</span><br>nn.MaxPool2d(<span class="hljs-number">2</span>),<br></code></pre></td></tr></table></figure><h3 id="2-TTA测试集数据扩增（Test-Time-Augmentation）"><a href="#2-TTA测试集数据扩增（Test-Time-Augmentation）" class="headerlink" title="2 TTA测试集数据扩增（Test Time Augmentation）"></a>2 TTA测试集数据扩增（Test Time Augmentation）</h3><p>&#x3D;&#x3D;<strong>数据扩增</strong>不仅可以在训练时候用，而且可以同样在<strong>预测时候进行数据扩增</strong>&#x3D;&#x3D;</p><p>对<strong>同一个样本预测三次</strong>，然后对三次结果进行<strong>平均</strong></p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/image-20220209161606036.png" alt="image-20220209161606036"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 5 预测</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">test_loader, model, tta=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-comment">#切换预测模式</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    test_pred_tta = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># TTA 次数</span><br>    <span class="hljs-comment"># 对同一个样本（？）循环10次预测</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tta): <span class="hljs-comment">#_表示循环标志，循环tta次=10</span><br>        test_pred = []<span class="hljs-comment">#记录预测输出</span><br>        <span class="hljs-comment"># 不记录模型梯度信息</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> i, (<span class="hljs-built_in">input</span>, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(test_loader):<br>                <span class="hljs-comment">#使用gpu</span><br>                <span class="hljs-keyword">if</span> use_cuda:<br>                    <span class="hljs-built_in">input</span> = <span class="hljs-built_in">input</span>.cuda()<br>                    <span class="hljs-comment">#预测中不用将target传上cuda</span><br><br>                <span class="hljs-comment">#放在gpu以后/不使用gpu，计算预测值</span><br>                c0, c1, c2, c3, c4,c5 = model(<span class="hljs-built_in">input</span>)<br>                <span class="hljs-keyword">if</span> use_cuda:<span class="hljs-comment">#使用gpu计算输出</span><br>                    output = np.concatenate([<span class="hljs-comment">#输出=几个值的连接</span><br>                        c0.data.cpu().numpy(),<span class="hljs-comment">#c0.data放在cpu上，tensor转换成numpy</span><br>                        c1.data.cpu().numpy(),<br>                        c2.data.cpu().numpy(),<br>                        c3.data.cpu().numpy(),<br>                        c4.data.cpu().numpy(),<br>                        c5.data.cpu().numpy()],axis=<span class="hljs-number">1</span>) <span class="hljs-comment">#按列连接</span><br>                <span class="hljs-keyword">else</span>:<span class="hljs-comment">#没有gpu的输出</span><br>                    output = np.concatenate([<br>                        c0.data.numpy(),<span class="hljs-comment">#不放在cpu上</span><br>                        c1.data.numpy(),<br>                        c2.data.numpy(),<br>                        c3.data.numpy(),<br>                        c4.data.numpy(),<br>                        c5.data.cpu().numpy()], axis=<span class="hljs-number">1</span>) <span class="hljs-comment">#按列连接</span><br><br>                test_pred.append(output)<span class="hljs-comment">#预测输出+1个output</span><br><br>        test_pred = np.vstack(test_pred)<span class="hljs-comment">#将预测输出按行连接</span><br>        <span class="hljs-keyword">if</span> test_pred_tta <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<span class="hljs-comment">#初始值</span><br>            test_pred_tta = test_pred<span class="hljs-comment">#第一次预测结果</span><br>        <span class="hljs-keyword">else</span>:<br>            test_pred_tta += test_pred<span class="hljs-comment">#对预测结果做累加</span><br><br>    <span class="hljs-keyword">return</span> test_pred_tta<span class="hljs-comment">#返回预测结果</span><br></code></pre></td></tr></table></figure><h2 id="3-Snapshot"><a href="#3-Snapshot" class="headerlink" title="3 Snapshot"></a>3 Snapshot</h2><p>假如<strong>只训练了一个CNN模型</strong>，如何做模型集成呢？</p><p><img src="/CV%E5%85%A5%E9%97%A8-%E8%A1%97%E9%81%93%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.assets/1590045512135_Vdn3L47Ki4.jpg" alt="Image"></p><h4 id="cyclical-learning-rate进行训练模型"><a href="#cyclical-learning-rate进行训练模型" class="headerlink" title="cyclical learning rate进行训练模型"></a>cyclical learning rate进行训练模型</h4><p>在论文Snapshot Ensembles中，作者提出使<strong>用cyclical learning rate进行训练模型</strong>，并<strong>保存精度比较好的一些checkopint</strong>，最后将<strong>多个checkpoint进行模型集成</strong></p><h5 id="cyclical-learning-rate中学习率的变化："><a href="#cyclical-learning-rate中学习率的变化：" class="headerlink" title="cyclical learning rate中学习率的变化："></a>cyclical learning rate中学习率的变化：</h5><p>周期性变大和减少</p><ul><li>CNN模型很有可能在<strong>跳出局部最优</strong>进入<strong>另一个局部最优</strong></li><li>此种方法可以<strong>在一定程度上提高模型精度</strong>，但<strong>需要更长的训练时间</strong></li></ul><h2 id="结果后处理思路"><a href="#结果后处理思路" class="headerlink" title="结果后处理思路"></a>结果后处理思路</h2><p>从以下几个思路<strong>对预测结果进行后处理</strong></p><ul><li>统计图片中<strong>每个位置字符出现的频率</strong>，使用规则修正结果；</li><li>单独训练<strong>一个字符长度</strong>预测模型，用来<strong>预测图片中字符个数</strong>，并修正结果。</li></ul><h2 id="天池云导入虚拟环境"><a href="#天池云导入虚拟环境" class="headerlink" title="天池云导入虚拟环境"></a>天池云导入虚拟环境</h2><ul><li>先conda准备好</li><li>进入这个环境</li><li>conda install ipykernel</li><li>python -m ipykernel install –user –name stree</li><li>把python3换成需要的即可</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS231笔记</title>
    <link href="/2022/04/22/CS231%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/04/22/CS231%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="CS231n-1"><a href="#CS231n-1" class="headerlink" title="CS231n-1"></a>CS231n-1</h1><h1 id="什么是NLP"><a href="#什么是NLP" class="headerlink" title="什么是NLP"></a>什么是NLP</h1><p>人工智能：</p><p>计算机视觉</p><p>机器人技术</p><p>知识表达和推理</p><p>人类通过语言来思考、行动</p><p>地球上很多生物都拥有不错的视觉</p><p>但只有人类拥有语言</p><p>目标：</p><p>让电脑处理、理解人类语言，完成有意义的事</p><p>siri等：与人类交流</p><p> 把人类语言变成消费级技术</p><h1 id="7-3、损失函数和优化介绍"><a href="#7-3、损失函数和优化介绍" class="headerlink" title="7-3、损失函数和优化介绍"></a>7-3、损失函数和优化介绍</h1><p>线性分类：参数分类的一种</p><p>所有训练数据中的经验知识都体现在参数矩阵W中</p><p>而W通过训练过程得到</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107134755418.png" alt="image-20220107134755418"> </p><p>某一种类别得分更高，可能是什么<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107135456722.png" alt="image-20220107135456722"></p><p>线性分类器：每个种类的学习模板</p><p>矩阵W的每行都对应一个分类模板</p><h1 id="怎么选择W"><a href="#怎么选择W" class="headerlink" title="怎么选择W"></a>怎么选择W</h1><p>某一种类别得分更高，就可能是什么<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107135456722.png" alt="image-20220107135456722"></p><p>用人眼观察，可以</p><p>但是自动决定哪些W是最优？需要一个度量任意某个W的好坏的方法</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>一个函数，输入是W，看得分，定量估计W的好坏</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107140909461.png" alt="image-20220107140909461"></p><p>x：输入图像</p><p>y：label，f输出的结果</p><p>所有测试集损失函数的平均结果</p><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>多类别SVM：处理多分类时的一种推广 </p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107144726425.png" alt="image-20220107144726425"></p><p>某个值和0取max的损失函数：&#x3D;&#x3D;合页损失函数&#x3D;&#x3D;&#x3D;&#x3D;hinge loss&#x3D;&#x3D;（0-+∞）</p><p>Syi：训练样本真实分类的分数（通过分类器预测出来第i个样本的真实分类的分数）</p><p>​e.g.S1: 猫的分数，S2:狗的分数，yi：这个样本的正确的分类标签</p><p>y轴：损失</p><p>随着真实分类的分数↑，loss线性↓，直到分数超过一个阈值，loss&#x3D;0（已经为这个样本成功分对类）</p><h3 id="举例：SVM-loss"><a href="#举例：SVM-loss" class="headerlink" title="举例：SVM loss"></a>举例：SVM loss</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107150113580.png" alt="image-20220107150113580"></p><p>要对所有不正确的分类全都循环一遍</p><p>cat：因为car分数5.1是错的，造成max种选取了一个正数，导致最后有loss</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107150700396.png" alt="image-20220107150700396"></p><p> 最后的loss：三个的平均值</p><p>（第三个最应该的却最低，loss最大）</p><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107155407817.png" alt="image-20220107155407817"></p><h3 id="Q3-当所有的S，分数都近乎为0，差不多相等，loss-x3D-？"><a href="#Q3-当所有的S，分数都近乎为0，差不多相等，loss-x3D-？" class="headerlink" title="Q3:当所有的S，分数都近乎为0，差不多相等，loss&#x3D;？"></a>Q3:当所有的S，分数都近乎为0，差不多相等，loss&#x3D;？</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107155541218.png" alt="image-20220107155541218"></p><p>Sj≈Syi，所以&#x3D;&#x3D;&#x3D;分类的数量-1&#x3D;&#x3D;</p><p>∵遍历了C-1个类别</p><p>用于：刚开始训练时，第一次迭代的loss应该&#x3D;C-1，否则可能有bug</p><h3 id="x3D-x3D-矢量化code-x3D-x3D"><a href="#x3D-x3D-矢量化code-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;矢量化code&#x3D;&#x3D;"></a>&#x3D;&#x3D;矢量化code&#x3D;&#x3D;</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107160943466.png" alt="image-20220107160943466"></p><p>可以真正进入这里，把边际值归0</p><p>可以只迭代一个类，而不是所有</p><p>把想跳过的那个清0，然后计算总和</p><h3 id="Q：具有loss-x3D-0的w是唯一的吗？"><a href="#Q：具有loss-x3D-0的w是唯一的吗？" class="headerlink" title="Q：具有loss&#x3D;0的w是唯一的吗？"></a>Q：具有loss&#x3D;0的w是唯一的吗？</h3><p>否，还有其他的，2w也是L&#x3D;0</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107161748359.png" alt="image-20220107161748359"></p><p>w都翻倍，阈值1没变，最终的L也没变 </p><p>机器学习的重点：</p><p>使用训练数据来找到一些分类器，然后将这个东西应用于测试数据</p><p>并不关心训练集的表现，关心分类器在测试集上的表现</p><h3 id="正则项"><a href="#正则项" class="headerlink" title="正则项"></a>正则项</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107165038731.png" alt="image-20220107165038731"></p><p>鼓励模型以某种方式选择更简单的W</p><p><strong>标准损失函数</strong>：有2个项：&#x3D;&#x3D;数据丢失项+正则项&#x3D;&#x3D;</p><p>（R前的超参数入：用来平衡这两个项)</p><h4 id="这三个项之间有什么相互作用？"><a href="#这三个项之间有什么相互作用？" class="headerlink" title="这三个项之间有什么相互作用？"></a>这三个项之间有什么相互作用？</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107165550665.png" alt="image-20220107165550665"></p><p>对模型做的任何事情，种种所谓的“惩罚”，主要目的；<strong>减轻模型的复杂度</strong>，而非视图拟合数据</p><h3 id="Q-L2正则化如何度量复杂性？"><a href="#Q-L2正则化如何度量复杂性？" class="headerlink" title="Q:L2正则化如何度量复杂性？"></a>Q:L2正则化如何度量复杂性？</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107214433994.png" alt="image-20220107214433994"></p><h3 id="softmax-loss"><a href="#softmax-loss" class="headerlink" title="softmax loss"></a>softmax loss</h3><ul><li>log函数：单调递增，当概率最大时找到最大值</li><li>用softmax对分数进行转化处理，得到正确类别</li></ul><p><strong>损失函数</strong>是： -log P</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107220435295.png" alt="image-20220107220435295"></p><h3 id="Q-sofmax的最小值最大值是多少？"><a href="#Q-sofmax的最小值最大值是多少？" class="headerlink" title="Q:sofmax的最小值最大值是多少？"></a>Q:sofmax的最小值最大值是多少？</h3><ul><li>min：0，当正确类别的概率为1时</li><li>max：+∞，当正确类别的概率为0时<ul><li>不会达到这个极值（因为-∞的指数才为0）</li></ul></li></ul><p>第一次迭代：（S≈0）</p><p>loss应该为C取自然对数：lnC</p><h2 id="两个损失函数的对比"><a href="#两个损失函数的对比" class="headerlink" title="两个损失函数的对比"></a>两个损失函数的对比</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107221628109.png" alt="image-20220107221628109"></p><ol><li>通过线性分类器后，计算一样，得到的分值解释方法不同</li><li>SVM中，某一分值↑，因为max函数，不对loss值有影响</li><li>softmax中，会有影响，且希望越大越好，loss会越小</li></ol><h1 id="优化过程"><a href="#优化过程" class="headerlink" title="优化过程"></a>优化过程</h1><p>找到一种有效的方式，从W的可行域里找到W取什么值是最不坏的情况（loss最小）</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="法1：随机搜索"><a href="#法1：随机搜索" class="headerlink" title="法1：随机搜索"></a>法1：随机搜索</h3><p>需要很多权重值W，随机采样，将它们输入损失函数，再看效果如何</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107223603578.png" alt="image-20220107223603578"></p><h3 id="法2：Follow-the-slope"><a href="#法2：Follow-the-slope" class="headerlink" title="法2：Follow the slope"></a>法2：Follow the slope</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107224218986.png" alt="image-20220107224218986"></p><p>多元函数的导数：<strong>梯度</strong></p><ul><li>偏导数组成的<u>向量</u></li><li>梯度中的每个元素可以告诉我们：相关方向上函数f的斜率</li><li>指向函数增长最快的方向</li><li>负梯度：函数下降最快的方向</li><li>给出了函数在当前点的一阶线性逼近</li></ul><p><strong>任意方向的斜率</strong> &#x3D; 这一点梯度 · 该点单位方向向量（点积）</p><h2 id="两种梯度计算方法"><a href="#两种梯度计算方法" class="headerlink" title="两种梯度计算方法"></a>两种梯度计算方法</h2><h3 id="数值梯度"><a href="#数值梯度" class="headerlink" title="数值梯度"></a>数值梯度</h3><ul><li><p>简单，有意义，有用的debug工具</p></li><li><p><strong>实践中</strong>：</p><p>深度学习就是计算函数梯度，用这些梯度迭代，更新<u>参数向量</u></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107225617301.png" alt="image-20220107225617301"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107225702013.png" alt="image-20220107225702013"></p><p>当网络很大时，有很多输入–》W维度很高，计算会很慢</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107230013735.png" alt="image-20220107230013735"></p><p>提前计算梯度的表达式，dW</p></li></ul><h3 id="解析梯度"><a href="#解析梯度" class="headerlink" title="解析梯度"></a>解析梯度</h3><ul><li>写下损失的<strong>表达式</strong>，计算机计算</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107231030542.png" alt="image-20220107231030542"></p><ul><li>实践中常用</li></ul><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107230222539.png" alt="image-20220107230222539"></p><h3 id="debug策略"><a href="#debug策略" class="headerlink" title="debug策略"></a>debug策略</h3><ul><li>使用数值梯度作为单元测试，来确保解析梯度是正确的</li><li>因为计算缓慢，需要减少问题的参数数量</li></ul><h2 id="梯度算法"><a href="#梯度算法" class="headerlink" title="梯度算法"></a>梯度算法</h2><ul><li>梯度：指向函数的最大增加方向</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107234303259.png" alt="image-20220107234303259"></p><p>初始化W为随机值</p><ul><li>W为真时：计算损失和梯度，向<strong>梯度相反的方向</strong>&#x3D;&#x3D;更新权重值&#x3D;&#x3D;</li><li>向梯度减小的方向前进，一直重复，最后网络将会收敛</li><li>step_size：步长，学习率，超参数，在那个方向前进多少距离<ul><li>需要设置的第一个超参数</li></ul></li></ul><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107235523994.png" alt="image-20220107235523994"></p><p>实际中使用：随机梯度下降</p><ul><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107235548667.png" alt="image-20220107235548667"></p></li><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107235601658.png" alt="image-20220107235601658"></p></li><li><p>成为minibatch（小批量），用此估算误差总和以及实际梯度</p></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220107235946894.png" alt="image-20220107235946894"></p><ul><li>按惯例，都取2的幂次方，32，64等</li><li>这个data_batch是随机的</li></ul><h1 id="特征转换"><a href="#特征转换" class="headerlink" title="特征转换"></a>特征转换</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108002406033.png" alt="image-20220108002406033"></p><ul><li>将直角坐标—》极坐标，从而使可以线性可分</li></ul><h1 id="线性分类器实际做法"><a href="#线性分类器实际做法" class="headerlink" title="线性分类器实际做法"></a>线性分类器实际做法</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108000840601.png" alt="image-20220108000840601"></p><p>计算图像不同特征表示的差异&#x2F;方向梯度的直方图</p><p>将整个特征连接在一起 </p><p>神经网络：</p><ul><li>并非提前记录特征</li><li>直接从数据中学习特征</li><li>在整个网络中训练所有的权重</li></ul><h1 id="8-4、介绍神经网络-反向传播"><a href="#8-4、介绍神经网络-反向传播" class="headerlink" title="8-4、介绍神经网络-反向传播"></a>8-4、介绍神经网络-反向传播</h1><h2 id="如何用函数f定义一个分类器？"><a href="#如何用函数f定义一个分类器？" class="headerlink" title="如何用函数f定义一个分类器？"></a>如何用函数f定义一个分类器？</h2><ul><li><p>函数f的<strong>参数</strong>是&#x3D;&#x3D;权重矩阵W&#x3D;&#x3D;</p></li><li><p><strong>输入数据x</strong>并对你想要分类的每个类别都输出一个对应的<strong>得分向量</strong></p></li><li><p><strong>损失函数</strong>：结合<strong>正则项</strong></p><ul><li><p>正则项：表示模型的复杂程度</p></li><li><p>为了更好地泛化，倾向于取简单的模型</p></li></ul></li></ul><h3 id="与最小损失对应的W参数？"><a href="#与最小损失对应的W参数？" class="headerlink" title="与最小损失对应的W参数？"></a>与最小损失对应的W参数？</h3><p>找到L在W方向上的梯度：</p><ul><li>最优化</li><li>沿着最陡的下降方向（<strong>梯度的负方向</strong>）</li></ul><h2 id="如何计算任意复杂函数的解析梯度？"><a href="#如何计算任意复杂函数的解析梯度？" class="headerlink" title="如何计算任意复杂函数的解析梯度？"></a>如何计算任意复杂函数的解析梯度？</h2><p><strong>计算图</strong> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108112206873.png" alt="image-20220108112206873"></p><p>输入x和W，矩阵乘法，计算loss，加正则项得L</p><p>一旦我们能用计算图来表示一个函数， </p><p>用反向传播技术递归地调用链式法则计算图中每个变量的梯度</p><h2 id="卷积神经网络CNN"><a href="#卷积神经网络CNN" class="headerlink" title="卷积神经网络CNN"></a>卷积神经网络CNN</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108112742211.png" alt="image-20220108112742211"></p><p>输入必须经过多层网络才能到达最后的loss</p><h2 id="反向传播如何工作"><a href="#反向传播如何工作" class="headerlink" title="反向传播如何工作"></a>反向传播如何工作</h2><h3 id="反向传播示例"><a href="#反向传播示例" class="headerlink" title="反向传播示例"></a>反向传播示例</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108114058740.png" alt="image-20220108114058740"></p><p>红色：反向计算得到的梯度值（多维导数）</p><p>【链式法则的递归调用】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108113821911.png" alt="image-20220108113821911"></p><p>即<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108113837193.png" alt="image-20220108113837193"></p><p>（αf即df）</p><h2 id="例：求一个函数所有变量的梯度L-x2F-x"><a href="#例：求一个函数所有变量的梯度L-x2F-x" class="headerlink" title="例：求一个函数所有变量的梯度L&#x2F;x"></a>例：求一个函数所有变量的梯度L&#x2F;x</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108144433113.png" alt="image-20220108144433113"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108144736783.png" alt="image-20220108144736783"></p><p>上游梯度 * 本地梯度</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108145623892.png" alt="image-20220108145623892"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108145750146.png" alt="image-20220108145750146"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108145843997.png" alt="image-20220108145843997"></p><p>【加法】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108150017081.png" alt="image-20220108150017081"></p><p>【乘法】</p><h1 id="sigmoid-gate"><a href="#sigmoid-gate" class="headerlink" title="sigmoid gate"></a>sigmoid gate</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108155822739.png" alt="image-20220108155822739"></p><p>【在计算图中】</p><p>把节点组合在一起变成一个sigmoid门：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108160324691.png" alt="image-20220108160324691"></p><ul><li>想求一个复杂的梯度时：写出计算图–》反向传播+链式法则</li></ul><p><strong>sigmoid求导数</strong></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108160823231.png" alt="image-20220108160823231"></p><p>👇</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108160943662.png" alt="image-20220108160943662"></p><p>∴本地梯度&#x3D;0.2，上游梯度&#x3D;1</p><p>相乘&#x3D;0.2</p><h1 id="将计算图中部分节点组合以简化计算"><a href="#将计算图中部分节点组合以简化计算" class="headerlink" title="将计算图中部分节点组合以简化计算"></a>将计算图中部分节点组合以简化计算</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108161105791.png" alt="image-20220108161105791"></p><ul><li>（）视作整体，变成一个sigmoid</li><li>完整计算图中，有一部分节点组合起来是sigmoid形式  </li><li>将此部分视为sigmoid门</li></ul><h1 id="加法门"><a href="#加法门" class="headerlink" title="加法门"></a><strong>加法门</strong></h1><p>分发和传递<strong>完全相同</strong>的梯度<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108162116724.png" alt="image-20220108162116724"></p><h1 id="max门"><a href="#max门" class="headerlink" title="max门"></a><strong>max门</strong></h1><p>本地梯度：一个是0，一个是1</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108162412914.png" alt="image-20220108162412914"></p><ul><li>想象成一个梯度路由器（选择了某个，不要另一个）</li><li>只有这个最大值影响到函数计算</li></ul><h1 id="乘法门"><a href="#乘法门" class="headerlink" title="乘法门"></a><strong>乘法门</strong></h1><p>本地梯度&#x3D;另一个变量的值</p><ul><li>一个梯度转换器&#x2F;尺度缩放器<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108162723865.png" alt="image-20220108162723865"></li></ul><h1 id="多分支"><a href="#多分支" class="headerlink" title="多分支"></a>多分支</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108162917503.png" alt="image-20220108162917503"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108163518381.png" alt="image-20220108163518381"></p><p>（我们没有更新这些权重的值</p><p>只是有了对这些变量的梯度）</p><p>👇</p><p>有了这些梯度</p><p>&#x3D;&#x3D;weigth&#x3D;step_size * gradient&#x3D;&#x3D;</p><p>实现<strong>更新权重</strong>（得到了需要的参数）</p><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108164900855.png" alt="image-20220108164900855"></p><h2 id="雅各比矩阵"><a href="#雅各比矩阵" class="headerlink" title="雅各比矩阵"></a>雅各比矩阵</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/15bfbeaea179e9a15f78e8a969c940e6.svg" alt="img"></p><p>每一行：输出yi对每一个输入做偏导</p><p>矩阵size：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108165322291.png" alt="image-20220108165322291"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108165409833.png" alt="image-20220108165409833"></p><p>此yacob会是一个对角矩阵</p><p>∵每一个输入只影响对应的一个输出</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/format,f_jpg.jpeg" alt="对角矩阵"></p><ul><li>不需要写出整个矩阵</li><li>计算出y对x的偏导，填入矩阵对角线</li></ul><h2 id="输入是向量"><a href="#输入是向量" class="headerlink" title="输入是向量"></a>输入是向量</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/format,f_jpg-16506189815311.jpeg" alt="矩阵乘法"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108170435246.png" alt="image-20220108170435246"></p><h2 id="正向"><a href="#正向" class="headerlink" title="正向"></a>正向</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108170528883.png" alt="image-20220108170528883"></p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p><strong>作为检查</strong>：</p><p>&#x3D;&#x3D;<strong>梯度向量size总是和原向量大小保持一致</strong>&#x3D;&#x3D;</p><p>（∵梯度向量中的每一个元素代表这个元素对最终函数影响的大小）</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108171805250.png" alt="image-20220108171805250"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108171707595.png" alt="image-20220108171707595"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108172401625.png" alt="image-20220108172401625"></p><p>（因为f是几个q的平方和）</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108172555341.png" alt="image-20220108172555341">q是2X1向量，有2个元素</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108172940801.png" alt="image-20220108172940801"></p><h1 id="前向、后向代码"><a href="#前向、后向代码" class="headerlink" title="前向、后向代码"></a>前向、后向代码</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108173145361.png" alt="image-20220108173145361"></p><p>前向：计算每个节点的输出</p><p>反向：计算梯度</p><ul><li>每个节点想象成门</li><li>如果有整个计算图，可以迭代得到所有图的正向传播</li></ul><h1 id="乘法门代码"><a href="#乘法门代码" class="headerlink" title="乘法门代码"></a>乘法门代码</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108173638594.png" alt="image-20220108173638594"></p><p>【所有数值都是标量】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108173929670.png" alt="image-20220108173929670"></p><p>forward：</p><ul><li><p>【<strong>需要缓存前向传播的数值</strong>】</p></li><li><p>∵反向传播要用它计算多次</p></li></ul><p>backward：</p><ul><li>需要使用已保存的self.y，与dz相乘</li></ul><h1 id="正向反向的应用"><a href="#正向反向的应用" class="headerlink" title="正向反向的应用"></a>正向反向的应用</h1><ul><li><p>许多深度学习框架和库</p></li><li><p>sigmoid</p></li><li><p>卷积</p></li><li><p>Argmax</p></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108174425022.png" alt="image-20220108174425022"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220108174643101.png" alt="image-20220108174643101"></p><ul><li>使用反向传播<strong>计算梯度</strong>：神经网络的核心技术</li></ul><h1 id="9-4-1、介绍神经网络"><a href="#9-4-1、介绍神经网络" class="headerlink" title="9-4.1、介绍神经网络"></a>9-4.1、介绍神经网络</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109112525004.png" alt="image-20220109112525004"></p><h1 id="神经网络："><a href="#神经网络：" class="headerlink" title="神经网络："></a><strong>神经网络：</strong></h1><ul><li>由简单函数组成的一组函数</li><li>在顶层堆叠在一起</li><li>用一种层次化的方式将它们堆叠起来</li><li>为了形成一个更复杂的非线性函数</li></ul><p><strong>【多阶段分层计算】</strong></p><ul><li>将多个线性层堆在顶层</li><li>和其他非线性函数结合</li></ul><p><strong>w1：</strong>各式范本的集合（红车、黄车……不只有10个）</p><p><strong>h：</strong>对w1这些范本的所有<strong>得分</strong>（第一层输出）</p><p>​即max（0，W1x）</p><p><strong>再上一层：</strong>将这些得分组合起来</p><p><strong>w2：</strong>h里所有向量的权重（类比：w1是x向量的权重）</p><p>​所有范本的<strong>加权</strong>，使在<strong>多个范本中权衡</strong>，得到特定分类的最后得分</p><p>【非线性通常出现在h以前】</p><h1 id="任意深度的神经网络"><a href="#任意深度的神经网络" class="headerlink" title="任意深度的神经网络"></a>任意深度的神经网络</h1><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109140522880.png" alt="image-20220109140522880"></p><h2 id="2层网络代码"><a href="#2层网络代码" class="headerlink" title="2层网络代码"></a>2层网络代码</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109140601579.png" alt="image-20220109140601579"></p><p>20行</p><p>运用前向、反向、链式法则计算梯度</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109140748264.png" alt="image-20220109140748264"></p><h1 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109140927403.png" alt="image-20220109140927403"></p><ul><li>cell body后 f 处得到激活函数</li><li>应用在神经元端部</li><li>得到的值作为输出</li><li>最后将值传输到和相关联的神经元</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109141959042.png" alt="image-20220109141959042"></p><p>【要传递下去的是放电率firing_rate】</p><h1 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h1><p><strong>线性整流函数</strong>（Rectified Linear Unit, <strong>ReLU</strong>），又称<strong>修正线性单元，</strong>是一种<a href="https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/382460">人工神经网络</a>中常用的&#x3D;&#x3D;激活函数&#x3D;&#x3D;（activation function），通常指代以<a href="https://baike.baidu.com/item/%E6%96%9C%E5%9D%A1%E5%87%BD%E6%95%B0">斜坡函数</a>及其变种为代表的<a href="https://baike.baidu.com/item/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0/16029251">非线性函数</a>。</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/format,f_jpg-16506189884995.jpeg" alt="ReLU 函数"></p><p>和神经元工作机制最为相似的一个激活函数</p><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109142637034.png" alt="image-20220109142637034"></p><h1 id="两层神经网络-x2F-单隐藏层神经网络"><a href="#两层神经网络-x2F-单隐藏层神经网络" class="headerlink" title="两层神经网络&#x2F;单隐藏层神经网络"></a>两层神经网络&#x2F;单隐藏层神经网络</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109142851135.png" alt="image-20220109142851135"></p><p><strong>全连接层</strong>：hidden layer和output layer</p><h1 id="三层神经网络-x2F-双隐藏层神经网络"><a href="#三层神经网络-x2F-双隐藏层神经网络" class="headerlink" title="三层神经网络&#x2F;双隐藏层神经网络"></a>三层神经网络&#x2F;双隐藏层神经网络</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109142945882.png" alt="image-20220109142945882"></p><p>每一个隐藏层是一个向量</p><p>一组神经元的集合</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109145537870.png" alt="image-20220109145537870"></p><p>利用<strong>矩阵乘法</strong> np.dot（）计算神经元的值</p><p>通过一次矩阵乘法，得出了该层所有神经元输出结果</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109145845261.png" alt="image-20220109145845261"></p><p>f：使用了sigmoid函数（x）</p><blockquote><p>在<em>Python</em>中有两种函数,一种是def定义的函数,另一种是<em><strong>lambda函数</strong></em>,也就是大家常说的<strong>匿名函数</strong>。今天我就和大家聊聊<em>lambda函数</em>,在<em>Python</em>编程中,大家习惯将其称为表达式。</p></blockquote><p>x：numpy的randn函数，3X1随机矩阵</p><p>h1：w1和第一个输入x做矩阵乘法，+b1，sigmoid函数</p><p>h2：w2和第二个输入h2做矩阵乘法，+b2，sigmoid函数</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>神经网络：很多层线性层，在层间添加非线性</p><p> 通过学习任务所需的中间模板（红车、黄车、各种颜色车）</p><p>把中间特征组合起来得到<strong>某一类别的最终评分函数</strong></p><h1 id="10-5、卷积神经网络历史"><a href="#10-5、卷积神经网络历史" class="headerlink" title="10-5、卷积神经网络历史"></a>10-5、卷积神经网络历史</h1><blockquote><p>与神经网络类似，但需要训练卷积层</p><p>因为：卷积层更能保留输入的空间结构</p></blockquote><h1 id="11-5、1-卷积神经网络-卷积和池化"><a href="#11-5、1-卷积神经网络-卷积和池化" class="headerlink" title="11-5、1 卷积神经网络-卷积和池化"></a>11-5、1 卷积神经网络-卷积和池化</h1><h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109154218545.png" alt="image-20220109154218545"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109154045297.png" alt="image-20220109154045297"></p><p>W与输入x进行点积运算，从而得到一个数字</p><p>（神经元的值）</p><h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109154722465.png" alt="image-20220109154722465"></p><p><strong>与全连接层的区别</strong>：</p><p>可以保全空间结构</p><p>不用展开成长向量，可以保持图片原本的三维输入结构</p><p> <strong>权重：</strong></p><p>小卷积核</p><p>把卷积核在整个图像上滑动</p><p>计算出每一个空间定位时的&#x3D;&#x3D;<strong>点积</strong>结果&#x3D;&#x3D;</p><ul><li>原始图像：三维，三个通道，RGB</li><li>卷积核：也是立体，遍布三个通道</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109155147784.png" alt="image-20220109155147784"></p><p>一次卷积共计算：5 * 5 * 3次+bias偏置</p><h2 id="卷积核w的基本方法："><a href="#卷积核w的基本方法：" class="headerlink" title="卷积核w的基本方法："></a>卷积核w的基本方法：</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109155241286.png" alt="image-20220109155241286"></p><p>（将卷积核展开与w做向量点积&#x3D;卷积核每个元素与w对应相乘）</p><h2 id="如何滑动卷积核？"><a href="#如何滑动卷积核？" class="headerlink" title="如何滑动卷积核？"></a>如何滑动卷积核？</h2><p>从左上角开始</p><p>遍历输入的所有像素点</p><p>在每个位置，都进行点积运算</p><p>每一次运算都会在<strong>输出激活映射</strong>中产生一个值</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109160343151.png" alt="image-20220109160343151"></p><p>【维度发生了变化】</p><p>还可以一次滑动两个像素值</p><p>不同尺寸的输出取决于滑动方法</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109160800344.png" alt="image-20220109160800344"></p><h3 id="使用多个卷积核："><a href="#使用多个卷积核：" class="headerlink" title="使用多个卷积核："></a>使用多个卷积核：</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109160828536.png" alt="image-20220109160828536"></p><p>6个卷积核，每个对应不同激活层</p><h2 id="卷积神经网络中我们如何使用这些卷积层？"><a href="#卷积神经网络中我们如何使用这些卷积层？" class="headerlink" title="卷积神经网络中我们如何使用这些卷积层？"></a>卷积神经网络中我们如何使用这些卷积层？</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109161124112.png" alt="image-20220109161124112"></p><p>经过多个卷积核得到下一层……</p><ul><li><p>采用多个卷积核</p></li><li><p>每一个卷积核会产生一个激活映射</p></li><li><p>前面的卷积核：低阶的图像特征</p><ul><li>边缘特征</li></ul></li><li><p>越往后的卷积核：越复杂的图像特征</p><ul><li>边角、斑点</li></ul></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109163309334.png" alt="image-20220109163309334"></p><p>【堆叠在一起的层：从简单到复杂的特征序列】</p><p>激活函数映射的每一个都对应滑动这些卷积核的一个输出以及这些卷积核产生输出的位置</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109165102416.png" alt="image-20220109165102416"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109165336460.png" alt="image-20220109165336460"></p><p>一张图片通过：卷积层、非线性层relu、池化层（降低激活映射的采样尺寸）、全连接层连接所有的卷积输出，获得一个最终的分值函数</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109165921956.png" alt="image-20220109165921956"></p><p>7X7，步长为2，得到3X3输出</p><p>​步长为1，得到5X5输出</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109170048866.png" alt="image-20220109170048866"></p><p>步长为3，会导致不对称的输出，不拟合</p><h2 id="输出尺寸公式"><a href="#输出尺寸公式" class="headerlink" title="输出尺寸公式"></a>输出尺寸公式</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109170846639.png" alt="image-20220109170846639"></p><p>有哪些可用的步长：即这个方程的解（整数）</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109171432117.png" alt="image-20220109171432117"></p><p><strong>填补0</strong>：可以得到原始图像size 的输出size</p><p>【目的：保持原尺寸输出】</p><p> （∵缩小尺寸会丢失一些信息）</p><p>实际输出图像&#x3D;7X7X使用的卷积核数目</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109171825786.png" alt="image-20220109171825786"></p><p>深度：使用的卷积核的数量</p><p> 每个卷积核将产生一个1X7X7的激活映射图输出</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109172934017.png" alt="image-20220109172934017"></p><p>32x32x10</p><p>（因为卷积和做点乘时有深度，3已经算在里面了）</p><p>卷积核5x5，隐含了深度</p><p>👇</p><h3 id="参数数量是多少？"><a href="#参数数量是多少？" class="headerlink" title="参数数量是多少？"></a>参数数量是多少？</h3><p>5x5x3x10&#x3D;750</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109173230485.png" alt="image-20220109173230485"></p><h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109173440004.png" alt="image-20220109173440004"></p><h2 id="1x1卷积"><a href="#1x1卷积" class="headerlink" title="1x1卷积"></a>1x1卷积</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109173808802.png" alt="image-20220109173808802"></p><p>1<em>1的卷积作用就是<strong>改变深度</strong>，而且*<em>可以在后面加激活函数</em></em></p><h2 id="torch框架代码"><a href="#torch框架代码" class="headerlink" title="torch框架代码"></a>torch框架代码</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109174020051.png" alt="image-20220109174020051"></p><h2 id="caffe框架代码"><a href="#caffe框架代码" class="headerlink" title="caffe框架代码"></a>caffe框架代码</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109174149761.png" alt="image-20220109174149761"></p><h2 id="如何确定步长"><a href="#如何确定步长" class="headerlink" title="如何确定步长"></a>如何确定步长</h2><p>与分辨率有关</p><p>步长过大：</p><p>缩小了激活映射的尺寸</p><p>降采样，一种池化处理，但优于池化</p><h1 id="12-5、2-卷积神经网络-视觉之外的卷积神经网络"><a href="#12-5、2-卷积神经网络-视觉之外的卷积神经网络" class="headerlink" title="12-5、2 卷积神经网络-视觉之外的卷积神经网络"></a>12-5、2 卷积神经网络-视觉之外的卷积神经网络</h1><img src="CS231%E7%AC%94%E8%AE%B0.assets/image-20220109180656125.png" alt="image-20220109180656125" style="zoom:50%;" /><p>卷积层：感受野</p><p>卷积核的数量：深度</p><p>e.g.5个卷积核，得到深度为5的输出</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109180848727.png" alt="image-20220109180848727"></p><p>5个卷积核作用与统一区域，却又不同的功用</p><h1 id="对比全连接层："><a href="#对比全连接层：" class="headerlink" title="对比全连接层："></a>对比全连接层：</h1><p>神经元与全体输入量都发生关联</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109181125987.png" alt="image-20220109181125987"></p><p>卷积核：只与图像的一个局部区域发生关联</p><h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>让所生成的表示更小更容易控制</p><p>降低采样率</p><p>为了最后有更少的参数</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109182113443.png" alt="image-20220109182113443"></p><p>仅在平面上池化，深度不变</p><h2 id="最大池化法"><a href="#最大池化法" class="headerlink" title="最大池化法"></a>最大池化法</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109182316658.png" alt="image-20220109182316658"></p><p>滑动区域</p><p>步长为2，使得不会互相重叠（处理不同区域）</p><p>提取所在区域的输入的<strong>最大值</strong></p><p>（取得那些比较显著的像素\检测边缘、噪声）</p><p> <strong>改变步长和池化都可以降采样</strong></p><h1 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109182950832.png" alt="image-20220109182950832"></p><ul><li>输入层WXHXD</li><li>设置超参数卷积核尺寸、池化的空间范围、步长</li><li>计算输出WXHXD<ul><li>输出尺寸公式</li><li>深度不变</li></ul></li></ul><p><strong>一般不在池化层做填零</strong></p><p>因为池化层只做降采样</p><h2 id="池化层的常见设置："><a href="#池化层的常见设置：" class="headerlink" title="池化层的常见设置："></a>池化层的常见设置：</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109183457844.png" alt="image-20220109183457844"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109183502940.png" alt="image-20220109183502940"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109184220236.png" alt="image-20220109184220236"></p><ul><li><p>每一列都是激活映射的输出</p></li><li><p>除了第一次，每一次输入都是上一次输出</p></li><li><p>最后的输出拉长成<strong>一维向量</strong>，与<strong>朴素神经网络连接</strong>，得到卷积网络最后的<strong>全连接层</strong></p></li></ul><p>（最后得到像之前分值一样的输出）</p><p>&#x3D;&#x3D;相当于说<strong>卷积层</strong>是<strong>提取图片的特征</strong>，然后将特征送到<strong>全连接层</strong>来做<strong>分类</strong>&#x3D;&#x3D;</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220109185257700.png" alt="image-20220109185257700"></p><h1 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h1><p>一个趋势：</p><ul><li><p>小尺寸卷积核和大深度的网络结构</p></li><li><p>完全弃用池化和全连接层</p></li><li><p>保留卷积层</p></li><li><p>形成非常深的卷积网络</p></li></ul><h1 id="13-6、训练神经网络-激活函数"><a href="#13-6、训练神经网络-激活函数" class="headerlink" title="13-6、训练神经网络-激活函数"></a>13-6、训练神经网络-激活函数</h1><p>神经网络：</p><ul><li>任何函数都可以用计算图表示</li><li>一种计算图，包含若干个线性层，而层与层之间通过非线性函数进行连接实现堆叠</li></ul><p>卷积神经网络：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110115923156.png" alt="image-20220110115923156"></p><ul><li>一种特殊的网络</li><li>使用卷积层贯穿整个网络的层次结构中</li><li>保持输入的空间结构</li><li>经过卷积、降采样、全连接层</li><li><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110115838424.png" alt="image-20220110115838424"></li></ul><p>需要实现：</p><ul><li>学习滤波器权重或参数的值</li><li>然后通过优化来更新网络参数</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110120724131.png" alt="image-20220110120724131"></p><p>  在损失区域找到最低点，实现最优化</p><h2 id="最小批量数据："><a href="#最小批量数据：" class="headerlink" title="最小批量数据："></a>最小批量数据：</h2><ul><li>对数据连续的批量抽样</li><li>通过计算图或神经网络将数据进行<strong>正向传播</strong></li><li>得到<strong>loss</strong></li><li><strong>反向传播</strong>计算梯度（找到最小的loss）</li><li>（根据此loss）更新<strong>参数</strong>或权重</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110121246142.png" alt="image-20220110121246142"></p><h2 id="1-激活函数"><a href="#1-激活函数" class="headerlink" title="1 激活函数"></a>1 激活函数</h2><p>任意特定层如何产生输出：</p><ul><li>输入数据</li><li>在<strong>全连接层</strong>或<strong>卷积层</strong>，输入x权重值</li><li>输入一个<strong>激活函数&#x2F;非线性单元</strong>（计算图中一个结点）</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110122534049.png" alt="image-20220110122534049"></p><h3 id="1-1-sigmoid的几个问题"><a href="#1-1-sigmoid的几个问题" class="headerlink" title="1.1 sigmoid的几个问题"></a>1.1 sigmoid的几个问题</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110123214507.png" alt="image-20220110123214507"></p><h4 id="1-梯度消失"><a href="#1-梯度消失" class="headerlink" title="1 梯度消失"></a>1 梯度消失</h4><p>输入的x太大或太小：饱和的神经会会造成梯度消失：</p><ul><li>x&#x3D;10或-10，导数为0，反向传播，经过链式法则，梯度为0，无法得到梯度流的反馈<ul><li>阻止梯度的传递</li></ul></li><li>x&#x3D;0，一个比较好的梯度</li></ul><h4 id="2-sigmoid输出非0中心的函数"><a href="#2-sigmoid输出非0中心的函数" class="headerlink" title="2 sigmoid输出非0中心的函数"></a>2 sigmoid输出非0中心的函数</h4><h4 id=""><a href="#" class="headerlink" title=""></a><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110142951250.png" alt="image-20220110142951250"></h4><p>梯度更新的效率会非常低</p><h4 id="3-使用了指数函数，计算代价有点高"><a href="#3-使用了指数函数，计算代价有点高" class="headerlink" title="3 使用了指数函数，计算代价有点高"></a>3 使用了指数函数，计算代价有点高</h4><p>  但不是主要问题，因为进行卷积时和点乘的计算代价会更大！<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110143040655.png" alt="image-20220110143040655"></p><p>函数<a href="https://so.csdn.net/so/search?q=sigmoid">sigmoid</a>在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</p><h3 id="1-2-tanh"><a href="#1-2-tanh" class="headerlink" title="1.2 tanh"></a>1.2 tanh</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110143208098.png" alt="image-20220110143208098"></p><h4 id="1-梯度消失-1"><a href="#1-梯度消失-1" class="headerlink" title="1 梯度消失"></a>1 梯度消失</h4><ul><li>阻止梯度的传递</li></ul><h4 id="2-以0为中心"><a href="#2-以0为中心" class="headerlink" title="2 以0为中心"></a>2 以0为中心</h4><h4 id="3-范围被压缩在-1-1"><a href="#3-范围被压缩在-1-1" class="headerlink" title="3 范围被压缩在 [-1,1]"></a>3 范围被压缩在 [-1,1]</h4><h3 id="1-3-ReLU"><a href="#1-3-ReLU" class="headerlink" title="1.3 ReLU"></a>1.3 ReLU</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110143451286.png" alt="image-20220110143451286"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110143500662.png" alt="image-20220110143500662"></p><ul><li>在输入上按元素进行操作</li></ul><h4 id="1-正半轴无梯度消失-x2F-无饱和现象"><a href="#1-正半轴无梯度消失-x2F-无饱和现象" class="headerlink" title="1 正半轴无梯度消失&#x2F;无饱和现象"></a>1 正半轴无梯度消失&#x2F;无饱和现象</h4><h4 id="2-计算成本比其他低"><a href="#2-计算成本比其他低" class="headerlink" title="2 计算成本比其他低"></a>2 计算成本比其他低</h4><p>AlexNet是第一个在imagenet和大规模数据上表现出色的重要的卷积神经网络，使用了relu</p><h4 id="3-问题：不以0为中心"><a href="#3-问题：不以0为中心" class="headerlink" title="3 问题：不以0为中心"></a>3 问题：不以0为中心</h4><h4 id="4-问题：负半轴梯度饱和"><a href="#4-问题：负半轴梯度饱和" class="headerlink" title="4 问题：负半轴梯度饱和"></a>4 问题：负半轴梯度饱和</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110144222597.png" alt="image-20220110144222597"></p><p>-10：0</p><p>0：不确定，实践中可以取0</p><p>10：没问题</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110144840967.png" alt="image-20220110144840967"></p><p>data cloud：训练数据</p><p>如果权重设置的非常差，恰巧不在数据云里：</p><ul><li>出现了deadrelu（即经过relu激活函数输出为0）</li><li>无法得到一个激活神经元的数据输入</li><li>也不会有一个合适的梯度传回</li><li><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220110145122796.png" alt="image-20220110145122796"></li></ul><p>学习率太高（步长）：</p><ul><li>从一个ReLU函数开始</li><li>权值会不断更新（&#x3D;rate x 梯度）</li><li>relu单元会被数据的多样性所淘汰</li><li></li></ul><p><strong>权值矩阵不再更新</strong>的时候，要不就是到达<strong>最小点</strong>，要不就是<strong>挂了</strong></p><p>实际应用中：</p><p>使用较小的正偏置来初始化ReLU</p><p>以增加它在初始化时被激活的可能性</p><p>并获得一些更新</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111105203742.png" alt="image-20220111105203742"></p><h3 id="1-4-Leaky-ReLU"><a href="#1-4-Leaky-ReLU" class="headerlink" title="1.4 Leaky ReLU"></a>1.4 Leaky ReLU</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111105231148.png" alt="image-20220111105231148"></p><p>不是和0比较，和0.01x比较</p><h3 id="参数整流器-PReLU"><a href="#参数整流器-PReLU" class="headerlink" title="参数整流器 PReLU"></a>参数整流器 PReLU</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111105320484.png" alt="image-20220111105320484"></p><p>α：当作一个可以反向传播和学习的参数</p><h3 id="指数线性单元-ELU"><a href="#指数线性单元-ELU" class="headerlink" title="指数线性单元 ELU"></a>指数线性单元 ELU</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111105450850.png" alt="image-20220111105450850"></p><p>负饱和机制</p><p>输出均值&#x3D;0</p><p>比relus更饱和的行为</p><p><em>实际拟合过程中哪个激活函数好还是要通过结果说话</em></p><h2 id="最大输出神经元"><a href="#最大输出神经元" class="headerlink" title="最大输出神经元"></a>最大输出神经元</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111111427202.png" alt="image-20220111111427202"></p><p>泛化了relu和leaky relu</p><p>因为只是提取了这两个线性函数的最大值</p><p>不会饱和、消失</p><p>问题：每个神经元的参数数量翻倍</p><p>w1–&gt;w1、w2</p><h2 id="实际中选择激活函数："><a href="#实际中选择激活函数：" class="headerlink" title="实际中选择激活函数："></a>实际中选择激活函数：</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111111701527.png" alt="image-20220111111701527"></p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><ul><li>均值化</li><li>归一化</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111112142563.png" alt="image-20220111112142563"></p><p>为什么0均值化：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111112424982.png" alt="image-20220111112424982"></p><p>当输入数据全正时：输出梯度也全正？？</p><p>为什么归一化：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111112513410.png" alt="image-20220111112513410"></p><p>实际：</p><ul><li>0均值化√</li><li>归一化×<ul><li>因为图像中每个位置已经得到了<strong>相对可比较</strong>的范围与分布</li><li>一般机器学习会有<strong>差别很大</strong>的特征</li></ul></li><li>训练和测试过程都有数据预处理</li></ul><blockquote><p>我的理解是，因为到时候会有梯度下降，数据分布在0周围，那对w求梯度就不会恒正或者恒负，不会走不到二四象限</p></blockquote><blockquote><p>不做0标准化处理的话，你的数据会偏向某一侧，会导致你训练的过程中W可能不是沿着最优的方向前进</p></blockquote><p>零中心化是指**&#x3D;&#x3D;输入变量减去它的均值&#x3D;&#x3D;**</p><ul><li>那么此时（以二维数据为例）数据会聚集在原点的的周围</li><li>好处是能<strong>加快调参的速度</strong>；</li><li>还能增加<strong>基向量的正交性</strong></li></ul><h2 id="中心化"><a href="#中心化" class="headerlink" title="中心化"></a>中心化</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111114003688.png" alt="image-20220111114003688"></p><h3 id="输入-整张图像均值"><a href="#输入-整张图像均值" class="headerlink" title="输入-整张图像均值"></a>输入-整张图像均值</h3><h3 id="输入-单通道均值"><a href="#输入-单通道均值" class="headerlink" title="输入-单通道均值"></a>输入-单通道均值</h3><ul><li>和上没什么区别</li><li>更容易传送和处理</li></ul><h4 id="均值如何得到？"><a href="#均值如何得到？" class="headerlink" title="均值如何得到？"></a>均值如何得到？</h4><p>从你所有的训练图像中得到</p><p>拿到所有的训练图像，计算它们的均值</p><p>不用分批（per batch）</p><p>**&#x3D;&#x3D;网络第一层的输入&#x3D;&#x3D;**：零均值化！</p><p>用深度网络时</p><h2 id="初始化网络权值"><a href="#初始化网络权值" class="headerlink" title="初始化网络权值"></a>初始化网络权值</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111115527697.png" alt="image-20220111115527697"></p><h3 id="0初始化？"><a href="#0初始化？" class="headerlink" title="0初始化？"></a>0初始化？</h3><p>开始设置的参数不同，每个神经元的输出不同吧，后面神经元的梯度可能和前面神经元的输出有关，应该也会不同</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111120159348.png" alt="image-20220111120159348"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111120418725.png" alt="image-20220111120418725"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111120502326.png" alt="image-20220111120502326"></p><p>均值总是在0附近</p><p>这里讲的不是很清楚，建议好好研究一下<strong>前面的反向传播</strong>，尤其是关于<strong>向量和矩阵</strong>的，然后仔细做一下作业1，然后可能就能听懂了</p><p>简单来讲就是正向传播时，由于权重<strong>W设置的太小</strong>，一直乘W导致<strong>每一层的输入x越来越小</strong>，而反向传播时，梯度dW又是取决于x的大小的，<strong>x太小则dW太小，权重不更新</strong></p><p>正传w1w2w3x导致<strong>x变小</strong>，反传链式求导dw&#x3D;x1x2x3导致<strong>梯度变小</strong></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111121805881.png" alt="image-20220111121805881"></p><p>【层初始化代码】</p><p><strong>激活函数</strong>用于将一层的结果处理一下转交下一层。假设某层结果已经经过了tanh激活的处理变为输入x，那么<strong>x∈(-1, 1)<strong>，使得梯度在反向传播时乘上</strong>该层输入x变小</strong>（∵小数），如果层数很多，那么<strong>梯度最终会消失</strong>，饱和，权重不再更新</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111122105528.png" alt="image-20220111122105528"></p><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p>Softmax计算简单，效果显著，非常好用</p><p><strong>softmax用于多分类过程中</strong>，它将多个<a href="https://www.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E5%85%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:240869755%7D">神经元</a>的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/sum_j%7Be%5E%7BV_j%7D%7D%7D.svg+xml" alt="[公式]"></p><p>loss：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/sum_j%7Be%5Ej%7D%7D" alt="[公式]">.svg+xml)</p><h2 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h2><p>初始化网络</p><ul><li>权重太小：网络崩溃</li><li>权重太大：网络饱和</li></ul><img src="CS231%E7%AC%94%E8%AE%B0.assets/image-20220111122205543.png" alt="image-20220111122205543" style="zoom:200%;" /><p>输入少：➗较小数&#x3D;较大权重</p><p>有少量输入，用每一个输入值✖权重</p><p> 输入多：➗较大数&#x3D;较小权重</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220111122645330.png" alt="image-20220111122645330"></p><h1 id="14-6、1训练神经网络（上）-批量归一化"><a href="#14-6、1训练神经网络（上）-批量归一化" class="headerlink" title="14-6、1训练神经网络（上）-批量归一化"></a>14-6、1训练神经网络（上）-批量归一化</h1><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112150004214.png" alt="image-20220112150004214"></p><ul><li>在我们想要的高斯范围内保持激活</li><li>在<strong>训练开始</strong>时才设置这个值</li><li>而不是在权重初始化时</li><li>以便能够在<strong>每一层都有很好的单位高斯分布</strong></li></ul><p>高斯分布：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/format,f_jpg-16506190215909.jpeg" alt="正态分布"></p><p>我觉得是这样：<strong>权重有个初始化</strong>，在迭代过程中，<strong>根据梯度不断更新权重</strong></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112141503143.png" alt="image-20220112141503143"></p><ul><li>在FC或者卷积层后插入BN</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112143019615.png" alt="image-20220112143019615"></p><p>网络可以学习：</p><ul><li>y&#x3D;方差</li><li>β&#x3D;均值</li><li>都是通过学习得到的参数</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112143329100.png" alt="image-20220112143329100"></p><h2 id="BN思想"><a href="#BN思想" class="headerlink" title="BN思想"></a>BN思想</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112143450829.png" alt="image-20220112143450829"></p><p>输入：</p><p>计算小批量均值（对每个输入的小批量都做这个操作）</p><p>计算方差</p><p>通过均值和方差进行归一化</p><p> 额外的缩放和平移因子</p><p>从而改进了整个网络的梯度流</p><p>还具有更强的性能</p><p>能够在更广范围的学习率和不同初始值下工作</p><p>训练更容易</p><p>也是正则化的一种方法</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112150202497.png" alt="image-20220112150202497"></p><h2 id="Q-高斯后是否会影响网络结构？"><a href="#Q-高斯后是否会影响网络结构？" class="headerlink" title="Q:高斯后是否会影响网络结构？"></a>Q:高斯后是否会影响网络结构？</h2><p>不会，只是将数据缩放到一个能很好运行的区域</p><p>对每一层的输入进行归一化</p><p>（层间的输入大约相近与高斯分布）</p><p>在这个过程中没有改变权重</p><h1 id="如何监视训练"><a href="#如何监视训练" class="headerlink" title="如何监视训练"></a>如何监视训练</h1><p>在训练过程调整超参数</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112152809620.png" alt="image-20220112152809620"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112152819096.png" alt="image-20220112152819096"></p><p>有50个神经元的隐藏层</p><h3 id="step3-网络初始化："><a href="#step3-网络初始化：" class="headerlink" title="step3 网络初始化："></a>step3 网络初始化：</h3><p>检查loss是否合理</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112152946807.png" alt="image-20220112152946807"></p><p>网络进行前向传播</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112153202123.png" alt="image-20220112153202123"></p><p>初始损失：</p><p>loss，grad&#x3D;……（，，0.0）–》2.3</p><p>因为没有加正则化项，loss是数据的损失值</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112153520477.png" alt="image-20220112153520477"></p><p>添加额外的正则化项 le3：</p><p>loss上升了（符合预期）</p><blockquote><p>正则化：<strong>正则化是为了防止过拟合</strong></p><p><strong>规则化就是说给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀</strong></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/v2-62b74afd353d7fdaf9c8d6b20d38d3e1_720w.jpg" alt="img">红色过拟合，没必要这么多特征</p></blockquote><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112153741150.png" alt="image-20220112153741150"></p><h3 id="step-4-开始训练"><a href="#step-4-开始训练" class="headerlink" title="step 4 开始训练"></a>step 4 开始训练</h3><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112160734244.png" alt="image-20220112160734244"></p><p>先用一批小数据集</p><p>👆以上，都是完整性检查👆</p><h4 id="先来一个小的学习率："><a href="#先来一个小的学习率：" class="headerlink" title="先来一个小的学习率："></a>先来一个小的学习率：</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112161206697.png" alt="image-20220112161206697"></p><p>加上一个小的正则化项，调整学习率</p><p>loss基本不变：</p><ul><li>∵设置的<strong>学习率太低</strong>**</li><li>此时梯度<strong>更新就会很小</strong></li><li>cost也一样</li></ul><p>train：训练集</p><p>val：验证集</p><h3 id="Q-loss基本不变，为什么准确率提高了？"><a href="#Q-loss基本不变，为什么准确率提高了？" class="headerlink" title="Q:loss基本不变，为什么准确率提高了？"></a>Q:loss基本不变，为什么准确率提高了？</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112161458966.png" alt="image-20220112161458966"></p><p>A：虽然分布依然很分散</p><p>因此我们的损失项loss很接近</p><p>但是我们把这些所有的分布都朝着正确的方向在轻微的移动，权重参数在朝着正确的方向改变，现在准确率可能发生突变，因为正选取最大的准确率，所以准确率会得到一个很大的提升</p><blockquote><p>比如说选三类，原来概率都是三分之一，那么现在正确的那一类加了0.1，loss变化不大，但能选对了</p></blockquote><h4 id="一个大的学习率"><a href="#一个大的学习率" class="headerlink" title="一个大的学习率"></a>一个大的学习率</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112162015162.png" alt="image-20220112162015162"></p><p>cost&#x3D;nan，太大了！（即loss）</p><h4 id="一般来说设置学习率："><a href="#一般来说设置学习率：" class="headerlink" title="一般来说设置学习率："></a>一般来说设置学习率：</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112162133994.png" alt="image-20220112162133994"></p><p>根据loss调整大小</p><h1 id="超参数选择（超参数优化）"><a href="#超参数选择（超参数优化）" class="headerlink" title="超参数选择（超参数优化）"></a>超参数选择（超参数优化）</h1><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>在训练集上训练—&gt;验证集上验证</p><blockquote><p>epoth 使用训练集的全部数据进行一次完整的训练，称为“一代训练”</p></blockquote><p>选择较分散的数据，用几个epoch的迭代去学习，哪些超参数有效，做出调整</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112163004224.png" alt="image-20220112163004224"></p><p>采用对数来优化，效果会更好</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112163306520.png" alt="image-20220112163306520"></p><p>因为根据lr和val_acc可知，当lr在e-04左右，acc最高，所以调整lr在10-4~10-0区间</p><p>？？？<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112164110594.png" alt="image-20220112164110594"></p><h2 id="需要调整的超参数"><a href="#需要调整的超参数" class="headerlink" title="需要调整的超参数"></a>需要调整的超参数</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112164244910.png" alt="image-20220112164244910"></p><p>网络结构</p><p>学习率、衰减表、更新类型、正则化</p><p>隐藏层数量、深度</p><h2 id="一个好的学习率长啥样"><a href="#一个好的学习率长啥样" class="headerlink" title="一个好的学习率长啥样"></a>一个好的学习率长啥样</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112164701701.png" alt="image-20220112164701701"></p><ol><li>损失爆炸：lr太高</li><li>有突变：lr太高</li><li>过于线性：lr太低</li><li>好的：相对陡峭，又连续下降</li></ol><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112165007867.png" alt="image-20220112165007867"></p><p>初始，梯度平缓，什么也没学到（loss没变，即没什么改进）</p><p>某一点后：开始调节</p><h2 id="可视化精度"><a href="#可视化精度" class="headerlink" title="可视化精度"></a>可视化精度</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112165250910.png" alt="image-20220112165250910"></p><p>训练精度和验证精度间：</p><ul><li>big gap：过拟合，+正则项权重</li><li>no gap：没有过拟合，增加模型容量，</li></ul><h2 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112165603339.png" alt="image-20220112165603339"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112165611183.png" alt="image-20220112165611183"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220112165650244.png" alt="image-20220112165650244"></p><h1 id="15-7、训练-更好地优化"><a href="#15-7、训练-更好地优化" class="headerlink" title="15-7、训练-更好地优化"></a>15-7、训练-更好地优化</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113110306891.png" alt="image-20220113110306891"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113110647476.png" alt="image-20220113110647476"></p><p>网络深度↑，初始w更重要</p><p>因为是不断乘以w</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113110905231.png" alt="image-20220113110905231"></p><p>数据预处理：</p><ul><li>如果不归一化、正则化，稍微转动就会破坏分类器</li><li>loss会对w很敏感</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113111427017.png" alt="image-20220113111427017"></p><p>BN:</p><ul><li>在神经网络中额外加入一层</li><li>以获得中间的激活值</li><li>均值为0，方差为1</li></ul><p>用小批量数据计算平均值、方差，再对整个数据及逆行归一化</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113113609529.png" alt="image-20220113113609529"></p><p>x：训练次数</p><p>train：准确率不断上升</p><p>val：不再变化：过拟合了！—》加入正则化</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113113803750.png" alt="image-20220113113803750"></p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><h2 id="最简单的优化算法：随机梯度下降-SGD"><a href="#最简单的优化算法：随机梯度下降-SGD" class="headerlink" title="最简单的优化算法：随机梯度下降 SGD"></a>最简单的优化算法：随机梯度下降 SGD</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113121118017.png" alt="image-20220113121118017"></p><ul><li>评估小批数据中损失的梯度</li><li>想梯度为负的方向更新参数向量</li></ul><h3 id="problem-with-SGD"><a href="#problem-with-SGD" class="headerlink" title="problem with SGD"></a>problem with SGD</h3><h2 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113140323217.png" alt="image-20220113140323217"></p><ul><li>保持一个不随时间变化的速度<ul><li>初始化为0</li></ul></li><li>将速度估计添加到这个这个速度上</li><li>在这个速度的方向上步进  vx</li><li>而不是在梯度的方向上步进 dx<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113153859409.png" alt="image-20220113153859409"></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113153710181.png" alt="image-20220113153710181"></p><p> ρ：摩擦系数对速度进行衰减</p><p>意义：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113153959943.png" alt="image-20220113153959943"></p><p>有了速度，可以越过局部最优点</p><p>最近梯度平均的平滑移动</p><p>并且在梯度上有一个能够及时回来的指数衰减权重</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113160426481.png" alt="image-20220113160426481"></p><p> 两个轴，两个维度方向上，</p><p>梯度很大：</p><ul><li>会➗一个很大的数的平方，降低了这个方向上的训练进度</li></ul><p>梯度很小：</p><ul><li>与上相反</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113160817989.png" alt="image-20220113160817989"></p><h3 id="2-步长会越来越小（即x）"><a href="#2-步长会越来越小（即x）" class="headerlink" title="2 步长会越来越小（即x）"></a>2 步长会越来越小（即x）</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113161036134.png" alt="image-20220113161036134"></p><h2 id="改进：RMSProp"><a href="#改进：RMSProp" class="headerlink" title="改进：RMSProp"></a>改进：RMSProp</h2><p>梯度平方估计被衰减</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113161142673.png" alt="image-20220113161142673"></p><ul><li>给梯度的平方加上动量</li><li>而不是给梯度本身 </li><li>x（步长）在一个维度上训练会加快，在另一个维度上训练减慢</li></ul><p>训练可能会变慢</p><h4 id="Q：1e-7是干嘛的"><a href="#Q：1e-7是干嘛的" class="headerlink" title="Q：1e-7是干嘛的"></a>Q：1e-7是干嘛的<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113164248551.png" alt="image-20220113164248551"></h4><p>保证不是➗0，∴＋一个很小的常数</p><h4 id="如何避免开始时步长很大？"><a href="#如何避免开始时步长很大？" class="headerlink" title="如何避免开始时步长很大？"></a>如何避免开始时步长很大？</h4><h2 id="结合-动量-amp-AdaGrad-amp-RMSProp的方法：Adam"><a href="#结合-动量-amp-AdaGrad-amp-RMSProp的方法：Adam" class="headerlink" title="结合+动量&amp;AdaGrad&amp;RMSProp的方法：Adam"></a>结合+动量&amp;AdaGrad&amp;RMSProp的方法：Adam</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113162304261.png" alt="image-20220113162304261"></p><h3 id="1-更新第一动量和第二动量"><a href="#1-更新第一动量和第二动量" class="headerlink" title="1 更新第一动量和第二动量"></a>1 更新第一动量和第二动量</h3><p>第一动量 &#x3D; <strong>梯度</strong>的加权和</p><p>第二动量：<strong>梯度平方</strong>的动态近似值</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113165653234.png" alt="image-20220113165653234"></p><h3 id="2-构造第一动量和第二动量的无偏估计"><a href="#2-构造第一动量和第二动量的无偏估计" class="headerlink" title="2 构造第一动量和第二动量的无偏估计"></a>2 构造第一动量和第二动量的无偏估计</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113165846180.png" alt="image-20220113165846180"></p><p>现在再用无偏估计值做更新，而不是1中的动量</p><h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3 小结"></a>3 小结</h3><p>Adam也是像带动量的SGD一样，但没有像SGD动量一样绕过太多</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170223274.png" alt="image-20220113170223274"></p><p>【损失函数等高线图】</p><h3 id="实际中"><a href="#实际中" class="headerlink" title="实际中"></a>实际中</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170620935.png" alt="image-20220113170620935"></p><p>不同阶段使用不同学习率</p><h3 id="衰减策略"><a href="#衰减策略" class="headerlink" title="衰减策略"></a>衰减策略</h3><h4 id="1-步长衰减："><a href="#1-步长衰减：" class="headerlink" title="1 步长衰减："></a>1 步长衰减：</h4><ul><li><p>在第10万次迭代时，可以衰减一个因子，然后继续训练<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170750598.png" alt="image-20220113170750598"></p></li><li><p>残差网络中：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170944058.png" alt="image-20220113170944058"></p></li><li><p>衰减处（降低学习率）：迭代时把学习率<strong>乘上一个因子</strong></p><ul><li>模型已经接近一个不错的取值区域，此时梯度已经很小，保持原有学习速率只能在最优点附近徘徊</li><li>降低学习率，目标函数能够进一步降低（即loss函数上进一步取得进步</li><li></li></ul></li></ul><h4 id="2-指数衰减："><a href="#2-指数衰减：" class="headerlink" title="2 指数衰减："></a>2 指数衰减：</h4><ul><li><p>训练时<strong>持续衰减</strong><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170758571.png" alt="image-20220113170758571"></p></li><li><p>另一种连续衰减方法（非指数衰减）<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113170855458.png" alt="image-20220113170855458"></p></li></ul><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><p>开始训练时应该不带学习率衰减，用一个不错的学习率</p><h2 id="二阶优化"><a href="#二阶优化" class="headerlink" title="二阶优化"></a>二阶优化</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113172240164.png" alt="image-20220113172240164"></p><ol><li>用一个二次函数来局部逼近函数</li><li>∵二次函数，可以直接跳到<strong>最小值点</strong></li></ol><h2 id="牛顿步长"><a href="#牛顿步长" class="headerlink" title="牛顿步长"></a>牛顿步长</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113172512465.png" alt="image-20220113172512465"></p><ul><li>没有学习率</li><li>二次逼近直接走到二次函数的最小值点</li><li>N*N矩阵，太大了！</li></ul><p>👇</p><h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113172911506.png" alt="image-20220113172911506"></p><ul><li>很少随机性、少参数</li></ul><h2 id="如何减少训练和测试之间的误差差距？"><a href="#如何减少训练和测试之间的误差差距？" class="headerlink" title="如何减少训练和测试之间的误差差距？"></a>如何减少训练和测试之间的误差差距？</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113173048726.png" alt="image-20220113173048726"></p><ol><li><p>选择从<strong>不同的随机初始值</strong>上训练10个不同的模型</p></li><li><p>测试时：测试10个，再取平均</p><p>提升了性能</p></li></ol><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>提高单一模型的效果</p><p>（并不需要测试10个这样的集成模型方法）</p><p>在模型中加入一些成分，防止训练集上的过拟合</p><p>从而使测试集上的效果得到提升</p><h2 id="1-加入额外的一项"><a href="#1-加入额外的一项" class="headerlink" title="1 加入额外的一项"></a>1 加入额外的一项</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113174615237.png" alt="image-20220113174615237"></p><h2 id="2-dropout"><a href="#2-dropout" class="headerlink" title="2 dropout"></a>2 dropout</h2><p>全连接网络：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113174844491.png" alt="image-20220113174844491"></p><p>经过dropout：（置零一些神经元（激活函数））</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113174854439.png" alt="image-20220113174854439"></p><ul><li>网络变小了一号</li><li>只用到了其中一部分神经元</li><li>每次遍历，正向传递都是不同的部分</li></ul><p>在哪使用：一般全连接层</p><p>​sometimes卷积层</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113175316595.png" alt="image-20220113175316595"></p><p>随机将一部分神经元置零：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113175350990.png" alt="image-20220113175350990"></p><h3 id="dropout为什么有用？"><a href="#dropout为什么有用？" class="headerlink" title="dropout为什么有用？"></a>dropout为什么有用？</h3><ul><li>避免了特征间的相互适应</li><li>每一层通过不同的零散特征来判断，而非一些特征组合<ul><li>抑制了过拟合<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113175603253.png" alt="image-20220113175603253"></li></ul></li></ul><h3 id="dropout后的网络的变化"><a href="#dropout后的网络的变化" class="headerlink" title="dropout后的网络的变化"></a>dropout后的网络的变化</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113175833137.png" alt="image-20220113175833137"></p><p>多了一个z，表示随机被置零的项</p><h4 id="？平均化这个随机性（也不能太随机了！"><a href="#？平均化这个随机性（也不能太随机了！" class="headerlink" title="？平均化这个随机性（也不能太随机了！"></a>？平均化这个随机性（也不能太随机了！</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113180939666.png" alt="image-20220113180939666"></p><p>通过积分来边缘化随机性</p><p>局部逼近这个积分？</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113181416768.png" alt="image-20220113181416768"></p><p>训练期间：最后取了平均</p><p>train期望是test期望的一半</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113181842271.png" alt="image-20220113181842271"></p><p>预测函数：概率*输出层的输出</p><h3 id="dropout小结"><a href="#dropout小结" class="headerlink" title="dropout小结"></a>dropout小结</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113182120659.png" alt="image-20220113182120659"></p><p>dropout在</p><p>正向传播中：添加两行</p><p>预测predict：乘以概率p</p><h4 id="逆转dropout，使得测试高效"><a href="#逆转dropout，使得测试高效" class="headerlink" title="逆转dropout，使得测试高效"></a>逆转dropout，使得测试高效</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220113182505641.png" alt="image-20220113182505641"></p><p>测试时消除✖p这一乘法运算带来的时间消耗：</p><ul><li>测试时使用整个权重w</li><li>训练时➗ p</li></ul><p>👇</p><p>训练时间会更多，因为每一次都是一些新的网络</p><p>但测试的鲁棒性更强</p><h1 id="16-7、1-正则化"><a href="#16-7、1-正则化" class="headerlink" title="16-7、1 正则化"></a>16-7、1 正则化</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>&#x3D;&#x3D;为了减小训练误差和测试误差的间隙&#x3D;&#x3D;</p><h2 id="如何做"><a href="#如何做" class="headerlink" title="如何做"></a>如何做</h2><ul><li>训练时加入一些随机量</li><li>测试时抵消随机量</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114133119315.png" alt="image-20220114133119315"></p><h2 id="比如："><a href="#比如：" class="headerlink" title="比如："></a>比如：</h2><ul><li><h3 id="1-dropout"><a href="#1-dropout" class="headerlink" title="1 dropout"></a>1 dropout</h3><ul><li>通过调整p调整正则化的力度（可控）</li></ul></li><li><h3 id="2-Batch-Normalization"><a href="#2-Batch-Normalization" class="headerlink" title="2 Batch Normalization"></a>2 Batch Normalization</h3><ul><li>因为每一次训练每个数据和谁一起被训练是随机的</li></ul></li><li><p>两个都具有正则化效果</p></li><li><p>最好<strong>不要</strong>在模型中<strong>同时使用</strong>BN和dropout，同时使用会导致<strong>方差偏移</strong>现象</p></li><li><h3 id="3-数据增强"><a href="#3-数据增强" class="headerlink" title="3 数据增强"></a>3 数据增强</h3><ul><li><p>以某种方式<strong>随机</strong>地<strong>转换图像</strong></p></li><li><p>使得标签可以<strong>保留不变</strong></p></li><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114132559433.png" alt="image-20220114132559433"></p></li><li><h4 id="1-随机裁剪"><a href="#1-随机裁剪" class="headerlink" title="1 随机裁剪"></a>1 随机裁剪</h4></li><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114132618586.png" alt="image-20220114132618586"></p></li><li><p>5中标准裁剪+翻转<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114132844985.png" alt="image-20220114132844985"></p></li><li><h4 id="2-色彩抖动"><a href="#2-色彩抖动" class="headerlink" title="2 色彩抖动"></a>2 色彩抖动</h4></li></ul></li><li><h3 id="4-DropConnect"><a href="#4-DropConnect" class="headerlink" title="4 DropConnect"></a>4 DropConnect</h3><ul><li><p>随机将<strong>权重矩阵</strong>的一些值置零<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114133404508.png" alt="image-20220114133404508"></p></li><li><p>而不是神经元置零</p></li><li><p>wx，w&#x3D;0，输出wx&#x3D;0，激活层少一个输入</p></li></ul></li><li><h3 id="5-Fractional-Max-Pooling"><a href="#5-Fractional-Max-Pooling" class="headerlink" title="5 Fractional Max Pooling"></a>5 Fractional Max Pooling</h3><ul><li>局部最大池化</li><li>每次在池化层操作时，将随机池化正在池化的区域 </li><li>测试时再抵消随机化<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114133911459.png" alt="image-20220114133911459"></li></ul></li><li><h3 id="6-Stochastic-Depth"><a href="#6-Stochastic-Depth" class="headerlink" title="6 Stochastic Depth"></a>6 Stochastic Depth</h3><ul><li><strong>训练时</strong>随机丢掉一些网络层，只用部分层</li><li><strong>测试时</strong>用全部网络</li><li>效果类似dropout<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114134150132.png" alt="image-20220114134150132"></li></ul></li></ul><h2 id="in-practice："><a href="#in-practice：" class="headerlink" title="in practice："></a>in practice：</h2><ul><li>通常使用BN就够了</li><li>帮助收敛，尤其很深的网络</li><li>如果发现网络<strong>过拟合</strong>，BN不够，可以加入dropout</li></ul><h2 id="过拟合产生的原因"><a href="#过拟合产生的原因" class="headerlink" title="过拟合产生的原因"></a>过拟合产生的原因</h2><ul><li>数据过少</li><li>……</li></ul><p>解决：</p><ul><li>正则化</li><li>迁移学习（见下章）</li></ul><h1 id="17-7、2-迁移学习"><a href="#17-7、2-迁移学习" class="headerlink" title="17-7、2 迁移学习"></a>17-7、2 迁移学习</h1><h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><ul><li>不需要使用超大的样本集</li><li>也能训练CNN（卷积神经网络）</li></ul><h2 id="小数据集"><a href="#小数据集" class="headerlink" title="小数据集"></a>小数据集</h2><ul><li>重新随机初始化最后的矩阵 FC-C</li><li>冻结前面层的权重</li><li>只需要训练一个线性分类器（最后层 ）<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114135424600.png" alt="image-20220114135424600"></li></ul><h2 id="充裕数据集"><a href="#充裕数据集" class="headerlink" title="充裕数据集"></a>充裕数据集</h2><ul><li>微调网络</li><li>在最后一层收敛</li><li>在数据集上充分训练后</li><li>试着更新整个网络的权值</li><li>通用策略：<ul><li>更新网络时调低学习率</li><li>因为已经在imagenet上有较强泛化能力，只需要微小调整来适应此数据集</li></ul></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114141207320.png" alt="image-20220114141207320"></p><h2 id="迁移学习很常见"><a href="#迁移学习很常见" class="headerlink" title="迁移学习很常见"></a>迁移学习很常见</h2><ul><li>所有模型都有一个卷积神经网络CNN</li><li>不会从头训练</li><li>大多情况：在ImageNet预训练，然后根据任务精调</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114141759693.png" alt="image-20220114141759693"></p><p>对于各种模型，如果没有大数据集：</p><ul><li>下载相关的预训练模型<ul><li>重新初始化部分模型</li><li>or 在数据上精调模型</li><li>model zoo，提供了不同模型的预训练版本</li></ul></li></ul><h2 id="小结-6"><a href="#小结-6" class="headerlink" title="小结"></a>小结</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114142040221.png" alt="image-20220114142040221"></p><ul><li>最优化：<ul><li>改进训练效果</li></ul></li><li>正则化：<ul><li>改变测试集上的性能</li><li>集成模型</li><li>dropout</li></ul></li><li>迁移学习：<ul><li>​小样本也能训练好</li></ul></li></ul><h1 id="18-8、深度学习软件"><a href="#18-8、深度学习软件" class="headerlink" title="18-8、深度学习软件"></a>18-8、深度学习软件</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151224721.png" alt="image-20220114151224721"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151331763.png" alt="image-20220114151331763"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151409171.png" alt="image-20220114151409171"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151801771.png" alt="image-20220114151801771"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151806482.png" alt="image-20220114151806482"></p><p>矩阵乘法！</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114151856206.png" alt="image-20220114151856206"></p><p>GPU：并行，同时把每个点计算出来</p><p>CPU：穿行，一个一个点计算</p><p>cuDNN：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114152101541.png" alt="image-20220114152101541"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114152109054.png" alt="image-20220114152109054"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114152219234.png" alt="image-20220114152219234"></p><h2 id="数据从cpu读取太慢，无法匹配GPU"><a href="#数据从cpu读取太慢，无法匹配GPU" class="headerlink" title="数据从cpu读取太慢，无法匹配GPU"></a>数据从cpu读取太慢，无法匹配GPU</h2><ul><li>设定好cpu的预读内容</li><li>避免比较笨的序列化操作</li><li>先把数据从硬盘里读出来</li></ul><h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114152850769.png" alt="image-20220114152850769"></p><h3 id="用处："><a href="#用处：" class="headerlink" title="用处："></a>用处：</h3><h4 id="1-构建计算图"><a href="#1-构建计算图" class="headerlink" title="1 构建计算图"></a>1 构建计算图</h4><p>计算任何想要计算的函数</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114153904493.png" alt="image-20220114153904493"></p><h4 id="2-自动计算梯度、反向传播"><a href="#2-自动计算梯度、反向传播" class="headerlink" title="2 自动计算梯度、反向传播"></a>2 自动计算梯度、反向传播</h4><h4 id="3-能够在GPU上高效执行"><a href="#3-能够在GPU上高效执行" class="headerlink" title="3 能够在GPU上高效执行"></a>3 能够在GPU上高效执行</h4><h2 id="举例：一个计算图"><a href="#举例：一个计算图" class="headerlink" title="举例：一个计算图"></a>举例：一个计算图</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154231932.png" alt="image-20220114154231932"></p><h3 id="使用numpy"><a href="#使用numpy" class="headerlink" title="使用numpy"></a>使用numpy</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154524047.png" alt="image-20220114154524047"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154533876.png" alt="image-20220114154533876"></p><p>必须自己计算梯度、不能在gpu跑</p><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154742353.png" alt="image-20220114154742353"></p><p>能计算梯度、能在gpu跑</p><p>gpu和cpu切换：&#x2F;gpu：0或 &#x2F;cpu：0</p><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154907756.png" alt="image-20220114154907756"></p><p>反向传播：.backward()</p><p>计算梯度：x.grad.data</p><p>切换到gpu：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114154928930.png" alt="image-20220114154928930"></p><p>把所有东西转换成cuda数据类型</p><h3 id="比较三个框架"><a href="#比较三个框架" class="headerlink" title="比较三个框架"></a>比较三个框架</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114155132451.png" alt="image-20220114155132451"></p><h1 id="示例：tf进行全连接"><a href="#示例：tf进行全连接" class="headerlink" title="示例：tf进行全连接"></a>示例：tf进行全连接</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114155616284.png" alt="image-20220114155616284"></p><p>先定义图结构</p><p>再多次运行图</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114155804143.png" alt="image-20220114155804143"></p><p>【placeholder创建四个输入结点】</p><p>运行图模型时，会输入数据，将它们放到计算图中的输入槽中</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114162050847.png" alt="image-20220114162050847"></p><p>【在这些符号变量上做操作，以便想要进行的运算】</p><ul><li>maximum：relu的非线性特性</li><li>matmul：矩阵乘法计算输出的预测结果</li><li>diff：欧氏距离</li><li>loss：目标值y和预测值之间的损失</li></ul><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114162429340.png" alt="image-20220114162429340"></p><p>【计算损失值在w1和w2方向上的梯度】</p><p>免去了写反向传播代码</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114162610188.png" alt="image-20220114162610188"></p><p>【进入session运行计算&amp;输入数据】</p><ul><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114162646762.png" alt="image-20220114162646762">【tf从numpy数组中接收数据】</p></li><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114162941609.png" alt="image-20220114162941609"></p></li><li><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220114163328698.png" alt="image-20220114163328698">【训练网络】</p><ul><li>session.run请求tf计算损失和梯度</li><li>计算梯度以更新权重</li></ul></li></ul><h3 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115104448064.png" alt="image-20220115104448064"></p><p>​【在cpu和gpu之间进行<strong>数据复制非常耗费资源</strong>】</p><ul><li>梯度值的个数与权重值个数一致，</li><li>每次运行图时，我们将从numpy数组中复制<strong>权重</strong>到tf中，才能得到<strong>梯度</strong></li><li>然后从tf中复制<strong>梯度</strong>到numpy数组</li></ul><h3 id="改进：把w1、w2变成variable"><a href="#改进：把w1、w2变成variable" class="headerlink" title="改进：把w1、w2变成variable"></a>改进：把w1、w2变成variable</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115104621174.png" alt="image-20220115104621174"></p><p>【变量可以一直保持在图中】</p><p>与占位符的区别：</p><ul><li>占位符：相当于w1、w2都在计算图外部，需要用numpy数组初始化</li><li>变量：存在于计算图中，tf负责初始化<ul><li>tf.randomnormal<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115104936605.png" alt="image-20220115104936605"></li></ul></li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115105337764.png" alt="image-20220115105337764"></p><p>【在计算图中改变参数w】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115105548266.png" alt="image-20220115105548266"></p><p>【初始化w1、w2】</p><p>【计算loss】</p><p>问题：</p><p>并没有更新w，loss一直没变！</p><p>解决：</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115110001635.png" alt="image-20220115110001635"></p><p>添加w1、w2作为输出<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115110105397.png" alt="image-20220115110105397"></p><p>【将w12作为仿制结点，告诉图计算仿制结点】</p><h4 id="Q-为什么不把xy也放进计算图？"><a href="#Q-为什么不把xy也放进计算图？" class="headerlink" title="Q:为什么不把xy也放进计算图？"></a>Q:为什么不把xy也放进计算图？</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115110533263.png" alt="image-20220115110533263"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115110543332.png" alt="image-20220115110543332"></p><p>（人为输入不同值，而不是被更新）</p><h2 id="tf：optimizer"><a href="#tf：optimizer" class="headerlink" title="tf：optimizer"></a>tf：optimizer</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115111311744.png" alt="image-20220115111311744"></p><p>optimizer：计算梯度，1e-5是学习率</p><p>.minimize最小化loss并得到更新权重</p><h2 id="tf：loss"><a href="#tf：loss" class="headerlink" title="tf：loss"></a>tf：loss</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115112139174.png" alt="image-20220115112139174"></p><h2 id="tf：layers"><a href="#tf：layers" class="headerlink" title="tf：layers"></a>tf：layers<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115112531235.png" alt="image-20220115112531235"></h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115112412383.png" alt="image-20220115112412383"></p><p>初始化取值更好了</p><p>收敛速度更快</p><h2 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h2><p>一个非常方便的API</p><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115112812321.png" alt="image-20220115112812321"></p><h1 id="pytorch-1"><a href="#pytorch-1" class="headerlink" title="pytorch"></a>pytorch</h1><h2 id="明确定义了三个层次"><a href="#明确定义了三个层次" class="headerlink" title="明确定义了三个层次"></a>明确定义了三个层次</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115113400484.png" alt="image-20220115113400484"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115113413285.png" alt="image-20220115113413285"></p><h3 id="1-tensors"><a href="#1-tensors" class="headerlink" title="1 tensors"></a>1 tensors</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115113559582.png" alt="image-20220115113559582"></p><ol><li>建立随机数据</li><li>前向传播</li><li>反向传播</li><li>手动更新权值</li></ol><h4 id="使得代码在gpu运行："><a href="#使得代码在gpu运行：" class="headerlink" title="使得代码在gpu运行："></a>使得代码在gpu运行：<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115113836695.png" alt="image-20220115113836695"></h4><p>把张量类型转换成cuda类型</p><p>&#x3D;&#x3D;pytorch张量&#x3D;Numpy+GPU&#x3D;&#x3D;</p><h3 id="2-variable"><a href="#2-variable" class="headerlink" title="2 variable"></a>2 variable</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115114717807.png" alt="image-20220115114717807"></p><p>x:结点，<strong>变量</strong></p><p>x.data: <strong>张量</strong></p><p>x.grad: 梯度<strong>变量</strong></p><p>x.grad.data：梯度变量的<strong>张量</strong></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115114907446.png" alt="image-20220115114907446"></p><p>【告诉构造器是否需要计算该变量上的梯度】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115115028598.png" alt="image-20220115115028598"></p><p>【计算预测值、loss】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115115105266.png" alt="image-20220115115105266"></p><p>【反向传播得到需要的所有梯度值】</p><p>&#x3D;&#x3D;梯度都是自动求解的&#x3D;&#x3D;</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115115144164.png" alt="image-20220115115144164"></p><p>【用梯度对权重进行更新】</p><p>梯度都在.grad.data中</p><h3 id="pytorch与tf区别"><a href="#pytorch与tf区别" class="headerlink" title="pytorch与tf区别"></a>pytorch与tf区别</h3><ul><li>tf：<ul><li>先构建显示的图</li><li>重复运行</li></ul></li><li>pytorch：<ul><li>每次前向传播都会构建一个新的图</li></ul></li></ul><h3 id="自定义前向后向"><a href="#自定义前向后向" class="headerlink" title="自定义前向后向"></a>自定义前向后向</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115115913140.png" alt="image-20220115115913140"></p><p>【定义了一个relu的for、back】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115120025453.png" alt="image-20220115120025453"></p><p>【使用这个relu】</p><p>把rulu固定在了计算图中</p><h3 id="nn包"><a href="#nn包" class="headerlink" title="nn包"></a>nn包</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115120532061.png" alt="image-20220115120532061"></p><p>【定义model为一些层的序列】</p><ul><li>线性层</li><li>relu</li></ul><p>MSELoss：均方差损失</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115121824128.png" alt="image-20220115121824128"></p><p>【前向传播】</p><p>&#x3D;&#x3D;每次前向传播都建立了新计算图&#x3D;&#x3D;</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115121838098.png" alt="image-20220115121838098"></p><p>【计算梯度】</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115121923426.png" alt="image-20220115121923426"></p><p>【循环】 </p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115124547380.png" alt="image-20220115124547380"></p><p>【使用adam更新法则】</p><p>【对模型中的参数进行优化，parameters、lr</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115124649190.png" alt="image-20220115124649190"></p><h4 id="定义一个新的modules"><a href="#定义一个新的modules" class="headerlink" title="定义一个新的modules"></a>定义一个新的modules</h4><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115125022041.png" alt="image-20220115125022041"></p><p>forward中，x传入第一层，clamp计算relu，输出作为第二层的输入</p><h3 id="dataloaders"><a href="#dataloaders" class="headerlink" title="dataloaders"></a>dataloaders</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115125701543.png" alt="image-20220115125701543"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115125719659.png" alt="image-20220115125719659"></p><h3 id="pytorch：预训练模型"><a href="#pytorch：预训练模型" class="headerlink" title="pytorch：预训练模型"></a>pytorch：预训练模型</h3><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115125843723.png" alt="image-20220115125843723"></p><h2 id="torch-vs-pytorch"><a href="#torch-vs-pytorch" class="headerlink" title="torch vs pytorch"></a>torch vs pytorch</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115130031658.png" alt="image-20220115130031658"></p><h2 id="tf-vs-pytorch"><a href="#tf-vs-pytorch" class="headerlink" title="tf vs pytorch"></a>tf vs pytorch</h2><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115131320069.png" alt="image-20220115131320069"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115131605201.png" alt="image-20220115131605201"></p><p>动态图：</p><ul><li>在循环中，每次运行都是不同的图结构</li><li>便于做<strong>条件控制</strong></li><li>便于<strong>循环</strong><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115132253835.png" alt="image-20220115132253835"></li><li>应用：<ul><li>循环网络</li><li>循环网络</li><li>模块化网络</li></ul></li></ul><p>静态图：</p><ul><li>在循环外，固定好了模式</li></ul><h1 id="caffe"><a href="#caffe" class="headerlink" title="caffe"></a>caffe</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115133942626.png" alt="image-20220115133942626"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115133808023.png" alt="image-20220115133808023"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115134020453.png" alt="image-20220115134020453"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115134030301.png" alt="image-20220115134030301"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115134237013.png"></p><h1 id="使用建议"><a href="#使用建议" class="headerlink" title="使用建议"></a>使用建议</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115134342382.png" alt="image-20220115134342382"></p><p>手机端、生产：caffe</p><p>科研：pytorch</p><h1 id="框架小结"><a href="#框架小结" class="headerlink" title="框架小结"></a>框架小结</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115160747647.png" alt="image-20220115160747647"></p><p>主要是：</p><ul><li>1 调制forward、backward函数<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115160905699.png" alt="image-20220115160905699"></li><li>2 定义顺序<img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115160916696.png" alt="image-20220115160916696"></li></ul><h1 id="19-9、CNN"><a href="#19-9、CNN" class="headerlink" title="19-9、CNN"></a>19-9、CNN</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161023415.png" alt="image-20220115161023415"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161035079.png" alt="image-20220115161035079"></p><h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161136929.png" alt="image-20220115161136929"></p><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161244359.png" alt="image-20220115161244359"></p><p>  5层卷积，2层池化，2层正则，3层全连接</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161615029.png" alt="image-20220115161615029"></p><p> 深度：96，即 55x55x96</p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161835566.png" alt="image-20220115161835566"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115161841907.png" alt="image-20220115161841907"></p><p>【池化层没有参数】</p><ul><li>parameter：0</li><li>只是观察池化区域，取得最大值</li><li>卷积层有参数w权重</li></ul><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115162206512.png" alt="image-20220115162206512"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115162333867.png" alt="image-20220115162333867"></p><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115162452843.png" alt="image-20220115162452843"></p><h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p><img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115163211928.png" alt="image-20220115163211928"></p><ul><li>使用最小的卷积核3x3</li><li>只关注相邻的像素</li></ul><p>意思应该是用<strong>多次小的卷积</strong>相比<strong>一次大的卷积</strong>用的参数更少然后更加非线性？</p><p>3层3<em>3（步长1）的卷积核来卷积效果等同于用1层7</em>7（步长1）的卷积核来卷积，可以用(N-F)&#x2F;stride+1这个公式自己推导</p><p> <img src="/CS231%E7%AC%94%E8%AE%B0.assets/image-20220115164623328.png" alt="image-20220115164623328"></p><p>continue….</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>武汉大学超算中心使用方法</title>
    <link href="/2022/04/22/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2022/04/22/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="武汉大学超算中心"><a href="#武汉大学超算中心" class="headerlink" title="武汉大学超算中心"></a>武汉大学超算中心</h1><h2 id="Linux基础知识"><a href="#Linux基础知识" class="headerlink" title="Linux基础知识"></a>Linux基础知识</h2><p>武大超算采用的是Linux系统</p><p>用户对计算机资源管理和使用的一种工具</p><p>用户通过登录的shel<strong>l与远程计算机资源建立连接</strong></p><p>每个用户都被管理员赋予一定的<strong>权限</strong>和<strong>计算机的资源</strong></p><p>可以通过自己的<strong>shell输入命令</strong>来指导<strong>远程计算机</strong>对自己分配到的一些资源进行操作</p><p>比如在文件夹下<strong>新建文件、删除文件</strong>，或者<strong>安装程序</strong></p><p>用户登录时都在计算机的<strong>登录节点</strong>操作自己的文件</p><p>如果有一些<strong>运算量比较大</strong>的程序，用户可以<strong>输入命令</strong>，将这些程序<strong>提交到其它节点</strong>去执行</p><h3 id="2-Linux的一些基本命令——对文件和目录的管理"><a href="#2-Linux的一些基本命令——对文件和目录的管理" class="headerlink" title="2.Linux的一些基本命令——对文件和目录的管理"></a><strong>2.Linux的一些基本命令——对文件和目录的管理</strong></h3><p>cd </p><p>绝对路径以“&#x2F;”开头</p><p>相对路径就是从当前操作的路径开始，不以“&#x2F;”开头</p><p>rm</p><p>删除文件</p><p>cp</p><p>复制文件从一个位置到另外一个位置</p><p>mv</p><p>移动文件从一个位置到另外一个位置</p><p>ls</p><p>列出该目录下的所有文件</p><h3 id="3-shell脚本和vi编辑器"><a href="#3-shell脚本和vi编辑器" class="headerlink" title="3.shell脚本和vi编辑器"></a><strong>3.shell脚本和vi编辑器</strong></h3><h4 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本:"></a>shell脚本:</h4><p>把每次都要在shell输入的命令写在一个后缀为.sh的文本文件中</p><p>linux运行这个shell脚本相当于执行里面的语句</p><h4 id="vi是什么呢？"><a href="#vi是什么呢？" class="headerlink" title="vi是什么呢？"></a>vi是什么呢？</h4><p>linux文件下<strong>编辑文本文件</strong>的一个工具</p><p>输入”vi xxx.cpp”就可以<strong>编辑cpp文件</strong></p><p>输入“vi xxx.py”就可以<strong>编辑python脚本</strong></p><p>输入”vi xxx.sh”皆可以<strong>编辑sh脚本</strong></p><p>&#x3D;&#x3D;使用vi，可以直接在linux上编辑自己的程序脚本还有提交要用的sh脚本&#x3D;&#x3D;</p><p>在命令行输入conda activate base，再输入命令python，就可以看到python的版本已经变化了，代表可以使用安装的anaconda。</p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/v2-13998440b32e5d164ef1d67b7f3af8ab_1440w.png" alt="img"></p><p>输入**quit()**退出python解释器</p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/image-20220126205508958.png" alt="image-20220126205508958"></p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/image-20220126205520977.png" alt="image-20220126205520977"></p><p>如果要提交到<strong>计算节点</strong>运行python程序的话</p><h2 id="使用gpu"><a href="#使用gpu" class="headerlink" title="使用gpu"></a>使用gpu</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">srun -A panjun --gres=gpu:1 -p gpu -u python train.py<br></code></pre></td></tr></table></figure><h2 id="查看预装软件"><a href="#查看预装软件" class="headerlink" title="查看预装软件"></a>查看预装软件</h2><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-keyword">module</span> avail<br></code></pre></td></tr></table></figure><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/image-20220126210632064.png" alt="image-20220126210632064"></p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/image-20220126210609081.png" alt="image-20220126210609081"></p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/image-20220126210647886.png" alt="image-20220126210647886"></p><h2 id="一些功能应用"><a href="#一些功能应用" class="headerlink" title="一些功能应用"></a>一些功能应用</h2><p>1.<a href="https://so.csdn.net/so/search?q=module&spm=1001.2101.3001.7020">module</a>模块</p><p>可以看超算上面有什么<strong>已经安装好的底层</strong></p><p>有gcc<strong>从5到7</strong>的版本</p><p>也有cuda9，9.2，10，10.1</p><p><strong>根据自己的需求module load</strong></p><p>3.gpu</p><p>gpu是付费的，需要跟导师申请，然后<strong>gpu–导师</strong>，一般有两种方式<strong>salloc和sbatch</strong>都可以连接上gpu</p><p>salloc：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">salloc -A XXX（这里改成自己导师的名字） -p gpu --gres=gpu:4（这是申请导师的4块gpu，然后就进入节点了）<br>ssh 节点（进入节点）<br>module load XXX（自己需要的模块）<br>python XXX.py(或者编译C代码，matlab，java看自己需求)<br></code></pre></td></tr></table></figure><p>sbatch：</p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MjI3NDU3,size_16,color_FFFFFF,t_70.jpeg" alt="画红线的位置需要填写的，蓝色位置为导师名字"></p><p>把这个提交，<strong>用srun提交作业</strong></p><h2 id="使用gpu-1"><a href="#使用gpu-1" class="headerlink" title="使用gpu"></a>使用gpu</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">srun -A panjun <span class="hljs-attribute">--gres</span>=gpu:1 -p gpu -u python train.py<br></code></pre></td></tr></table></figure><p>## 使用付费节点</p><p>- CPU: <code>-A zywang4</code></p><p>- GPU: <code>-A zywang4 -p gpu --gres=gpu:</code></p><p>如：</p><p><strong>srun -A zywang4 -p gpu –gres&#x3D;gpu:1  0.bash</strong></p><p>sbatch -p gpu –gres&#x3D;gpu:4 x.sbatch</p><h2 id="磁盘查询命令"><a href="#磁盘查询命令" class="headerlink" title="磁盘查询命令"></a>磁盘查询命令</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">lfs quota -uh jrhu <span class="hljs-regexp">/home/</span>jrhu<br>lfs quota -uh jrhu <span class="hljs-regexp">/project/</span>jrhu<br></code></pre></td></tr></table></figure><h1 id="login-node"><a href="#login-node" class="headerlink" title="login node"></a>login node</h1><p>202.114.96.180</p><h1 id="file-node"><a href="#file-node" class="headerlink" title="file node"></a>file node</h1><p>千兆带宽的传输节点 202.114.96.177</p><h2 id="查看残留进程"><a href="#查看残留进程" class="headerlink" title="查看残留进程"></a>查看残留进程</h2><p>非常重要 没事就检查一下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">squeue -u jrhu<br></code></pre></td></tr></table></figure><h2 id="kill-残留进程"><a href="#kill-残留进程" class="headerlink" title="kill 残留进程"></a>kill 残留进程</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scancel </span>&lt;<span class="hljs-keyword">job </span>id&gt;<br></code></pre></td></tr></table></figure><h2 id="2-Sinfo"><a href="#2-Sinfo" class="headerlink" title="2. Sinfo"></a>2. Sinfo</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">sinfo               <span class="hljs-comment">#查看所有分区状态</span><br>sinfo -a            <span class="hljs-comment">#查看所有分区状态</span><br>sinfo -N            <span class="hljs-comment">#查看节点状态</span><br>sinfo -n node-name  <span class="hljs-comment">#查看指定节点状态</span><br>sinfo --<span class="hljs-built_in">help</span>        <span class="hljs-comment">#查看sinfo的说明</span><br></code></pre></td></tr></table></figure><p>Slurm提交作业有3种模式，分别为交<strong>互模式</strong>，<strong>批处理模式</strong>，分配模式</p><h3 id="4-1-参数说明"><a href="#4-1-参数说明" class="headerlink" title="4.1 参数说明"></a>4.1 参数说明</h3><p>以下所有参数在 <code>srun, sbatch, salloc</code> 中均可以使用。更多参数见<code>srun --help</code>, <code>sbatch --help</code>, <code>salloc --help</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">-c, --cpu-per-task=NCPUs        #指定每个进程使用核数，不指定默认为1<br>-e, --error=error_filename      #指定错误文件输出<br>-J, --job-name=JOBNAME          #指定作业名称<br>--mail-type=END/FAIL/ALL        #邮件提醒，可选:END,FAIL,ALL<br>--mail-user=mail_address        #通知邮箱地址<br>-n, --ntask=NTASKs #指定总进程数；不使用cpus-per-task，可理解为进程数即为核数 <br>--ntask-per-node=N #指定每个节点进程数/核数，使用-n参数后变为每个节点最多运行的进程数<br>-N, --nodes=N                   #指定节点数量<br>-o, --output=out_filename       #指定输出文件输出<br>-p, --partion=debug             #指定分区<br>-t, --time=dd-hh:mm:ss          #作业最大运行时间<br>-w, --nodelist=node[1,2]        #指定优先使用节点，不可与避免节点冲突<br>-x, --exclude=node[3,5-6]       #指定避免使用节点，不可与优先节点冲突<br>--mem-per-cpu=MB                #指定计算cpu最大占用内存大小<br></code></pre></td></tr></table></figure><h3 id="交互模式-Srun"><a href="#交互模式-Srun" class="headerlink" title="交互模式 Srun"></a>交互模式 Srun</h3><p>交互式作业提交，提交命令后，<strong>等待作业执行完成之后返回命令行窗口</strong>。</p><h3 id="批处理模式-Sbatch"><a href="#批处理模式-Sbatch" class="headerlink" title="批处理模式 Sbatch"></a>批处理模式 Sbatch</h3><p>用户编写作业脚本，指定资源需求约束，提交后台执行作业</p><p>运行 <code>sbatch filename</code> 来<strong>提交任务</strong></p><p>计算开始后，工作目录中会生成<strong>以 slurm 开头的.out 文件</strong>为输出文件</p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDUwNjA2Nw==,size_16,color_FFFFFF,t_70.png" alt="img"></p><p><img src="/%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%B6%85%E7%AE%97%E4%B8%AD%E5%BF%83%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDUwNjA2Nw==,size_16,color_FFFFFF,t_70-16506188801391.png" alt="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用卷积神经网络检测面部关键点教程</title>
    <link href="/2022/04/22/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A3%80%E6%B5%8B%E9%9D%A2%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%95%99%E7%A8%8B/"/>
    <url>/2022/04/22/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A3%80%E6%B5%8B%E9%9D%A2%E9%83%A8%E5%85%B3%E9%94%AE%E7%82%B9%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="使用卷积神经网络检测面部关键点教程Using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial"><a href="#使用卷积神经网络检测面部关键点教程Using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial" class="headerlink" title="使用卷积神经网络检测面部关键点教程Using convolutional neural nets to detect facial keypoints tutorial"></a>使用卷积神经网络检测面部关键点教程Using convolutional neural nets to detect facial keypoints tutorial</h1><h2 id="Data-prepare"><a href="#Data-prepare" class="headerlink" title="Data prepare"></a>Data prepare</h2><p>If you let training run long enough, you’ll notice that after about 75 epochs, it’ll have reached a test accuracy of around 98%.</p><p><strong>normalize data:</strong></p><ul><li>x from 0-255 trun to 0-1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.vstack(df[<span class="hljs-string">&#x27;Image&#x27;</span>].values) / <span class="hljs-number">255.</span>  <span class="hljs-comment"># scale pixel values to [0, 1]</span><br></code></pre></td></tr></table></figure><ul><li>y from 0-95 trun to -1,1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">y = (y - <span class="hljs-number">48</span>) / <span class="hljs-number">48</span>  <span class="hljs-comment"># scale target coordinates to [-1, 1]</span><br></code></pre></td></tr></table></figure><p><strong>Dropping</strong> all rows with missing values is what this line does:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df = df.dropna()  <span class="hljs-comment"># drop all rows that have missing values in them</span><br></code></pre></td></tr></table></figure><h2 id="First-model"><a href="#First-model" class="headerlink" title="First model"></a>First model</h2>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习学习路线</title>
    <link href="/2022/04/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <url>/2022/04/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h1 id="深度学习学习路线"><a href="#深度学习学习路线" class="headerlink" title="深度学习学习路线"></a>深度学习学习路线</h1><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105145931766.png" alt="image-20220105145931766"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105150241328.png" alt="image-20220105150241328"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105150342492.png" alt="image-20220105150342492"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105150923433.png" alt="image-20220105150923433"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105150945848.png" alt="image-20220105150945848"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151135700.png" alt="image-20220105151135700"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151152018.png" alt="image-20220105151152018"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151257574.png" alt="image-20220105151257574"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151334229.png" alt="image-20220105151334229"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151416959.png" alt="image-20220105151416959"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151633775.png" alt="image-20220105151633775"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151708059.png" alt="image-20220105151708059"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151805899.png" alt="image-20220105151805899"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151905768.png" alt="image-20220105151905768"></p><p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.assets/image-20220105151935447.png" alt="image-20220105151935447"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型的预测精度不高解决方法</title>
    <link href="/2022/04/22/%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E4%B8%8D%E9%AB%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2022/04/22/%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E4%B8%8D%E9%AB%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="如果模型的预测精度不高怎么办？"><a href="#如果模型的预测精度不高怎么办？" class="headerlink" title="如果模型的预测精度不高怎么办？"></a>如果模型的预测精度不高怎么办？</h1><p><a href="https://oomake.com/question/3302500">https://oomake.com/question/3302500</a></p><h1 id="如何打造高质量的机器学习数据集？"><a href="#如何打造高质量的机器学习数据集？" class="headerlink" title="如何打造高质量的机器学习数据集？"></a>如何打造高质量的机器学习数据集？</h1><ol><li>什么是高质量</li><li>基本工具</li><li>数据与标签来源</li><li>适可而止的预处理</li><li>验证可用性，尽早构造数据集迭代闭环</li><li>关于复杂NLP任务</li></ol><h2 id="什么是高质量"><a href="#什么是高质量" class="headerlink" title="什么是高质量"></a>什么是高质量</h2><p>解决问题</p><h2 id="基本工具"><a href="#基本工具" class="headerlink" title="基本工具"></a>基本工具</h2><p>在做数据集之前先掌握一些好用的工具和tricks，可以大大减少无谓的重复和低效劳动，提高迭代效率</p><p><strong>github</strong><br>写爬虫和清洗<strong>最原始</strong>数据之前先在github找一下</p><p><strong>正则表达式</strong><br>文本清洗利器，不解释</p><p><strong>Hadoop&#x2F;Spark</strong><br>千万级以上的语料就别去为难你的小服务器了</p><p><img src="/%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E4%B8%8D%E9%AB%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.assets/image-20220216164129846.png" alt="image-20220216164129846"></p><h2 id="数据与标签来源"><a href="#数据与标签来源" class="headerlink" title="数据与标签来源"></a>数据与标签来源</h2><p>数据可以通过<strong>人工构造、撰写</strong>的方式来产生</p><p>从互联网上爬取或对公开数据集进行二次加工</p><p>标签同样可以<strong>人工标注</strong></p><p>也可以<strong>远程监督</strong>的方式来获取</p><p><strong>人工构造和标注</strong></p><p><img src="/%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E4%B8%8D%E9%AB%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.assets/image-20220216201527204.png" alt="image-20220216201527204"></p><p><img src="/%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%B2%BE%E5%BA%A6%E4%B8%8D%E9%AB%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.assets/image-20220216201542155.png" alt="image-20220216201542155"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测mAP</title>
    <link href="/2022/04/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BmAP/"/>
    <url>/2022/04/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BmAP/</url>
    
    <content type="html"><![CDATA[<h1 id="目标检测中的均值平均精度（mAP）"><a href="#目标检测中的均值平均精度（mAP）" class="headerlink" title="目标检测中的均值平均精度（mAP）"></a>目标检测中的均值平均精度（mAP）</h1><p>普通<strong>单标签</strong>图像分类</p><p><strong>多标签</strong><a href="https://so.csdn.net/so/search?q=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&spm=1001.2101.3001.7020">图像分类</a>：图片的标签不止一个</p><ul><li>采用的是和信息检索中类似的方法—<strong>mAP（mean Average Precision）</strong></li></ul><p>训练好的模型得到<strong>所有测试样本的confidence score</strong></p><p><strong>每一类</strong>（如car）的confidence score保存到一个文件中（如comp1_cls_test_car.txt）</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩池云使用</title>
    <link href="/2022/04/22/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8/"/>
    <url>/2022/04/22/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="矩池云使用"><a href="#矩池云使用" class="headerlink" title="矩池云使用"></a>矩池云使用</h1><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213202308422.png" alt="image-20220213202308422"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213202328604.png" alt="image-20220213202328604"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213202633958.png" alt="image-20220213202633958"></p><p>用户名：root</p><h1 id="seaship环境"><a href="#seaship环境" class="headerlink" title="seaship环境"></a>seaship环境</h1><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213210849337.png" alt="image-20220213210849337"></p><h2 id="fvcore库的安装"><a href="#fvcore库的安装" class="headerlink" title="fvcore库的安装"></a>fvcore库的安装</h2><p>pip install -U ‘git+<a href="https://github.com/facebookresearch/fvcore&#39;">https://github.com/facebookresearch/fvcore&#39;</a></p><h1 id="编译detectron2"><a href="#编译detectron2" class="headerlink" title="编译detectron2"></a>编译detectron2</h1><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213235218028.png" alt="image-20220213235218028"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220213235255234.png" alt="image-20220213235255234"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub | apt-key add - &amp;&amp; \ <br>echo &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /&quot; &gt; /etc/apt/sources.list.d/cuda.list &amp;&amp; \ <br>apt-get update &amp;&amp; apt-get install -y --no-install-recommends \    <br>apt install cuda-command-line-tools-11-0 \    <br>rm -rf /var/lib/apt/lists/*<br></code></pre></td></tr></table></figure><h1 id="2-14-myconda环境"><a href="#2-14-myconda环境" class="headerlink" title="2.14 myconda环境"></a>2.14 myconda环境</h1><p>py3.7 torch1.7？cuda11.1</p><h2 id="fvcore库的安装-1"><a href="#fvcore库的安装-1" class="headerlink" title="fvcore库的安装"></a>fvcore库的安装</h2><p>pip install -U ‘git+<a href="https://github.com/facebookresearch/fvcore&#39;">https://github.com/facebookresearch/fvcore&#39;</a></p><h2 id="detectron2"><a href="#detectron2" class="headerlink" title="detectron2"></a>detectron2</h2><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214090549344.png" alt="image-20220214090549344"></p><p> python projects&#x2F;CenterNet2&#x2F;demo.py –config-file projects&#x2F;CenterNet2&#x2F;configs&#x2F;CenterNet2_R50_1x.yaml –input imgs&#x2F; –output imgout –opts MODEL.WEIGHTS projects&#x2F;CenterNet2&#x2F;CenterNet2_R50_1x.pth</p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214092121071.png" alt="image-20220214092121071"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214092152501.png" alt="image-20220214092152501"></p><p>测试环境成功！</p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214100038897.png" alt="image-20220214100038897"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214100920621.png" alt="image-20220214100920621"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214150327628.png" alt="image-20220214150327628"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214164715709.png" alt="image-20220214164715709"></p><h2 id="跑了下《源码》"><a href="#跑了下《源码》" class="headerlink" title="跑了下《源码》"></a>跑了下《源码》</h2><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214165203805.png" alt="image-20220214165203805"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214194222245.png" alt="image-20220214194222245"><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214194222374.png" alt="image-20220214194222374"></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert</span>(<span class="hljs-params">xml_list, xml_dir, json_file</span>)<span class="hljs-symbol">:</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;ore carrier&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;passenger ship&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;bulk cargo carrier&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;general cargo ship&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;container ship&quot;</span><br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;fishing boat&quot;</span><br></code></pre></td></tr></table></figure><h2 id="重新做了数据集"><a href="#重新做了数据集" class="headerlink" title="重新做了数据集"></a>重新做了数据集</h2><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214223753756.png" alt="image-20220214223753756"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214223855509.png" alt="image-20220214223855509"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214223935381.png" alt="image-20220214223935381"></p><p>1750是val集？</p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214224102921.png" alt="image-20220214224102921"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220214224822260.png" alt="image-20220214224822260"></p><h1 id="ubuntu-qt-qpa-xcb-could-not-connect-to-display"><a href="#ubuntu-qt-qpa-xcb-could-not-connect-to-display" class="headerlink" title="ubuntu qt.qpa.xcb: could not connect to display"></a><a href="https://www.cnblogs.com/liujiaxin2018/p/15452030.html">ubuntu qt.qpa.xcb: could not connect to display</a></h1><p><a href="https://www.cnblogs.com/liujiaxin2018/p/15452030.html">https://www.cnblogs.com/liujiaxin2018/p/15452030.html</a></p><h1 id="2-0调整数据集大小-计算MAP-2-15"><a href="#2-0调整数据集大小-计算MAP-2-15" class="headerlink" title="2.0调整数据集大小+计算MAP-2.15"></a>2.0调整数据集大小+计算MAP-2.15</h1><h2 id="修改yaml文件参数"><a href="#修改yaml文件参数" class="headerlink" title="修改yaml文件参数"></a>修改yaml文件参数</h2><p>Base-CenterNet2.yaml：</p><p><strong>BASE_LR：</strong>设置学习率。</p><p><strong>STEPS：</strong>设置训练多少步之后调整学习率。</p><p><strong>MAX_ITER：</strong>最大迭代次数。</p><p><strong>CHECKPOINT_PERIOD：</strong>设置迭代多少次保存一次模型</p><p>调整如下：</p><p>BASE_LR: 0.01<br>STEPS: (10000, 50000)<br>MAX_ITER: 100000<br>CHECKPOINT_PERIOD: 5000</p><h2 id="调整数据集-5000-1000-1000"><a href="#调整数据集-5000-1000-1000" class="headerlink" title="调整数据集 5000 1000 1000"></a>调整数据集 5000 1000 1000</h2><p>train1750 +2750+500  &#x3D;5000</p><p>val1750  -750   1000</p><p>test3000  -2000  1000</p><h1 id="2-17重新训练"><a href="#2-17重新训练" class="headerlink" title="2.17重新训练"></a>2.17重新训练</h1><ul><li>调整了数据集大小</li><li>各参数</li></ul><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220217123214888.png" alt="image-20220217123214888"></p><p><img src="/%E7%9F%A9%E6%B1%A0%E4%BA%91%E4%BD%BF%E7%94%A8.assets/image-20220217123234159.png" alt="image-20220217123234159"></p>]]></content>
    
    
    
    <tags>
      
      <tag>工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML/DL自己写代码记录</title>
    <link href="/2022/04/22/ML-DL%E8%87%AA%E5%B7%B1%E5%86%99%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/04/22/ML-DL%E8%87%AA%E5%B7%B1%E5%86%99%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="机器-x2F-深度学习自己写代码记录"><a href="#机器-x2F-深度学习自己写代码记录" class="headerlink" title="机器&#x2F;深度学习自己写代码记录"></a>机器&#x2F;深度学习自己写代码记录</h1><h2 id="txt变矩阵"><a href="#txt变矩阵" class="headerlink" title="txt变矩阵"></a>txt变矩阵</h2><ul><li><p>[idx,:]第idx行所有列被[0-2]列赋值</p><ul><li>&#96;&#96;&#96;python<br>returnMat[index,:] &#x3D; listFromLine[0:3]<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-code">    </span><br><span class="hljs-code"></span><br><span class="hljs-bullet">-</span> 初始化矩阵：<br><br><span class="hljs-bullet">  -</span> <span class="hljs-code">```python</span><br><span class="hljs-code">    returnMat = np.zeros((lenthOfLines,3))</span><br></code></pre></td></tr></table></figure></li></ul></li><li><p>去掉首位空格并按换行分割：</p><ul><li>&#96;&#96;&#96;<br>for line in file.readlines():<br>everyLine &#x3D; line.strip().split(‘\t’)  # 按换行分割<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs llvm"><br><br><br>## shape/获取训练集大小<br><span class="hljs-symbol"></span><br><span class="hljs-symbol">https:</span>//www.tensorflow.org/tutorials/keras/classification?hl<span class="hljs-operator">=</span>zh-cn#<span class="hljs-variable">%E5</span><span class="hljs-variable">%AF</span><span class="hljs-variable">%BC</span><span class="hljs-variable">%E5</span><span class="hljs-variable">%85</span><span class="hljs-variable">%A5_fashion_mnist_</span><span class="hljs-variable">%E6</span><span class="hljs-variable">%95</span><span class="hljs-variable">%B0</span><span class="hljs-variable">%E6</span><span class="hljs-variable">%8</span>D<span class="hljs-variable">%AE</span><span class="hljs-variable">%E9</span><span class="hljs-variable">%9</span>B<span class="hljs-variable">%86</span><br><br>在训练模型之前，我们先浏览一下数据集的格式。以下代码显示训练集中有 **<span class="hljs-number">60</span><span class="hljs-punctuation">,</span><span class="hljs-number">000</span> 个图像**，**每个图像由 <span class="hljs-number">28</span> <span class="hljs-keyword">x</span> <span class="hljs-number">28</span> 的像素表示**：<br><br>```python<br>train_images.shape<br></code></pre></td></tr></table></figure></li></ul></li></ul><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">(<span class="hljs-number">60000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">28</span><span class="hljs-punctuation">,</span> <span class="hljs-number">28</span>)<br></code></pre></td></tr></table></figure><ul><li>所以.shape获得整个数据集所有图像的形状</li><li><strong>.shape[0]取出图像数量</strong></li><li>&#x3D;&#x3D;(num,height,width)&#x3D;&#x3D;</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_labels)<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">60000<br></code></pre></td></tr></table></figure><ul><li><strong>len()取出图像数量</strong></li></ul><h2 id="batchsize设定"><a href="#batchsize设定" class="headerlink" title="batchsize设定"></a>batchsize设定</h2><ul><li>设置为1，太小，速度会比较慢！</li><li>如果太大：</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>tensorflow安装</title>
    <link href="/2022/04/22/tensorflow%E5%AE%89%E8%A3%85/"/>
    <url>/2022/04/22/tensorflow%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="tensorflow安装"><a href="#tensorflow安装" class="headerlink" title="tensorflow安装"></a>tensorflow安装</h1><h2 id="conda创建环境"><a href="#conda创建环境" class="headerlink" title="conda创建环境"></a>conda创建环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n tensorflow python=3.5.2<br><br>conda create -n tensorflow python=3.6.13<br></code></pre></td></tr></table></figure><h2 id="conda激活"><a href="#conda激活" class="headerlink" title="conda激活"></a>conda激活</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">activate tensorflow<br></code></pre></td></tr></table></figure><h2 id="conda下安装python"><a href="#conda下安装python" class="headerlink" title="conda下安装python"></a>conda下安装python</h2><p>1已完成</p><h2 id="conda下安装tensorflow"><a href="#conda下安装tensorflow" class="headerlink" title="conda下安装tensorflow"></a>conda下安装tensorflow</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ https://mirrors.tuna.tsinghua.edu.cn/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl<br></code></pre></td></tr></table></figure><p>报错：</p><p>HTTP error 404 while getting <a href="https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp36-cp36m-win_amd64.whl%E2%80%9D">https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp36-cp36m-win_amd64.whl”</a></p><p>说明这个源也失效了（访问不了）。</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>使用命令,不指定下载地址，让pip自己找源，可以指定一个tensorflow版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install --ignore-installed --upgrade tensorflow==1.0.1<br><br>python版本是3.6.13:<br>pip install --ignore-installed --upgrade tensorflow==1.11.0<br></code></pre></td></tr></table></figure><p><img src="/tensorflow%E5%AE%89%E8%A3%85.assets/image-20220311105548525.png" alt="image-20220311105548525"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch与tensorflow对比</title>
    <link href="/2022/04/22/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94/"/>
    <url>/2022/04/22/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch和tensorFlow的区别"><a href="#pytorch和tensorFlow的区别" class="headerlink" title="pytorch和tensorFlow的区别"></a>pytorch和tensorFlow的区别</h1><p>ytorch是一个动态的框架，而TensorFlow是一个静态的框架</p><p>TensorFlow的尿性是：</p><ol><li>需要先构建一个TensorFlow的计算图</li><li>构建好了之后，这样一个计算图是不能够变的了</li><li>然后我们再传入不同的数据进去，进行计算</li></ol><p>固定了计算的流程，势必带来了不灵活性</p><p>pytorch就是一个<strong>动态的框架</strong>，这就和python的逻辑是一样的，要对<strong>变量</strong>做<strong>任何操作</strong>都是灵活的</p><p><img src="/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2liZWxpZXZlODAxMw==,size_16,color_FFFFFF,t_70.png" alt="img"></p><p>【tensorflow】</p><p><img src="/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2liZWxpZXZlODAxMw==,size_16,color_FFFFFF,t_70-16506185603801.png" alt="img"></p><p>【pytorch】</p><ul><li>里面都包含了<strong>建立前向计算图</strong>，<strong>传入变量数据</strong>，<strong>求梯度</strong>等操作</li><li>pytorch的代码更为凝练</li></ul><p><strong>一个好的框架应该要具备三点：</strong></p><ul><li>对大的计算图能方便的实现</li><li>能自动求变量的导数</li><li>能简单的运行在GPU上</li></ul><p>现在很多公司用的都是TensorFlow，而pytorch由于比较<strong>灵活</strong>，在学术科研上用得比较多一点</p><p>TensorFlow在<strong>GPU的分布式</strong>计算上更为出色，在<strong>数据量巨大时</strong>效率比pytorch要高一些</p><p><strong>pytorch包括了三个层次：</strong></p><p>&#x3D;&#x3D;tensor , variable , Module&#x3D;&#x3D;</p><p>tensor</p><ul><li>即张量的意思</li><li>由于是矩阵的运算，十分适合在GPU上跑</li></ul><p>variable</p><ul><li>只是tensor的一个封装</li><li>能够保存住该variable在整个计算图中的位置</li><li>为了反向求梯度</li></ul><p>Module</p><ul><li>是一个更高的层次</li><li>是一个神经网络的层次</li><li>可以直接调用全连接层，卷积层，等等神经网络</li></ul><h2 id="pytorch代码举例"><a href="#pytorch代码举例" class="headerlink" title="pytorch代码举例"></a>pytorch代码举例</h2><p><img src="/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94.assets/image-20220109000328326.png" alt="image-20220109000328326"></p><p><img src="/Pytorch%E4%B8%8Etensorflow%E5%AF%B9%E6%AF%94.assets/image-20220109000832036.png" alt="image-20220109000832036"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch报错合集</title>
    <link href="/2022/04/22/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86/"/>
    <url>/2022/04/22/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch报错合集"><a href="#pytorch报错合集" class="headerlink" title="pytorch报错合集"></a>pytorch报错合集</h1><h2 id="RGBA问题"><a href="#RGBA问题" class="headerlink" title="RGBA问题"></a>RGBA问题</h2><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/1647954566571-d9e6428e-a0ad-4482-a119-a14c522ab4ba.png" alt="img"></p><p>Util.py种，加上jpeg格式</p><p>观察了一番，拆分数据好像不是通过split_data进行的</p><h3 id="将alldataset中的两个py文件删掉了"><a href="#将alldataset中的两个py文件删掉了" class="headerlink" title="将alldataset中的两个py文件删掉了"></a>将alldataset中的两个py文件删掉了</h3><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/1647954827730-c4346a26-1750-43a3-8ec6-4b9a9fa47d84.png" alt="img"></p><h3 id="注释掉了"><a href="#注释掉了" class="headerlink" title="注释掉了"></a>注释掉了</h3><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/1647955743477-e2c83579-7815-453b-ab7e-be5b176cb2ac.png" alt="img"></p><p>还是有问题！！！</p><h3 id="重新在dataset下建立了一个父文件夹allclass，再把所有类放进去"><a href="#重新在dataset下建立了一个父文件夹allclass，再把所有类放进去" class="headerlink" title="重新在dataset下建立了一个父文件夹allclass，再把所有类放进去"></a>重新在dataset下建立了一个父文件夹allclass，再把所有类放进去</h3><p>可以了……</p><p>什么有病……%￥￥#%@2123131#￥%#@</p><hr><p>第二天又有问题了……</p><p>不知道是不是全部的上传有问题（昨天最后的时候failed了）</p><p>检查了下最后几个文件夹，有gif类，改了后缀为jpg</p><h2 id="tensorboard无法加载：tensorboard-not-found"><a href="#tensorboard无法加载：tensorboard-not-found" class="headerlink" title="tensorboard无法加载：tensorboard not found"></a>tensorboard无法加载：tensorboard not found</h2><p>尝试了找到根目录等方法，无果</p><p>：</p><p>切换tensorboard版本重新安装</p><p>pip install tensorboard&#x3D;&#x3D;1.15.0</p><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/1647956187283-ae31d6e5-22e3-4dba-9956-29c1ae83715f.png" alt="img"></p><p>但是网页打开什么都没有</p><h3 id="第二天换了一个服务器"><a href="#第二天换了一个服务器" class="headerlink" title="第二天换了一个服务器~"></a>第二天换了一个服务器~</h3><p>成功</p><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/1648001462445-8f342e4d-4701-4bf1-8708-34b7293ffd56.png" alt="img"></p><h2 id="unexpected-keys"><a href="#unexpected-keys" class="headerlink" title="unexpected keys"></a>unexpected keys</h2><h3 id="Unexpected-and-missing-keys-in-state-dict"><a href="#Unexpected-and-missing-keys-in-state-dict" class="headerlink" title="Unexpected and missing keys in state_dict"></a><a href="https://stackoverflow.com/questions/55898666/unexpected-and-missing-keys-in-state-dict-when-converting-pytorch-to-onnx">Unexpected and missing keys in state_dict</a></h3><p>In line 19, try using <code>model=runner.load_state_dict(..., strict=False)</code>.</p><p>Using the parameter <code>strict=False</code> tells the <code>load_state_dict</code> function that there might be <strong>missing keys in the checkpoint,</strong> which usually <strong>come from the BatchNorm layer</strong> as I see in this case.</p><h2 id="ValueError-num-samples-should-be-a-positive-integer-value-but-got-num-samples-x3D-0"><a href="#ValueError-num-samples-should-be-a-positive-integer-value-but-got-num-samples-x3D-0" class="headerlink" title="ValueError: num_samples should be a positive integer value, but got num_samples&#x3D;0"></a>ValueError: num_samples should be a positive integer value, but got num_samples&#x3D;0</h2><p>解决方法：</p><p>\1. 检查dataset中的<strong>路径，路径不对</strong>，读取不到数据。</p><p>​后在data_path中发现，少传了一部分，应该是：new_data&#x2F;all_class，只写了new_data</p><p>\2. 检查Dataset的__len__()函数为何输出为零</p><h2 id="no-module-named-pyplot"><a href="#no-module-named-pyplot" class="headerlink" title="no module named pyplot"></a>no module named pyplot</h2><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/image-20220330160322464.png" alt="image-20220330160322464"></p><p>本来下了matplotlib，但是此处pyplot会报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install matplotlib==2.2.3<br></code></pre></td></tr></table></figure><p>下载低版本后，不报错（原版：3.5.1）</p><p>但是，後來報了pysparsing的錯</p><p>conda install -c conda-forge matplotlib</p><p><img src="/Pytorch%E6%8A%A5%E9%94%99%E5%90%88%E9%9B%86.assets/image-20220330220748621.png" alt="image-20220330220748621"></p><p>安裝上了pyparsing</p><h1 id="floating-point-exception-core-dump"><a href="#floating-point-exception-core-dump" class="headerlink" title="floating point exception(core dump)"></a>floating point exception(core dump)</h1><p>出现这种情况的原因有两个：</p><p>1、<a href="https://so.csdn.net/so/search?q=gcc&spm=1001.2101.3001.7020">gcc</a>版本不匹配 </p><p>2、出现了分母为0的情况</p><h2 id="reload-sys-nameerror-name-‘reload’-is-not-defined"><a href="#reload-sys-nameerror-name-‘reload’-is-not-defined" class="headerlink" title="reload(sys) nameerror name ‘reload’ is not defined"></a>reload(sys) nameerror name ‘reload’ is not defined</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pthon">import sys<br># reload(sys)<br># sys.setdefaultencoding(&#x27;utf8&#x27;)<br>import importlib<br>importlib.reload(sys)<br></code></pre></td></tr></table></figure><p>版本问题</p><p>不能直接reload</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch杂碎记录</title>
    <link href="/2022/04/22/Pytorch%E6%9D%82%E7%A2%8E%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/04/22/Pytorch%E6%9D%82%E7%A2%8E%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torch<span class="hljs-selector-class">.manual_seed</span>(<span class="hljs-number">0</span>) torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cudnn</span><span class="hljs-selector-class">.deterministic</span> = False torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cudnn</span><span class="hljs-selector-class">.benchmark</span> = True <br></code></pre></td></tr></table></figure><p>​             </p><h2 id="TORCH-BACKENDS"><a href="#TORCH-BACKENDS" class="headerlink" title="TORCH.BACKENDS"></a><strong>TORCH.BACKENDS</strong></h2><p>控制PyTorch支持的<strong>各种后端行为</strong></p><p>These backends include:</p><ul><li>torch.backends.cuda</li><li>torch.backends.cudnn</li><li>torch.backends.mkl</li><li>torch.backends.mkldnn</li><li>torch.backends.openmp</li></ul><h3 id="torch-backends-cuda"><a href="#torch-backends-cuda" class="headerlink" title="torch.backends.cuda"></a><strong>torch.backends.cuda</strong></h3><h4 id="torch-backends-cuda-is-built"><a href="#torch-backends-cuda-is-built" class="headerlink" title="torch.backends.cuda.is_built()"></a>torch.backends.cuda.is_built()</h4><p>返回PyTorch<strong>是否支持CUDA</strong>。请注意，这并<strong>不一定意味着CUDA可用</strong>;只是如果这个PyTorch二进制文件<strong>运行在一台有CUDA驱动和设备的机器上</strong>，我们就<strong>可以使用它。</strong></p><h3 id="torch-backends-cudnn"><a href="#torch-backends-cudnn" class="headerlink" title="torch.backends.cudnn"></a><strong>torch.backends.cudnn</strong></h3><h4 id="torch-backends-cudnn-version"><a href="#torch-backends-cudnn-version" class="headerlink" title="torch.backends.cudnn.version() "></a>torch.backends.cudnn.version()<a href="https://pytorch.org/docs/stable/_modules/torch/backends/cudnn.html#version"> </a></h4><p>返回cuDNN的版本<a href="https://pytorch.org/docs/stable/_modules/torch/backends/cudnn.html#version"> </a></p><h4 id="torch-backends-cudnn-is-available"><a href="#torch-backends-cudnn-is-available" class="headerlink" title="torch.backends.cudnn.is_available**()**"></a>torch.backends.cudnn.is_available**()**</h4><p>返回一个bool值，指示<strong>CUDNN当前是否可用。</strong></p><h4 id="torch-backends-cudnn-enabled"><a href="#torch-backends-cudnn-enabled" class="headerlink" title="torch.backends.cudnn.enabled"></a>torch.backends.cudnn.enabled</h4><p>控制是否启用cuDNN的bool。</p><h4 id="x3D-x3D-torch-backends-cudnn-deterministic-x3D-x3D"><a href="#x3D-x3D-torch-backends-cudnn-deterministic-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;torch.backends.cudnn.deterministic&#x3D;&#x3D;"></a>&#x3D;&#x3D;torch.backends.cudnn.deterministic&#x3D;&#x3D;</h4><p>如果为True，则导致cuDNN只使用<strong>确定性卷积算法</strong></p><h2 id="TORCH-USE-DETERMINISTIC-ALGORITHMS"><a href="#TORCH-USE-DETERMINISTIC-ALGORITHMS" class="headerlink" title="TORCH.USE_DETERMINISTIC_ALGORITHMS"></a><strong>TORCH.USE_DETERMINISTIC_ALGORITHMS</strong></h2><p><strong>设置 PyTorch 操作是否必须使用“确定性”算法</strong></p><ul><li>在给定相同输入的情况下，在相同的软件和硬件上运行时，算法总是产生相同的输出</li></ul><h4 id="x3D-x3D-torch-backends-cudnn-benchmark-x3D-x3D"><a href="#x3D-x3D-torch-backends-cudnn-benchmark-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;torch.backends.cudnn.benchmark&#x3D;&#x3D;"></a>&#x3D;&#x3D;torch.backends.cudnn.benchmark&#x3D;&#x3D;</h4><p>使cuDNN对<strong>多个卷积算法</strong>进行基准测试，<strong>并选择最快的卷积算法</strong></p><h2 id="torch-manual-seed-1"><a href="#torch-manual-seed-1" class="headerlink" title="torch.manual_seed(1)"></a>torch.manual_seed(1)</h2><h3 id="解释1"><a href="#解释1" class="headerlink" title="解释1"></a>解释1</h3><ul><li>在神经网络中，参数默认是进行<strong>随机初始化</strong>的</li><li>如果不设置的话<strong>每次训练时的初始化都是随机的</strong>，导致结果不确定</li><li>果<strong>设置初始化</strong>，则<strong>每次初始化都是固定的</strong></li><li>如果使用<strong>多个GPU</strong>，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子</li></ul><h3 id="解释2"><a href="#解释2" class="headerlink" title="解释2"></a>解释2</h3><ul><li>由于每次实验都需要生成数据，设置随机种子是为了确保<strong>每次生成固定的随机数（生成的随机数是一样的）</strong></li><li>使得<strong>每次实验结果显示一致了</strong></li><li>有利于实验的比较和改进</li><li>在需要生成随机数的实验中，使得每次运行该 python文件时<strong>生成的随机数相同</strong></li><li>torch.manual_seed()为 <strong>CPU&#x2F;GPU</strong> 中设置种子，生成随机数</li></ul><h3 id="解释3"><a href="#解释3" class="headerlink" title="解释3"></a>解释3</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">1</span>)<br><span class="hljs-attribute">torch</span>.rand(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><ul><li>👆输出结果一样，每一次rand都是一样的</li><li>若去掉 <strong>torch.manual_seed(1)</strong> 直接torch.rand（1,2） 则生成的结果是不一样的</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br>torch<span class="hljs-selector-class">.manual_seed</span>(<span class="hljs-number">0</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.rand(<span class="hljs-number">1</span>)</span></span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(torch.rand(<span class="hljs-number">1</span>)</span></span>)<br></code></pre></td></tr></table></figure><p>是每次运行<code>test.py</code>文件的输出结果都一样，而不是每次随机函数生成的结果一样</p><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([<span class="hljs-number">0.4963</span>])<br>tensor([<span class="hljs-number">0.7682</span>])<br></code></pre></td></tr></table></figure><h2 id="torch-autograd-Variable"><a href="#torch-autograd-Variable" class="headerlink" title="torch.autograd.Variable"></a>torch.autograd.Variable</h2><h3 id="了解Variable"><a href="#了解Variable" class="headerlink" title="了解Variable"></a>了解Variable</h3><p>一种可以变化的变量，这正好就符合了<a href="https://so.csdn.net/so/search?q=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&spm=1001.2101.3001.7020">反向传播</a>，参数更新的属性</p><p>Variable就是一个存放会变化值的地理位置</p><p>里面的值（tensor）会不停发生变化</p><p>pytorch都是有tensor计算的</p><p>tensor里面的参数都是Variable的形式</p><p>&#x3D;&#x3D;【<a href="https://so.csdn.net/so/search?q=tensor&spm=1001.2101.3001.7020">tensor</a> 是一个多维矩阵】&#x3D;&#x3D;</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs clean"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable # torch 中 Variable 模块<br>tensor = torch.FloatTensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])<br># 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度<br>variable = Variable(tensor, requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;注：tensor不能反向传播，variable可以反向传播。&#x3D;&#x3D;</p><h1 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h1><p>图形的基本构建模块(计算图？)</p><p><img src="/Pytorch%E6%9D%82%E7%A2%8E%E8%AE%B0%E5%BD%95.assets/image-20220128154152732.png" alt="image-20220128154152732"></p><h2 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a><a href="https://pytorch.org/docs/stable/nn.html#id1">Containers</a></h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>Module</code></a></h3><h3 id="Sequential顺序容器"><a href="#Sequential顺序容器" class="headerlink" title="Sequential顺序容器"></a><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"><code>Sequential顺序容器</code></a></h3><p><em>CLASS</em><code>torch.nn.``Sequential</code>(*<em>args</em>)[](<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential">https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential</a></p><ul><li>模块将按照它们在构造函数中传递的顺序添加</li><li>按照顺序执行</li><li>输入给1</li><li>1的输出给2</li><li>最后返回最后一层的输出</li><li>按顺序排列的层按<strong>级联方式</strong>连接</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Using Sequential to create a small model. When `model` is run,</span><br><span class="hljs-comment"># input will first be passed to `Conv2d(1,20,5)`. The output of</span><br><span class="hljs-comment"># `Conv2d(1,20,5)` will be used as the input to the first</span><br><span class="hljs-comment"># `ReLU`; the output of the first `ReLU` will become the input</span><br><span class="hljs-comment"># for `Conv2d(20,64,5)`. Finally, the output of</span><br><span class="hljs-comment"># `Conv2d(20,64,5)` will be used as input to the second `ReLU`</span><br>model = nn.Sequential(<br>          nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),<br>          nn.ReLU(),<br>          nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),<br>          nn.ReLU()<br>        )<br><br><span class="hljs-comment"># Using Sequential with OrderedDict. This is functionally the</span><br><span class="hljs-comment"># same as the above code</span><br>model = nn.Sequential(OrderedDict([<br>          (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),<br>          (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU()),<br>          (<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),<br>          (<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br>        ]))<br></code></pre></td></tr></table></figure><blockquote><p>加上OrderdeDict有序字典效果一样，</p><p>只是写法不同，要再前面加上名字</p></blockquote><h2 id="Tensor-view"><a href="#Tensor-view" class="headerlink" title="Tensor.view"></a><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view"><code>Tensor.view</code></a></h2><p>返回与张量具有相同数据<code>self</code>但不同的新张量<code>shape</code></p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">x = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">x.size()</span><br>torch.Size([4, 4])<br></code></pre></td></tr></table></figure><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">z = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)  <span class="hljs-comment"># the size -1 is inferred from other dimensions</span></span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">z.size()</span><br>torch.Size([2, 8])<br></code></pre></td></tr></table></figure><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">y = x.view(<span class="hljs-number">16</span>)</span><br><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">y.size()</span><br>torch.Size([16])<br></code></pre></td></tr></table></figure><h2 id="torch-argmax-输入"><a href="#torch-argmax-输入" class="headerlink" title="torch.argmax(输入)"></a><code>torch.argmax</code>(<em>输入</em>)</h2><p>返回张量中<strong>所有元素的最大值的索引</strong><code>input</code></p><blockquote><p>如果有多个最大值，则返回<strong>第一个最大值</strong>的索引。</p></blockquote><h2 id="torch-argmax-输入，dim，keepdim-x3D-False"><a href="#torch-argmax-输入，dim，keepdim-x3D-False" class="headerlink" title="torch.argmax(输入，dim，keepdim &#x3D; False)"></a><code>torch.argmax</code>(<em>输入</em>，dim，<em>keepdim &#x3D; False</em>)</h2><p>参数</p><ul><li><strong>input</strong> ( <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a> ) – 输入张量。</li><li><strong>dim</strong> ( <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> ) – 要减少的维度。如果<code>None</code>，则返回展平输入的 argmax。</li><li><strong>keepdim</strong> ( <a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a> ) – 输出张量是否<code>dim</code>保留。忽略如果<code>dim=None</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br>a<br>tensor([[ <span class="hljs-number">1.3398</span>,  <span class="hljs-number">0.2663</span>, -<span class="hljs-number">0.2686</span>,  <span class="hljs-number">0.2450</span>],<br>        [-<span class="hljs-number">0.7401</span>, -<span class="hljs-number">0.8805</span>, -<span class="hljs-number">0.3402</span>, -<span class="hljs-number">1.1936</span>],<br>        [ <span class="hljs-number">0.4907</span>, -<span class="hljs-number">1.3948</span>, -<span class="hljs-number">1.0691</span>, -<span class="hljs-number">0.3132</span>],<br>        [-<span class="hljs-number">1.6092</span>,  <span class="hljs-number">0.5419</span>, -<span class="hljs-number">0.2993</span>,  <span class="hljs-number">0.3195</span>]])<br>torch.argmax(a, dim=<span class="hljs-number">1</span>)<br>tensor([ <span class="hljs-number">0</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><strong>dim&#x3D;0</strong>：沿每个↓找</p><p><strong>dim&#x3D;1</strong>，沿每个→找</p><p>tensor([[ <strong>1.3398,</strong>  0.2663, -0.2686,  0.2450],<br>        [-0.7401, -0.8805, <strong>-0.3402</strong>, -1.1936],<br>        [ <strong>0.4907</strong>, -1.3948, -1.0691, -0.3132],<br>        [-1.6092,  <strong>0.5419</strong>, -0.2993,  0.3195]])</p><h1 id="python中"><a href="#python中" class="headerlink" title="python中"></a>python中</h1><h2 id="glob-glob-文件全提函数"><a href="#glob-glob-文件全提函数" class="headerlink" title="glob.glob()文件全提函数"></a>glob.glob()文件全提函数</h2><p>适用场景：深度学习的<strong>训练集、测试集一般放在不同的文件夹中</strong>，因此需要获取<strong>所有文件</strong>的地址</p><p>&#x3D;&#x3D;此时glob.glob()函数就可以非常方便的一次性获取<strong>一个文件夹内所有文件的地址</strong>&#x3D;&#x3D;</p><p>并把地址转为<strong>字符串</strong>形式</p><blockquote><p>文件全提，用 * 表示，最好用 &#x2F; 间隔</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> glob<br><br><span class="hljs-comment"># 最后train文件夹内的所有图片全要：</span><br>train_dataset_path = glob.glob(<span class="hljs-string">&#x27;E:/ceshi/train/*&#x27;</span>)<br><br><span class="hljs-built_in">len</span>( train_dataset_path )<br><span class="hljs-number">344</span><br><br><span class="hljs-built_in">type</span>( train_dataset_path[<span class="hljs-number">0</span>] )<br><span class="hljs-built_in">str</span><br></code></pre></td></tr></table></figure><blockquote><p>tensorflow官方喜欢用<strong>pathlib</strong>这个包进行文件的读取，但是我觉得用glob更加方便！</p></blockquote><h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><h2 id="numpy-concatenate"><a href="#numpy-concatenate" class="headerlink" title="numpy.concatenate"></a>numpy.concatenate</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">a</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[1, 2]</span>, <span class="hljs-selector-attr">[3, 4]</span>])<br>&gt;&gt;&gt; <span class="hljs-selector-tag">b</span> = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[5, 6]</span>])<br>&gt;&gt;&gt; np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>, b), axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[[1, 2]</span>,<br>       <span class="hljs-selector-attr">[3, 4]</span>,<br>       <span class="hljs-selector-attr">[5, 6]</span>])<br>&gt;&gt;&gt; np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>, <span class="hljs-selector-tag">b</span>.T), axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">array</span>(<span class="hljs-selector-attr">[[1, 2, 5]</span>,<br>       <span class="hljs-selector-attr">[3, 4, 6]</span>])<br>&gt;&gt;&gt; np<span class="hljs-selector-class">.concatenate</span>((<span class="hljs-selector-tag">a</span>, b), axis=None)<br><span class="hljs-function"><span class="hljs-title">array</span><span class="hljs-params">([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])</span></span><br></code></pre></td></tr></table></figure><h2 id="numpy-vstack"><a href="#numpy-vstack" class="headerlink" title="numpy.vstack"></a>numpy.vstack</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([[<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b = np.array([[<span class="hljs-number">4</span>], [<span class="hljs-number">5</span>], [<span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.vstack((a,b))<br>array([[<span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>],<br>       [<span class="hljs-number">4</span>],<br>       [<span class="hljs-number">5</span>],<br>       [<span class="hljs-number">6</span>]])<br>       <br>a = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>b = np.array([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>np.vstack((a,b))<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>       [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br></code></pre></td></tr></table></figure><p>按行连接</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MarcovChain马尔科夫链</title>
    <link href="/2022/04/22/MarcovChain%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/"/>
    <url>/2022/04/22/MarcovChain%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="Marcov-chain"><a href="#Marcov-chain" class="headerlink" title="Marcov chain"></a>Marcov chain</h1><p><a href="https://zhuanlan.zhihu.com/p/26453269">https://zhuanlan.zhihu.com/p/26453269</a></p><h2 id="转移概率矩阵P"><a href="#转移概率矩阵P" class="headerlink" title="转移概率矩阵P"></a>转移概率矩阵P</h2><p><img src="/MarcovChain%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE.assets/v2-a37633055694cccb8532ebafc15b66d8_1440w.jpg" alt="img"></p><h2 id="预测第N天的状态"><a href="#预测第N天的状态" class="headerlink" title="预测第N天的状态"></a>预测第N天的状态</h2><p><img src="/MarcovChain%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE.assets/v2-6f323d3b7b9a48382541e59d77906871_1440w.jpg" alt="img"></p><p> Sn &#x3D; Sn-1 * P (看见没，只跟它前面一个状态Sn-1有关)</p><p><strong>总结：马尔可夫链就是这样一个任性的过程，它将来的状态分布只取决于现在，跟过去无关！</strong></p><h2 id="随机变量随时间按照Markov性进行变化"><a href="#随机变量随时间按照Markov性进行变化" class="headerlink" title="随机变量随时间按照Markov性进行变化"></a>随机变量随时间按照Markov性进行变化</h2><p><img src="/MarcovChain%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE.assets/v2-13a9848cdad479aaafa282e51c0d6e72_1440w.jpg" alt="img"></p><p><a href="https://blog.csdn.net/mid_Faker/article/details/106867652">https://blog.csdn.net/mid_Faker/article/details/106867652</a></p><ul><li><strong>马尔可夫链（Markov chain）</strong>是<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1&spm=1001.2101.3001.7020">数学建模</a>和机器学习常用的工具</li><li>为状态空间中经过从一个状态到另一个状态的转换的随机过程</li><li>该过程要求具备<strong>“无记忆”</strong>的性质：下一状态的概率分布<strong>只能由当前状态决定</strong>，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作<strong>马尔可夫性质</strong>。</li></ul><p><a href="https://zhuanlan.zhihu.com/p/102014899">https://zhuanlan.zhihu.com/p/102014899</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KNN算法</title>
    <link href="/2022/04/22/KNN%E7%AE%97%E6%B3%95/"/>
    <url>/2022/04/22/KNN%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="kNN算法-HOMEWORK"><a href="#kNN算法-HOMEWORK" class="headerlink" title="kNN算法[HOMEWORK]"></a>kNN算法[HOMEWORK]</h1><h2 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h2><ul><li>基本分类与回归方法</li><li>监督学习：样本集中每个数据都存在标签<ul><li>样本集中每一个数据与所属分类的对应关系</li></ul></li><li>新数据的每个特征与样本集中数据<strong>对应的特征进行比较</strong>，算法提取<strong>样本最相似数据(最近邻)的分类标签</strong></li><li>k-近邻算法中k：只选择样本数据集中<strong>前k个最相似的数据</strong></li><li>选择k个最相似数据中<strong>出现次数最多的分类</strong></li></ul><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>一个样本在特征空间中的<strong>k个最相邻的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别</p><ul><li>只依据<strong>最邻近的一个或者几个</strong>样本的类别来决定待分样本所属的类别</li><li>“近朱者赤，近墨者黑”，由你的邻居来推断出你的类别</li><li>空间内两个点的距离来度量。距离越大，表示两个点越不相似</li></ul><h3 id="类别判定"><a href="#类别判定" class="headerlink" title="类别判定"></a>类别判定</h3><ul><li><p>对<strong>近邻</strong>的投票进行加权，<strong>距离越近则权重越大</strong>（权重为距离平方的倒数）</p></li><li><p>近邻中<strong>哪个类别的点最多</strong>就分为该类</p></li></ul><h2 id="适用于"><a href="#适用于" class="headerlink" title="适用于"></a>适用于</h2><ul><li>类域的<strong>交叉或重叠较多</strong>的待分样本集</li><li>不仅可以用于分类，还可以用于回归<ul><li>该样本的属性：将k个邻居的属性的<strong>平均值</strong>赋给该样本</li><li>更好的方法：将不同距离的邻居对该样本产生的影响<strong>给予不同的权值(weight)</strong><ul><li>e.g.距离越远，权值越小，影响越小</li></ul></li></ul></li><li>&#x3D;&#x3D;适合于<strong>多分类问题</strong>(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好&#x3D;&#x3D;</li></ul><h2 id="kNN计算流程"><a href="#kNN计算流程" class="headerlink" title="kNN计算流程"></a>kNN计算流程</h2><ul><li>计算<strong>已知类别数据集中的点</strong>与<strong>当前点</strong>之间的距离；<ul><li>inX这个要判别分类的点到dataSet中每个点之间的距离</li><li>dataSet为一个矩阵，<strong>每一行</strong>表示已知类别数据集中的<strong>一个点</strong>（一个向量）</li></ul></li><li>按照距离递增次序排序；<ul><li>distances.argsort() 从小到大排序</li></ul></li><li>选取与当前点距离最小的k个点；</li><li>确定前k个点所在类别的出现频率；</li><li>返回前k个点所出现频率最高的类别作为当前点的预测分类。</li></ul><p>&#x3D;&#x3D;shape[0]:行数&#x3D;&#x3D;</p><p>&#x3D;&#x3D;shape[1]:列数&#x3D;&#x3D;</p><p>np.tile(inX,(m, 1)) 行上复制m次，列上复制1次（即列不复制）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>np.tile(b, (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>))<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>       [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br></code></pre></td></tr></table></figure><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="numpy-argsort"><a href="#numpy-argsort" class="headerlink" title="numpy.argsort"></a>numpy.argsort</h3><p>numpy.argsort(a, axis&#x3D;- 1, kind&#x3D;None, order&#x3D;None)</p><p>axis：默认-1（最后axis）</p><p>axis&#x3D;0表示列相加，axis&#x3D;1表示行相加</p><p>kind：<strong>‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’</strong></p><p>order：<strong>str or list of str</strong></p><h2 id="python开方"><a href="#python开方" class="headerlink" title="python开方"></a>python开方</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cmake">import <span class="hljs-keyword">math</span><br><span class="hljs-keyword">math</span>.sqrt(<span class="hljs-number">144</span>) <span class="hljs-comment"># 12</span><br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pow</span>(<span class="hljs-number">144</span>, <span class="hljs-number">0</span>.<span class="hljs-number">5</span>) # <span class="hljs-number">12</span><br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">144</span>**<span class="hljs-number">0</span>.<span class="hljs-number">5</span> # <span class="hljs-number">12</span><br></code></pre></td></tr></table></figure><h2 id="Python-字典-Dictionary-get"><a href="#Python-字典-Dictionary-get" class="headerlink" title="Python 字典(Dictionary) get()"></a>Python 字典(Dictionary) get()</h2><p> <strong>get()</strong> 函数返回指定键的值。</p><p>classCount.get(voteIlabel,0)：</p><ul><li>指定了label键，默认值为0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 没有设置 Sex，也没有设置默认的值，输出 None</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Sex : %s&quot;</span> %  tinydict.get(<span class="hljs-string">&#x27;Sex&#x27;</span>))  <br><br><span class="hljs-comment"># 没有设置 Salary，输出默认的值  0.0</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;Salary: %s&#x27;</span> % tinydict.get(<span class="hljs-string">&#x27;Salary&#x27;</span>, <span class="hljs-number">0.0</span>))<br></code></pre></td></tr></table></figure><h2 id="Python-sorted"><a href="#Python-sorted" class="headerlink" title="Python sorted()"></a>Python sorted()</h2><p><strong>sorted()</strong> 函数对所有可迭代的对象进行<strong>排序操作</strong></p><p><em>sort 是应用在 <strong>list</strong> 上的方法，sorted 可以对<strong>所有可迭代的对象</strong>进行排序操作</em></p><p><em>sort 方法返回的是对已经存在的列表进行操作，无返回值</em></p><p><em>sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作</em></p><h2 id="assert"><a href="#assert" class="headerlink" title="assert"></a>assert</h2><p>程序在我的假设条件下，能够正常良好的运作，其实就相当于一个 if 语句</p><p>测试一下，一些最坏情况是否发生，所以这里有了 assert()。</p><p>其作用是如果它的条件返回错误，则终止程序执行</p><h2 id="Math-floor"><a href="#Math-floor" class="headerlink" title="Math.floor()"></a>Math.floor()</h2><p>返回小于或等于一个<strong>给定数字的最大整数</strong></p><p><strong>向下取整</strong></p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a><a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Math/floor#syntax">语法</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Math.floor(x)<br></code></pre></td></tr></table></figure><p>dataset.data:</p><h3 id="sklearn-数据库"><a href="#sklearn-数据库" class="headerlink" title="sklearn 数据库"></a>sklearn 数据库</h3><p>import sklearn.datasets as datasets</p><p>主要有两种：<br>- 封装好的经典数据。<strong>eg</strong>: boston 房价, 糖尿病, 数字, Iris 花。在代码中以“load”开头。</p><p>- 自己设计参数，然后生成的数据，例如用来训练线性回归模型的数据（强大）。在代码中以“make”开头</p><p><img src="https://morvanzhou.github.io/static/results/sklearn/2_3_1.png" alt="image"></p><h2 id="sklearn-model-selection-train-test-split"><a href="#sklearn-model-selection-train-test-split" class="headerlink" title="sklearn.model_selection.train_test_split"></a><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"><code>sklearn.model_selection</code></a>.train_test_split</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sklearn.model_selection.train_test_split(<span class="hljs-number">*a</span>rrays, <span class="hljs-attribute">test_size</span>=None, <span class="hljs-attribute">train_size</span>=None, <span class="hljs-attribute">random_state</span>=None, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">stratify</span>=None)[source]<br></code></pre></td></tr></table></figure><p>将数组&#x2F;矩阵随机打乱分成训练、测试样本</p><p><strong>test_size</strong>：</p><ul><li>测试集所占的比例，0-1之间或代表测试集的整数</li></ul><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308150715986.png" alt="image-20220308150715986"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308150728273.png" alt="image-20220308150728273"></p><ul><li>原本样本数：150，0.5取样后75</li><li>idx&#x3D;75</li><li>X有75行，y有75列</li><li>ystat中有3类，每类有不同样本数【每次随机】</li><li>75*0.2&#x3D;15个测试样本</li></ul><p>采样率换成0.1：</p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308151045761.png" alt="image-20220308151045761"></p><ul><li>150*0.1&#x3D;15</li><li>15个样本中有3类</li></ul><h2 id="random-sample"><a href="#random-sample" class="headerlink" title="random.sample"></a>random.sample</h2><p>对于random.sample的用法，多用于<strong>截取列表的指定长度的随机数</strong>，但是不会改变列表本身的排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span> = [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br>rs = random.sample(<span class="hljs-built_in">list</span>, <span class="hljs-number">2</span>) <span class="hljs-comment">#输出[2,4]，随机找了2个list中的数</span><br></code></pre></td></tr></table></figure><ul><li>此数组随着不同的执行，里面的元素随机，但都是两个</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">rs = random.sample(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">9</span>), <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(rs)<br><br><br>》》》[<span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><ul><li>使用range（），自动生成一个list:[0,1.2….9]</li></ul><h2 id="collections-—-Container-datatypes"><a href="#collections-—-Container-datatypes" class="headerlink" title="collections — Container datatypes"></a><a href="https://docs.python.org/3/library/collections.html#module-collections"><code>collections</code></a> — Container datatypes</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Tally occurrences of words in a list</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cnt = Counter()<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-string">&#x27;green&#x27;</span>, <span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-string">&#x27;blue&#x27;</span>]:<br><span class="hljs-meta">... </span>    cnt[word] += <span class="hljs-number">1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cnt<br>Counter(&#123;<span class="hljs-string">&#x27;blue&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;red&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;green&#x27;</span>: <span class="hljs-number">1</span>&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Find the ten most common words in Hamlet</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> re<br><span class="hljs-meta">&gt;&gt;&gt; </span>words = re.findall(<span class="hljs-string">r&#x27;\w+&#x27;</span>, <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hamlet.txt&#x27;</span>).read().lower())<br><span class="hljs-meta">&gt;&gt;&gt; </span>Counter(words).most_common(<span class="hljs-number">10</span>)<br>[(<span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-number">1143</span>), (<span class="hljs-string">&#x27;and&#x27;</span>, <span class="hljs-number">966</span>), (<span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-number">762</span>), (<span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-number">669</span>), (<span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-number">631</span>),<br> (<span class="hljs-string">&#x27;you&#x27;</span>, <span class="hljs-number">554</span>),  (<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-number">546</span>), (<span class="hljs-string">&#x27;my&#x27;</span>, <span class="hljs-number">514</span>), (<span class="hljs-string">&#x27;hamlet&#x27;</span>, <span class="hljs-number">471</span>), (<span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-number">451</span>)]<br></code></pre></td></tr></table></figure><h1 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h1><h2 id="dataset-x3D-datasets-load-iris"><a href="#dataset-x3D-datasets-load-iris" class="headerlink" title="dataset &#x3D; datasets.load_iris()"></a>dataset &#x3D; datasets.load_iris()</h2><p>sample_rate &#x3D; 0.5 # 采样率（可以选择小于1）</p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308152953872.png" alt="image-20220308152953872"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154044641.png" alt="image-20220308154044641"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154101537.png" alt="image-20220308154101537"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154143187.png" alt="image-20220308154143187"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154155165.png" alt="image-20220308154155165"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154208057.png" alt="image-20220308154208057"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308154218279.png" alt="image-20220308154218279"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308153131717.png" alt="image-20220308153131717"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308164311481.png" alt="image-20220308164311481"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308164609119.png" alt="image-20220308164609119"></p><p><img src="/KNN%E7%AE%97%E6%B3%95.assets/image-20220308164628930.png" alt="image-20220308164628930"></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>课程作业</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CV炼丹技巧总结2</title>
    <link href="/2022/04/22/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932/"/>
    <url>/2022/04/22/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932/</url>
    
    <content type="html"><![CDATA[<h1 id="cv调参经验汇总"><a href="#cv调参经验汇总" class="headerlink" title="cv调参经验汇总"></a>cv调参经验汇总</h1><h2 id="1-数据集准备"><a href="#1-数据集准备" class="headerlink" title="1 数据集准备"></a>1 数据集准备</h2><ul><li><p>大量、高质量且带有准确标签</p></li><li><h3 id="验证集使用："><a href="#验证集使用：" class="headerlink" title="验证集使用："></a>验证集使用：</h3><ul><li>使用验证集，可以知道<strong>什么时候开始降低学习率</strong>和<strong>什么时候停止训练</strong>；</li></ul></li><li><h3 id="数据增广："><a href="#数据增广：" class="headerlink" title="数据增广："></a><strong><strong>数据增广：</strong></strong></h3><ul><li>数据尽可能多：扭转剪切分割以扩充</li></ul></li></ul><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2 数据预处理"></a>2 数据预处理</h2><ul><li>0均值和1方差化</li><li>其他方法</li><li>打乱训练集顺序</li></ul><h2 id="3-参数设定"><a href="#3-参数设定" class="headerlink" title="3 参数设定"></a>3 参数设定</h2><ul><li>将各个参数的设置部分集中在一起，否则调参很麻烦</li><li>先参考相关论文，<strong>以论文中给出的参数作为初始参数</strong></li><li>先调重要、对实验结果影响大的参数(lr&gt;正则值\dropout值)</li></ul><h3 id="batch-size"><a href="#batch-size" class="headerlink" title="batch-size:"></a>batch-size:</h3><ul><li><p>取全量的数据进行梯度的更新:</p><ul><li>梯度总能朝正确的方向进行下降</li><li>一个epoch计算时间会较多</li></ul></li><li><p>one by one:</p><ul><li>梯度的游走路径波动显然会很大</li></ul></li><li><h2 id="取部分样本数据也就是mini-batch-size"><a href="#取部分样本数据也就是mini-batch-size" class="headerlink" title="取部分样本数据也就是mini batch size:"></a>取部分样本数据也就是mini batch size:</h2></li></ul><h3 id="minibatch："><a href="#minibatch：" class="headerlink" title="minibatch："></a>minibatch：</h3><ul><li>一般建议用128,<strong>8</strong>这组，但是<strong>128</strong>,<strong>1</strong>也很好，只是效率会非常慢</li><li>千万不要用<strong>过大</strong>的数值，否则很容易<strong>过拟合</strong></li></ul><h3 id="梯度归一化："><a href="#梯度归一化：" class="headerlink" title="梯度归一化："></a>梯度归一化：</h3><p>&#x3D;&#x3D;其实就是计算出来梯度之后，要除以<strong>Minibatch</strong>的数量，这个可以通过阅读源码得知（我之前有写过<strong>SGD</strong>）；&#x3D;&#x3D;</p><h3 id="学习率lr："><a href="#学习率lr：" class="headerlink" title="学习率lr："></a>学习率lr：</h3><ul><li><p>0.001 0.01 0.1 1 10（以10为阶数进行尝试）</p></li><li><p>一般都会有<strong>默认</strong>的学习率，但是刚开始还是用<strong>一般的</strong>去学习，然后<strong>逐渐的减小</strong>它；</p><ul><li>一般值：建议值是<strong>0.1</strong></li></ul></li><li><p>多小？对于大数据，0.00001都不为过</p></li><li><p>一个对于调度学习率的建议：如果在<strong>验证集上性能不再增加</strong>就让*<em>学习率除以2或者5</em>***，然后继续，学习率会一直变得很小，到最后就可以停止训练了；</p></li><li><p>一般要<strong>随着训练进行衰减</strong>，验证集准确率不再上升时，或固定训练多少个周期以后<img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220322105528867.png" alt="image-20220322105528867"></p></li></ul><h3 id="weight初始化"><a href="#weight初始化" class="headerlink" title="weight初始化"></a>weight初始化</h3><h3 id="dropout值"><a href="#dropout值" class="headerlink" title="dropout值"></a>dropout值</h3><ul><li>0.3 0.5 0.7</li><li>L1和L2这样的正则化技术通过<strong>修改代价函数</strong>来减少过拟合。而丢弃法<strong>修改神经网络本身</strong></li><li>当我们丢弃不同神经元集合（∵每次随机）的时候，就等同于训练不同的神经网络</li></ul><h3 id="准确率低-x2F-欠拟合"><a href="#准确率低-x2F-欠拟合" class="headerlink" title="准确率低&#x2F;欠拟合"></a>准确率低&#x2F;欠拟合</h3><ul><li>增强模型的拟合能力</li><li>增加网络层数</li><li>增加节点数</li><li>减少dropout值</li><li>减少L2正则值</li></ul><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><ul><li>提高模型泛化能力的方向</li><li>与上相反，增加dropout、</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>在<strong>损失函数</strong>后面加一个正则化项，常见的有L1正则化和L2正则化</p><h5 id="L1惩罚项："><a href="#L1惩罚项：" class="headerlink" title="L1惩罚项："></a>L1惩罚项：</h5><ul><li>是使<strong>权重绝对值</strong>最小化</li><li><strong>数据足够简单</strong>，可以精确建模的话，L1更适合</li></ul><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220326133436512.png" alt="image-20220326133436512"></p><h5 id="L2惩罚项："><a href="#L2惩罚项：" class="headerlink" title="L2惩罚项："></a>L2惩罚项：</h5><ul><li>使<strong>权重的平方</strong>最小化</li><li>数据过于复杂时，能够学习<strong>数据中呈现的内在模式</strong></li></ul><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220326133442695.png" alt="image-20220326133442695"></p><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220326133529794.png" alt="image-20220326133529794"></p><blockquote><p>对于我遇到的大多数计算机视觉问题，<strong>L2正则化</strong>几乎总是可以给出更好的结果。然而<strong>L1不容易受到离群值的影响</strong>。所以正确的正则化选项取决于我们想要解决的问题</p></blockquote><h2 id="4-模型调整"><a href="#4-模型调整" class="headerlink" title="4 模型调整"></a>4 模型调整</h2><ul><li><h3 id="dropout："><a href="#dropout：" class="headerlink" title="dropout："></a>dropout：</h3><ul><li>防止过拟合</li></ul></li></ul><h2 id="5-结果评估"><a href="#5-结果评估" class="headerlink" title="5 结果评估"></a>5 结果评估</h2><ul><li>评价最终结果的时候，<strong>多做几次</strong>，然后<strong>平均</strong>一下他们的结果</li><li></li></ul><h2 id="6-画图"><a href="#6-画图" class="headerlink" title="6 画图"></a>6 画图</h2><ul><li>训练数据遍历一轮后，输出一下<strong>训练集和验证集准确率</strong>。同时画到一张图上。(trainloss,valloss-epoch)</li><li>训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数</li></ul><h2 id="7-规范化"><a href="#7-规范化" class="headerlink" title="7 规范化"></a>7 规范化</h2><ul><li>输出模型的损失函数值以及训练集和验证集上的准确率</li><li>设计一个<strong>子程序</strong>，可以根据给定的参数，启动训练并<strong>监控和周期性保存评估结果</strong>。再由一个<strong>主程序</strong>，<strong>分配参数以及并行启动一系列子程序</strong></li></ul><h2 id="8-提高速度"><a href="#8-提高速度" class="headerlink" title="8 提高速度"></a>8 提高速度</h2><ul><li>调参：为了精度</li><li>产出最终模型：需要速度也快</li><li><u>一般在小数据集上合适的参数，在大数据集上效果也不会太差</u><ul><li><u>对数据进行精简</u>，以提高速度、在有限的时间内可以尝试更多参数</li><li><strong>先取一部分数据</strong>（采样法）</li><li><strong>减少训练类别</strong>（例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何）</li></ul></li><li></li></ul><h2 id="9-自动调参"><a href="#9-自动调参" class="headerlink" title="9 自动调参"></a>9 自动调参</h2><h3 id="Gird-Search"><a href="#Gird-Search" class="headerlink" title="Gird Search."></a>Gird Search.</h3><ul><li>每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下</li></ul><h3 id="Random-Search"><a href="#Random-Search" class="headerlink" title="Random Search."></a>Random Search.</h3><ul><li>Random Search比Gird Search更有效</li><li>先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练</li></ul><h3 id="Bayesian-Optimization"><a href="#Bayesian-Optimization" class="headerlink" title="Bayesian Optimization."></a>Bayesian Optimization.</h3><p>- </p><h1 id="炼丹经验贴"><a href="#炼丹经验贴" class="headerlink" title="炼丹经验贴"></a>炼丹经验贴</h1><p>很好：</p><p><a href="https://zhuanlan.zhihu.com/p/56745640">https://zhuanlan.zhihu.com/p/56745640</a></p><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220324211335112.png" alt="image-20220324211335112"></p><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%932.assets/image-20220324212603488.png" alt="image-20220324212603488"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CV炼丹技巧总结</title>
    <link href="/2022/04/22/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/"/>
    <url>/2022/04/22/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="调参-CV炼丹技巧-x2F-经验"><a href="#调参-CV炼丹技巧-x2F-经验" class="headerlink" title="[调参]CV炼丹技巧&#x2F;经验 "></a>[<a href="https://www.cnblogs.com/kk17/p/10156849.html">调参]CV炼丹技巧&#x2F;经验 </a></h1><p><a href="https://www.cnblogs.com/kk17/p/10156849.html">https://www.cnblogs.com/kk17/p/10156849.html</a></p><p>调参就是trial-and-error.</p><p>快速尝试, 快速纠错这是调参的关键</p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>你只是<strong>训练完成后</strong>(或者<strong>准确率到达一个阶段</strong>后), 才能可视化</p><p>权重的可视化[Visualize Layer Weights]</p><h2 id="不满足平滑结果的图像"><a href="#不满足平滑结果的图像" class="headerlink" title="不满足平滑结果的图像"></a>不满足平滑结果的图像</h2><p>数据不好? 没有预处理? 网络结构问题? Learning Rate太大或者太小? 或者就是差了一个LRN层？</p><h2 id="现在的趋势"><a href="#现在的趋势" class="headerlink" title="现在的趋势"></a>现在的趋势</h2><p>鼓励使用<strong>小filter</strong>, <strong>3x3</strong>大小, 多加层次</p><p>具有<strong>不平滑的权重的网络</strong>同样可以获得很好的结果</p><h2 id="基本原则"><a href="#基本原则" class="headerlink" title="基本原则:"></a><strong>基本原则:</strong></h2><h3 id="1-快速试错"><a href="#1-快速试错" class="headerlink" title="1 快速试错"></a><strong>1 快速试错</strong></h3><p>刚开始, 先上<strong>小规模数据</strong>, <strong>模型往大了放</strong>, 只要不爆显存, 能用<strong>256个filter</strong>你就别用128个. 直接奔着<strong>过拟合</strong>去. 没错, 就是训练过拟合网络, 连测试集验证集这些都可以不用</p><h4 id="验证训练代码的流程正确与否"><a href="#验证训练代码的流程正确与否" class="headerlink" title="验证训练代码的流程正确与否"></a>验证训练代码的流程正确与否</h4><ul><li>小数据量<ul><li>生成速度快</li><li>少跑点循环</li></ul></li><li>如果小数据量下，网络很大，都没有朝着过拟合趋近—》输入输出可能有问题</li></ul><h3 id="2-loss设计要合理"><a href="#2-loss设计要合理" class="headerlink" title="2 loss设计要合理"></a>2 loss设计要合理</h3><ul><li><p>一般来说<strong>分类就是Softmax</strong>, <strong>回归就是L2的loss</strong></p></li><li><h4 id="L1loss-amp-L2loss"><a href="#L1loss-amp-L2loss" class="headerlink" title="L1loss&amp;L2loss"></a>L1loss&amp;L2loss</h4></li></ul><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93.assets/image-20220322094426088.png" alt="image-20220322094426088"></p><ul><li><p>绝对值和平方差的区别</p></li><li><p>回归问题中，计算loss时注意输入归一化(normalization)。输出也要归一化(normalization)</p><ul><li>否则使用L2loss时，预测1000和实际0之间的差值太大，影响实际的loss</li></ul></li></ul><h3 id="3-观察loss胜于观察准确率"><a href="#3-观察loss胜于观察准确率" class="headerlink" title="3 观察loss胜于观察准确率"></a>3 观察loss胜于观察准确率</h3><blockquote><p>一般来说，评测指标：准确率</p></blockquote><ul><li>准确率有时会突变，000000–突然—&gt;1</li><li>训练过程观察loss更合理</li><li>不能说前面一段时间没起色就不管了. 有些情况下就是前面一段时间看不出起色, 然后开始稳定学习.</li></ul><h3 id="4-确认分类网络学习充分"><a href="#4-确认分类网络学习充分" class="headerlink" title="4 确认分类网络学习充分"></a>4 确认分类网络学习充分</h3><ul><li>图片分类时，如果<strong>预测各类别都在0.5上下</strong>，说明还没有学习出类别之间的界限，速妖再学习学习</li><li>随着学习过程, 网络预测会慢慢的移动到<strong>0,1这种极值</strong>附近</li></ul><h3 id="5-Learning-Rate设置合理"><a href="#5-Learning-Rate设置合理" class="headerlink" title="5 Learning Rate设置合理"></a>5 Learning Rate设置合理</h3><p><strong>太大，需要调小：</strong></p><ul><li>loss爆炸&#x2F;NAN</li></ul><p><strong>太小，需要调大：</strong></p><ul><li>学不到东西，没有更新</li><li>半天loss没反应</li></ul><p><strong>loss爆炸，太大，NaN：</strong></p><ul><li>太大了，调小</li></ul><p><strong>学不到东西，半天loss没有反应：</strong></p><ul><li>太小了，调大</li></ul><p><strong>loss一路降下来，突然不再降：</strong></p><ul><li>进一步调小</li></ul><blockquote><p><strong>先上一个小lr保证不爆炸，等loss下降后，再升lr，之后再慢慢降lr</strong></p><p>LR在可以工作的最大值下往小收一收, 免得ReLU把神经元弄死了</p></blockquote><h3 id="6-对比训练集和验证集的loss"><a href="#6-对比训练集和验证集的loss" class="headerlink" title="6 对比训练集和验证集的loss"></a>6 对比训练集和验证集的loss</h3><ul><li><p><strong>过拟合</strong>：</p><ul><li>训练集好，验证集差</li></ul></li><li><p><strong>欠拟合</strong>：</p><ul><li>训练集差，验证集差</li></ul></li></ul><h3 id="7-清楚receptive-field的大小"><a href="#7-清楚receptive-field的大小" class="headerlink" title="7 清楚receptive field的大小"></a>7 清楚receptive field的大小</h3><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93.assets/image-20220322095808834.png" alt="image-20220322095808834"></p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项:"></a><strong>注意事项:</strong></h2><ol><li>预处理: -mean&#x2F;std zero-center就够了</li><li>打乱数据</li><li>Dropout</li><li>第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要)</li><li>调参请在验证集上<ol><li>用验证集判断是否过拟合！(validate loss very big)【0.3~0.6】</li><li>好：【&lt;0.01】</li><li>欠拟合：train+val  all【&gt;0.3】</li></ol></li></ol><p><img src="/CV%E7%82%BC%E4%B8%B9%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93.assets/image-20220322100732586.png" alt="image-20220322100732586"></p><p><strong>模型在验证集的表现由两部分组成</strong>:</p><ul><li>对训练集规律的掌握 trainset loss</li><li>对训练集的适应程度 validation set loss</li></ul><p>&#x3D;&#x3D;调参的本质就是要找到那个<strong>best model</strong> 平衡点&#x3D;&#x3D;</p><h2 id="调试hyperparameters-的先后顺序"><a href="#调试hyperparameters-的先后顺序" class="headerlink" title="调试hyperparameters 的先后顺序"></a>调试hyperparameters 的先后顺序</h2><h3 id="1、优先调-learning-rate"><a href="#1、优先调-learning-rate" class="headerlink" title="1、优先调 learning rate"></a><strong>1、优先调 learning rate</strong></h3><p>如果对表现不满意应该优先调学习速率</p><h3 id="2、加-Dropout，-加-BN-加Data-Argument"><a href="#2、加-Dropout，-加-BN-加Data-Argument" class="headerlink" title="2、加 Dropout， 加 BN, 加Data Argument"></a><strong>2、加 Dropout， 加 BN, 加Data Argument</strong></h3><p><strong>设计模型之初</strong>：</p><ul><li><p>模型尽量深，卷积核尽量多，强行让模型拟合训练集</p></li><li><p>容易遇到的问题就是<strong>过拟合</strong></p></li><li><h4 id="过拟合解决方法："><a href="#过拟合解决方法：" class="headerlink" title="过拟合解决方法："></a>过拟合解决方法：</h4><ul><li>正则化</li><li>加大训练数据集<ul><li>多标记</li><li>数据增强（图像裁剪、对称变换、旋转平移）</li></ul></li></ul></li></ul><h3 id="3、调模型的层数和卷积核数量"><a href="#3、调模型的层数和卷积核数量" class="headerlink" title="3、调模型的层数和卷积核数量"></a><strong>3、调模型的层数和卷积核数量</strong></h3><p><strong>增加模型层数（变高）：</strong></p><ul><li><p>这两个参数都是对模型理论容量具有<strong>巨大影响</strong>的参数</p></li><li><p><strong>增大模型的层数</strong>和<strong>卷积核的数量</strong>都会提升模型的容量</p></li><li><p>增大<strong>模型层数</strong>（让模型变高）可以让模型获得<strong>更好的非线性</strong></p><ul><li>缺点：会让模型更难训练，面临梯度消失的风险，引入死单元</li></ul></li></ul><p><strong>增加卷积核（变胖）</strong></p><ul><li><strong>不引入训练困难</strong>的情况下<strong>让模型更好的拟合训练集</strong></li><li>降低 <strong>training loss</strong></li><li>更容易<strong>过拟合</strong></li></ul><p>&#x3D;&#x3D;理想情况下表现优越的模型一般长的<strong>高高瘦瘦</strong>（每层卷积核不多）&#x3D;&#x3D;</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>近两年论文基本都用同样的参数设定：</p><ul><li>几十到几百epoch</li><li>sgd，mini batch size从几十到几百皆可</li><li>步长0.1，可手动收缩</li><li>weight decay取0.005</li><li>momentum取0.9</li><li>dropout加relu</li><li>weight用高斯分布初始化</li><li>bias全初始化为0</li><li>输入特征和预测目标都做好归一化</li></ul><h1 id="如何成为一名成功的“炼丹师”——DL训练技巧"><a href="#如何成为一名成功的“炼丹师”——DL训练技巧" class="headerlink" title="如何成为一名成功的“炼丹师”——DL训练技巧"></a>如何成为一名成功的“炼丹师”——DL训练技巧</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&amp;mid=2247484020&amp;idx=3&amp;sn=6882d32290f44d15cb353ea92849356b&amp;chksm=fb727ea8cc05f7be022025b20d6e8e71170eb5eb1ce383d526aabe8b92a759fd426e9cd114b3#rd">https://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&amp;mid=2247484020&amp;idx=3&amp;sn=6882d32290f44d15cb353ea92849356b&amp;chksm=fb727ea8cc05f7be022025b20d6e8e71170eb5eb1ce383d526aabe8b92a759fd426e9cd114b3#rd</a></p><blockquote><p><strong>DNN（深度神经网络）</strong>在训练过程中遇到的一些问题</p></blockquote><h2 id="1-数据集准备"><a href="#1-数据集准备" class="headerlink" title="1 数据集准备"></a>1 数据集准备</h2><p>大量、高质量且带有准确标签</p><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2 数据预处理"></a>2 数据预处理</h2><ul><li>0均值和1方差化</li></ul><h2 id="3-Minibatch"><a href="#3-Minibatch" class="headerlink" title="3 Minibatch"></a>3 <strong>Minibatch</strong></h2><ul><li>一般建议用128,<strong>8</strong>这组，但是<strong>128</strong>,<strong>1</strong>也很好，只是效率会非常慢</li><li>千万不要用<strong>过大</strong>的数值，否则很容易<strong>过拟合</strong></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CSS</title>
    <link href="/2022/04/22/CSS/"/>
    <url>/2022/04/22/CSS/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="Html-amp-CSS"><a href="#Html-amp-CSS" class="headerlink" title="Html&amp;CSS"></a>Html&amp;CSS</h1><h2 id="关于height-100-和height-100vh的区别"><a href="#关于height-100-和height-100vh的区别" class="headerlink" title="关于height:100%和height:100vh的区别"></a>关于height:100%和height:100vh的区别</h2><p>height:100vh &#x3D;&#x3D; height:100%;</p><p>当元素没有内容时候，设置height:100%，该元素不会被撑开，此时高度为0</p><p>height:100vh，该元素会被撑开屏幕高度一致</p><p>vh就是当前屏幕可见高度的1%</p><h1 id="网页复现"><a href="#网页复现" class="headerlink" title="网页复现"></a>网页复现</h1><p><img src="/CSS.assets/image-20220322211645843.png" alt="image-20220322211645843"></p><p><img src="/CSS.assets/image-20220322211746171.png" alt="image-20220322211746171"></p><p><img src="/CSS.assets/image-20220322211914196.png" alt="image-20220322211914196"></p><p><img src="/CSS.assets/image-20220322211917448.png" alt="image-20220322211917448"></p><ul><li>让块级盒子水平居中对齐：<ul><li>有一个宽：width:298px</li><li>margin：xxpx  auto；上下有边距，左右居中</li></ul></li></ul><p><img src="/CSS.assets/image-20220322223131172.png" alt="image-20220322223131172"></p><h3 id="图片过大："><a href="#图片过大：" class="headerlink" title="图片过大："></a>图片过大：</h3><ul><li>设置宽度为100%，使得和父元素一样宽</li></ul><p><img src="/CSS.assets/image-20220322225549154.png" alt="image-20220322225549154"></p><h3 id="设置中间的段落"><a href="#设置中间的段落" class="headerlink" title="设置中间的段落"></a>设置中间的段落</h3><ul><li>高度固定</li><li>宽度没设置</li><li>字体大小</li><li>左右设置padding<ul><li>由于没有width，此处设置padding不会撑开</li></ul></li><li>上设置margintop<ul><li>由于有height，若设置padding会把height撑开，故此处设置margin</li></ul></li></ul><h3 id="预览图"><a href="#预览图" class="headerlink" title="预览图"></a>预览图</h3><p><img src="/CSS.assets/image-20220323113321517.png" alt="image-20220323113321517"></p><p><img src="/CSS.assets/image-20220323113412500.png" alt="image-20220323113412500"></p><p><img src="/CSS.assets/image-20220323114614801.png" alt="image-20220323114614801"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="/CSS.assets/image-20220323114741253.png" alt="image-20220323114741253"></p><p><img src="/CSS.assets/image-20220323114813701.png" alt="image-20220323114813701"></p><p><img src="/CSS.assets/image-20220323114949621.png" alt="image-20220323114949621"></p><h2 id="自我小结"><a href="#自我小结" class="headerlink" title="自我小结"></a>自我小结</h2><ul><li><p>布局，一个大盒子，从上到下分成几个div</p><ul><li>如果一个img，直接用img作div，无需外套div</li><li>每个div命一个class名</li></ul></li><li><p>*全局设置<code>margin:0;padding:0;</code></p></li><li><p>body{}设置整个页面</p><ul><li>背景颜色 <code>background-color</code></li></ul></li><li><p>最外层div定义高宽:<code>width;height;</code></p><ul><li>背景颜色<code>background-color</code>(与body颜色不同可以区分出两个布局)</li><li>让此div位于页面正中间<ul><li><code>margin:上下距离 auto；</code></li></ul></li></ul></li><li><p>最上层img</p><ul><li>设置<code>width：100%</code>使得和父div一样宽</li></ul></li><li><p>下方的文字</p><ul><li>还是要分成几个div，一行一个div</li></ul></li><li><p>review：</p><ul><li>字体：<code>font-size:20px;</code></li><li>字体颜色：<code>color:;</code></li><li>整个div的高度：量屏幕获得；</li><li>左右边距：<code>padding:0 30px;</code></li><li>和上面img的距离：<code>margin-top:30px;</code></li><li>如果要让文字居中:<code>text-align:center;</code></li></ul></li><li><p>appraise:</p><ul><li>颜色</li><li>字体大小 <code>font-size:15px;</code></li><li>和上面div的距离 <code>margin-top:20px;</code></li><li>左右距离 <code>padding:0 30px;</code></li></ul></li><li><p>info:</p><ul><li>分成左右两块，左边可点链接，h4，右边span，显示价格</li><li>info h4:<ul><li>h4是块级元素，&#x3D;&#x3D;<code>display:inline-block</code>&#x3D;&#x3D; 变成行内元素</li><li>取消加粗 &#x3D;&#x3D;<code>font-weight:400;</code>&#x3D;&#x3D;</li><li>颜色</li></ul></li></ul></li><li><p>所有的a<strong>不要带下划线：</strong></p><ul><li>&#x3D;&#x3D;<code>text-decoration:none;</code>&#x3D;&#x3D;</li></ul></li><li><p>图片变a：</p><ul><li>先<a>后<img></li></ul></li><li><p>em斜线</p><ul><li><strong>使得竖直</strong>：&#x3D;&#x3D;<code>font-style:normal;</code>&#x3D;&#x3D;</li><li>和左右边距：<ul><li>由于h4\em\span可以看成3个div，此处<strong>用margin间隔</strong> &#x3D;&#x3D;<code>margin:0 6px 0 15px;</code>&#x3D;&#x3D;</li></ul></li></ul></li></ul><h1 id="box-shadow"><a href="#box-shadow" class="headerlink" title="box-shadow"></a>box-shadow</h1><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-comment">/* x偏移量 | y偏移量 | 阴影模糊半径 | 阴影扩散半径 | 阴影颜色 */</span><br><span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">2px</span> <span class="hljs-number">2px</span> <span class="hljs-number">2px</span> <span class="hljs-number">1px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>);<br></code></pre></td></tr></table></figure><h1 id="快报模块"><a href="#快报模块" class="headerlink" title="快报模块"></a>快报模块</h1><p><img src="/CSS.assets/image-20220323152435385.png" alt="image-20220323152435385"></p><ul><li>排列整齐：列表ul无序列表</li><li>横线：一个只有下横线的div</li></ul><p><img src="/CSS.assets/image-20220323152723101.png" alt="image-20220323152723101"></p><p><img src="/CSS.assets/image-20220323152801197.png" alt="image-20220323152801197"></p><p><img src="/CSS.assets/image-20220323153031673.png" alt="image-20220323153031673"></p><h1 id="浮动"><a href="#浮动" class="headerlink" title="浮动"></a>浮动</h1><h2 id="170-传统网页布局的三种方式"><a href="#170-传统网页布局的三种方式" class="headerlink" title="170-传统网页布局的三种方式"></a>170-传统网页布局的三种方式</h2><ul><li><h3 id="普通"><a href="#普通" class="headerlink" title="普通"></a>普通</h3><ul><li>按照<strong>默认方式</strong>排列</li><li>块级元素独占一行：&#x3D;&#x3D;div hr p h1-h6 ul ol form&#x3D;&#x3D;</li><li>行内元素：&#x3D;&#x3D;span a i em&#x3D;&#x3D;</li><li><img src="/CSS.assets/image-20220323155258742.png" alt="image-20220323155258742"></li></ul></li><li><h3 id="浮动-171"><a href="#浮动-171" class="headerlink" title="浮动-171"></a>浮动-171</h3><ul><li>可以<strong>改变默认的排列方式</strong></li><li>让多个元素<strong>一行摆放</strong></li><li>&#x3D;&#x3D;横着布局：标准流&#x3D;&#x3D;</li><li>&#x3D;&#x3D;竖着布局：浮动&#x3D;&#x3D;  <code>float:left;</code></li><li><img src="/CSS.assets/image-20220323155733408.png" alt="image-20220323155733408"></li></ul></li><li><h4 id="什么是浮动？"><a href="#什么是浮动？" class="headerlink" title="什么是浮动？"></a>什么是浮动？</h4><p><img src="/CSS.assets/image-20220323160118315.png" alt="image-20220323160118315"></p><ul><li><img src="/CSS.assets/image-20220323160010232.png" alt="image-20220323160010232"></li><li>left？right？</li><li>找<strong>左或右边缘</strong>（body或者各种div的边缘也是）</li><li><img src="/CSS.assets/image-20220323160053786.png" alt="image-20220323160053786"></li><li><img src="/CSS.assets/image-20220323160059366.png" alt="image-20220323160059366"></li></ul></li><li><h4 id="浮动特性1：脱标"><a href="#浮动特性1：脱标" class="headerlink" title="浮动特性1：脱标"></a>浮动特性1：脱标<img src="/CSS.assets/image-20220323160821904.png" alt="image-20220323160821904"></h4><ul><li><p>&#x3D;&#x3D;脱标的盒子<strong>不再保留原先位置</strong>（其他标准流位置<strong>补上来</strong>）&#x3D;&#x3D;<img src="/CSS.assets/image-20220323161007347.png" alt="image-20220323161007347"></p></li><li><p>&#x3D;&#x3D;已经不再是标准流元素&#x3D;&#x3D;</p></li><li><p><img src="/CSS.assets/image-20220323162155827.png" alt="image-20220323162155827"></p></li><li><h4 id="如果只给粉加了float，蓝色没加，会叠加在一起"><a href="#如果只给粉加了float，蓝色没加，会叠加在一起" class="headerlink" title="如果只给粉加了float，蓝色没加，会叠加在一起"></a>如果只给粉加了float，蓝色没加，会叠加在一起<img src="/CSS.assets/image-20220323162229525.png" alt="image-20220323162229525"></h4></li></ul></li><li><h4 id="浮动特性2：多个float时一行显示"><a href="#浮动特性2：多个float时一行显示" class="headerlink" title="浮动特性2：多个float时一行显示"></a>浮动特性2：多个float时一行显示</h4><ul><li>一行了</li><li>上沿（顶端）对齐</li></ul></li></ul><p><img src="/CSS.assets/image-20220323162549048.png" alt="image-20220323162549048"></p><p><img src="/CSS.assets/image-20220323162600899.png" alt="image-20220323162600899"></p><ul><li>对div加上float：按行排列</li><li><img src="/CSS.assets/image-20220323162639486.png" alt="image-20220323162639486"></li><li>缩小框时会换行</li></ul><p><img src="/CSS.assets/image-20220323162723398.png" alt="image-20220323162723398"></p><ul><li><img src="/CSS.assets/image-20220324102010863.png" alt="image-20220324102010863"></li></ul><p><strong>会换行</strong></p><hr><h3 id="特性3：具有行内块元素特性"><a href="#特性3：具有行内块元素特性" class="headerlink" title="特性3：具有行内块元素特性"></a>特性3：具有行内块元素特性</h3><p><img src="/CSS.assets/image-20220324102251578.png" alt="image-20220324102251578"></p><p> <img src="/CSS.assets/image-20220324102334935.png" alt="image-20220324102334935"></p><p><img src="/CSS.assets/image-20220324102437872.png" alt="image-20220324102437872"></p><p><img src="/CSS.assets/image-20220324102449369.png" alt="image-20220324102449369"></p><h2 id="浮动与标准流搭配使用"><a href="#浮动与标准流搭配使用" class="headerlink" title="浮动与标准流搭配使用"></a>浮动与标准流搭配使用</h2><p><img src="/CSS.assets/image-20220324104652862.png" alt="image-20220324104652862"></p><p><img src="/CSS.assets/image-20220324104711066.png" alt="image-20220324104711066"></p><p>不要直接给多个div添加float，先准备一个大盒子，内部再float（否则会以浏览器为基准浮动）</p><p><img src="/CSS.assets/image-20220324104823685.png" alt="image-20220324104823685"></p><p>父盒子高宽、居中</p><p>浮动盒子左右一个</p><h2 id="小米布局案例"><a href="#小米布局案例" class="headerlink" title="小米布局案例"></a>小米布局案例</h2><p><img src="/CSS.assets/image-20220324105104460.png" alt="image-20220324105104460"></p><p>中间没有缝隙-&amp;在一行–》float</p><p><img src="/CSS.assets/image-20220324105542348.png" alt="image-20220324105542348"></p><p><strong>&#x3D;&#x3D;使在一行显示：&#x3D;&#x3D;</strong></p><ul><li>&#x3D;&#x3D;通过宽1+宽2&#x3D;总宽&#x3D;&#x3D;</li><li>&#x3D;&#x3D;最外层标准居中&#x3D;&#x3D;</li><li>&#x3D;&#x3D;内层都浮动&#x3D;&#x3D;</li></ul><p><img src="/CSS.assets/image-20220324105644617.png" alt="image-20220324105644617"></p><p>&#x3D;&#x3D;margin控制盒子与盒子之间的距离&#x3D;&#x3D;</p><p><img src="/CSS.assets/image-20220324110009628.png" alt="image-20220324110009628"></p><p>最后一个盒子单独写外边距</p><p><img src="/CSS.assets/image-20220324110106078.png" alt="image-20220324110106078"></p><p>权重！下面的权重低，没有起效果</p><p><img src="/CSS.assets/image-20220324110121654.png" alt="image-20220324110121654"></p><p><img src="/CSS.assets/image-20220324110357075.png" alt="image-20220324110357075"></p><p><img src="/CSS.assets/image-20220324111031845.png" alt="image-20220324111031845"></p><p><img src="/CSS.assets/image-20220324111042850.png" alt="image-20220324111042850"></p><p><img src="/CSS.assets/image-20220324111054911.png" alt="image-20220324111054911"></p><p><img src="/CSS.assets/image-20220324111720469.png" alt="image-20220324111720469"></p><h2 id="手机布局"><a href="#手机布局" class="headerlink" title="手机布局"></a>手机布局</h2><p><img src="/CSS.assets/image-20220324111808061.png" alt="image-20220324111808061"></p><p><img src="/CSS.assets/image-20220324114033893.png" alt="image-20220324114033893"></p><ol><li>&#x3D;&#x3D;小<strong>间距</strong>：margin-left（边框不变，往右移动）【盒子之间用margin】&#x3D;&#x3D;</li><li>用此方法，量大盒子时使用的width应该是包含边距的</li></ol><h1 id="网页布局准则"><a href="#网页布局准则" class="headerlink" title="网页布局准则"></a>网页布局准则</h1><ol><li>标准流上下，浮动左右排</li><li>先设置盒子大小，再设置位置</li></ol><h1 id="常见网页布局"><a href="#常见网页布局" class="headerlink" title="常见网页布局"></a>常见网页布局</h1><p><img src="/CSS.assets/image-20220324114503654.png" alt="image-20220324114503654"></p><p><img src="/CSS.assets/image-20220324115416593.png" alt="image-20220324115416593"></p><p><strong>【通栏】：</strong>&#x3D;&#x3D;块级元素不设置宽度时会默认和浏览器一样宽&#x3D;&#x3D;</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;!DOCTYPE <span class="hljs-selector-tag">html</span>&gt;<br>&lt;<span class="hljs-selector-tag">html</span> lang=&quot;en&quot;&gt;<br>&lt;head&gt;<br>  &lt;meta charset=&quot;UTF-<span class="hljs-number">8</span>&quot;&gt;<br>  &lt;meta http-equiv=&quot;X-UA-Compatible&quot; <span class="hljs-attribute">content</span>=&quot;IE=edge&quot;&gt;<br>  &lt;meta name=&quot;viewport&quot; <span class="hljs-attribute">content</span>=&quot;<span class="hljs-attribute">width</span>=device-<span class="hljs-attribute">width</span>, initial-scale=<span class="hljs-number">1.0</span>&quot;&gt;<br>  &lt;title&gt;Document&lt;/title&gt;<br>  &lt;style&gt;<br>    *&#123;<br>      <span class="hljs-attribute">margin</span>: <span class="hljs-number">0</span>;<br>      <span class="hljs-attribute">padding</span>: <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-selector-class">.top</span>&#123;<br>      <span class="hljs-comment">/* 块级元素默认和浏览器一样宽，不用设置宽度 */</span><br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">50px</span>;<br>      <span class="hljs-attribute">background-color</span>: gray;<br>    &#125;<br>    <span class="hljs-selector-class">.banner</span>&#123;<br>      <span class="hljs-attribute">width</span>: <span class="hljs-number">980px</span>;<br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">150px</span>;<br>      <span class="hljs-comment">/* margin: 0 auto; */</span><br>      <span class="hljs-attribute">background-color</span>:grey;<br>      <span class="hljs-attribute">margin</span>: <span class="hljs-number">10px</span> auto;<br>    &#125;<br>    <span class="hljs-selector-tag">li</span>&#123;<br>      <span class="hljs-attribute">list-style</span>: none;<br>    &#125;<br>    <span class="hljs-selector-class">.box</span>&#123;<br>      <span class="hljs-attribute">width</span>: <span class="hljs-number">980px</span>;<br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">300px</span>;<br>      <span class="hljs-attribute">margin</span>: <span class="hljs-number">0</span> auto;<br>      <span class="hljs-attribute">background-color</span>: palegoldenrod;<br>    &#125;<br>    <span class="hljs-selector-class">.box</span> <span class="hljs-selector-tag">li</span>&#123;<br>      <span class="hljs-attribute">float</span>: left;<br>      <span class="hljs-attribute">width</span>: <span class="hljs-number">237px</span>;<br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">300px</span>;<br>      <span class="hljs-attribute">background-color</span>: gray;<br>      <span class="hljs-attribute">margin-right</span>: <span class="hljs-number">10px</span>;<br>    &#125;<br>    <span class="hljs-selector-class">.box</span> <span class="hljs-selector-tag">li</span><span class="hljs-selector-pseudo">:nth-child</span>(<span class="hljs-number">4</span>)&#123;<br>      <span class="hljs-comment">/* 最后一个没有右边距 */</span><br>      <span class="hljs-attribute">margin-right</span>: <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-selector-class">.footer</span>&#123;<br>      <span class="hljs-attribute">height</span>: <span class="hljs-number">200px</span>;<br>      <span class="hljs-attribute">background-color</span>: gray;<br>      <span class="hljs-attribute">margin-top</span>: <span class="hljs-number">10px</span>;<br>    &#125;<br>  &lt;/style&gt;<br>&lt;/head&gt;<br>&lt;<span class="hljs-selector-tag">body</span>&gt;<br>  &lt;<span class="hljs-selector-tag">div</span> class=&quot;<span class="hljs-attribute">top</span>&quot;&gt;&lt;/<span class="hljs-selector-tag">div</span>&gt;<br>  &lt;<span class="hljs-selector-tag">div</span> class=&quot;banner&quot;&gt;&lt;/<span class="hljs-selector-tag">div</span>&gt;<br>  &lt;<span class="hljs-selector-tag">div</span> class=&quot;box&quot;&gt;<br>    &lt;<span class="hljs-selector-tag">ul</span>&gt;<br>      &lt;<span class="hljs-selector-tag">li</span>&gt;<span class="hljs-number">1</span>&lt;/<span class="hljs-selector-tag">li</span>&gt;<br>      &lt;<span class="hljs-selector-tag">li</span>&gt;<span class="hljs-number">2</span>&lt;/<span class="hljs-selector-tag">li</span>&gt;<br>      &lt;<span class="hljs-selector-tag">li</span>&gt;<span class="hljs-number">3</span>&lt;/<span class="hljs-selector-tag">li</span>&gt;<br>      &lt;<span class="hljs-selector-tag">li</span>&gt;<span class="hljs-number">4</span>&lt;/<span class="hljs-selector-tag">li</span>&gt;<br>    &lt;/<span class="hljs-selector-tag">ul</span>&gt;<br>  &lt;/<span class="hljs-selector-tag">div</span>&gt;<br>  &lt;<span class="hljs-selector-tag">div</span> class=&quot;<span class="hljs-selector-tag">footer</span>&quot;&gt;&lt;/<span class="hljs-selector-tag">div</span>&gt;<br>&lt;/<span class="hljs-selector-tag">body</span>&gt;<br>&lt;/<span class="hljs-selector-tag">html</span>&gt;<br></code></pre></td></tr></table></figure><h1 id="浮动布局注意点"><a href="#浮动布局注意点" class="headerlink" title="浮动布局注意点"></a>浮动布局注意点</h1><p><img src="/CSS.assets/image-20220324120115339.png" alt="image-20220324120115339"></p><ol><li>父标准，子浮动</li><li>一浮多浮</li></ol><h1 id="清除浮动"><a href="#清除浮动" class="headerlink" title="清除浮动"></a>清除浮动</h1><h2 id="父盒子高度必须写吗？"><a href="#父盒子高度必须写吗？" class="headerlink" title="父盒子高度必须写吗？"></a>父盒子高度必须写吗？</h2><p>让子盒子撑开父盒子</p><p>不要给父盒子写高</p><p><img src="/CSS.assets/image-20220324120731317.png" alt="image-20220324120731317"></p><blockquote><p>给子元素加了浮动，不再占有位置，父元素宽度没了</p></blockquote><h2 id="清除浮动-1"><a href="#清除浮动-1" class="headerlink" title="清除浮动"></a>清除浮动</h2><p><img src="/CSS.assets/image-20220324121104775.png" alt="image-20220324121104775"></p><p><img src="/CSS.assets/image-20220324121150577.png" alt="image-20220324121150577"></p><p>&#x3D;&#x3D;【之前大框内几个小框，只是因为设置了宽度一样，效果上是撑开的，实际上父和子不在一个维度】&#x3D;&#x3D;</p><h3 id="如何清除"><a href="#如何清除" class="headerlink" title="如何清除"></a>如何清除</h3><ol><li>清楚浮动元素造成的影响</li></ol><p><img src="/CSS.assets/image-20220324121342269.png" alt="image-20220324121342269"></p><p><img src="/CSS.assets/image-20220324121404758.png" alt="image-20220324121404758"></p><h3 id="清除浮动方法"><a href="#清除浮动方法" class="headerlink" title="清除浮动方法"></a>清除浮动方法</h3><p><img src="/CSS.assets/image-20220324121439790.png" alt="image-20220324121439790"></p><p><img src="/CSS.assets/image-20220324202946577.png" alt="image-20220324202946577"></p><h2 id="1-额外标签法"><a href="#1-额外标签法" class="headerlink" title="1 额外标签法"></a>1 额外标签法</h2><p><img src="/CSS.assets/image-20220325102847351.png" alt="image-20220325102847351"></p><p><strong>新添加的元素必须是块级元素</strong></p><p><img src="/CSS.assets/image-20220325102906742.png" alt="image-20220325102906742">会使得box撑满l和r，同时box2不受浮动影响，被挤到该在的位置</p><p><img src="/CSS.assets/image-20220324203702795.png" alt="image-20220324203702795"></p><h2 id="2-父级添加overflow"><a href="#2-父级添加overflow" class="headerlink" title="2 父级添加overflow"></a>2 父级添加overflow</h2><p><img src="/CSS.assets/image-20220325103351826.png" alt="image-20220325103351826"></p><p><img src="/CSS.assets/image-20220325103501279.png" alt="image-20220325103501279"></p><p>【无法显示溢出的部分】</p><h2 id="3-：after伪元素法（给父元素）"><a href="#3-：after伪元素法（给父元素）" class="headerlink" title="3 ：after伪元素法（给父元素）"></a>3 ：after伪元素法（给父元素）</h2><p><img src="/CSS.assets/image-20220325103541043.png" alt="image-20220325103541043"></p><ol><li>把这两段复制在style中放好</li><li>给父盒子加一个clearfix类</li></ol><p><img src="/CSS.assets/image-20220325104117729.png" alt="image-20220325104117729"></p><h2 id="4-双伪元素"><a href="#4-双伪元素" class="headerlink" title="4 双伪元素"></a>4 双伪元素</h2><p><img src="/CSS.assets/image-20220325104410337.png" alt="image-20220325104410337"></p><h2 id="什么时候清除"><a href="#什么时候清除" class="headerlink" title="什么时候清除"></a>什么时候清除</h2><p><img src="/CSS.assets/image-20220325105219165.png" alt="image-20220325105219165"></p><ol><li>父级盒子没有高度时，会被浮动子元素影响高度</li><li>影响到其他布局了</li></ol><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><p><img src="/CSS.assets/image-20220325105256182.png" alt="image-20220325105256182"></p><p>感觉&#x3D;&#x3D;<strong>给父级添加overflow：hidden最简单</strong>&#x3D;&#x3D;</p><h1 id="195-学成在线案例"><a href="#195-学成在线案例" class="headerlink" title="195 学成在线案例"></a>195 学成在线案例</h1><h2 id="css书写顺序"><a href="#css书写顺序" class="headerlink" title="css书写顺序"></a>css书写顺序</h2><p><img src="/CSS.assets/image-20220325105940464.png" alt="image-20220325105940464"></p><h2 id="整体布局思路"><a href="#整体布局思路" class="headerlink" title="整体布局思路"></a>整体布局思路</h2><p><img src="/CSS.assets/image-20220325110313300.png" alt="image-20220325110313300"></p><h2 id="确定版心"><a href="#确定版心" class="headerlink" title="确定版心"></a>确定版心</h2><p><img src="/CSS.assets/image-20220325111204204.png" alt="image-20220325111204204"></p><h2 id="头部"><a href="#头部" class="headerlink" title="头部"></a>头部</h2><p><img src="/CSS.assets/image-20220325111417082.png" alt="image-20220325111417082"></p><h3 id="padding法"><a href="#padding法" class="headerlink" title="padding法"></a>padding法</h3><p><img src="/CSS.assets/image-20220325111459783.png" alt="image-20220325111459783"></p><p>测量最高高度</p><h3 id="margin法"><a href="#margin法" class="headerlink" title="margin法"></a>margin法</h3><p><img src="/CSS.assets/image-20220325111451799.png" alt="image-20220325111451799"></p><p>测量文字高度</p><p><img src="/CSS.assets/image-20220325111955655.png" alt="image-20220325111955655"></p><h2 id="导航栏"><a href="#导航栏" class="headerlink" title="导航栏"></a>导航栏</h2><p><img src="/CSS.assets/image-20220325113713931.png" alt="image-20220325113713931"></p><h3 id="logo"><a href="#logo" class="headerlink" title="logo"></a>logo</h3><p>直接设置img，设置高度、宽度</p><h3 id="导航栏-1"><a href="#导航栏-1" class="headerlink" title="导航栏"></a>导航栏</h3><p><img src="/CSS.assets/image-20220325114138281.png" alt="image-20220325114138281"></p><ol><li>在一行：<strong>每个li都加浮动</strong></li><li><strong>ul li a</strong>写导航栏</li><li>li<strong>去掉原点</strong></li><li>a<strong>去掉下划线</strong></li><li>文字居中：<strong>line-height</strong></li><li>设置高度<strong>height</strong>，不设置宽度（可能还有其他内容）</li><li>左右设置<strong>padding</strong>实现间隔</li></ol><p><img src="/CSS.assets/image-20220325114354046.png" alt="image-20220325114354046"></p><p><img src="/CSS.assets/image-20220325114824915.png" alt="image-20220325114824915"></p><h3 id="x3D-x3D-使得文字居中-x3D-x3D"><a href="#x3D-x3D-使得文字居中-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;使得文字居中&#x3D;&#x3D;"></a>&#x3D;&#x3D;使得文字居中&#x3D;&#x3D;</h3><p><img src="/CSS.assets/image-20220325114834268.png" alt="image-20220325114834268"></p><p>直接设置line-height,而不是height</p><p><img src="/CSS.assets/image-20220325115008405.png" alt="image-20220325115008405"></p><h2 id="表单"><a href="#表单" class="headerlink" title="表单"></a>表单</h2><p><img src="/CSS.assets/image-20220325152737754.png" alt="image-20220325152737754"></p><ul><li>一个表单包含左边输入框和右边搜索</li></ul><p><img src="/CSS.assets/image-20220325152851559.png" alt="image-20220325152851559"></p><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p><img src="/CSS.assets/image-20220408113713722.png" alt="image-20220408113713722"></p><p> <img src="/CSS.assets/image-20220408113723086.png" alt="image-20220408113723086"></p><p><img src="/CSS.assets/image-20220408113730790.png" alt="image-20220408113730790"></p><p><strong>固定在屏幕中的某个位置</strong></p><p><img src="/CSS.assets/image-20220408113820621.png" alt="image-20220408113820621"></p><p>&#x3D;&#x3D;定位&#x3D;定位模式+边偏移&#x3D;&#x3D;</p><p><img src="/CSS.assets/image-20220408114010937.png" alt="image-20220408114010937"></p><p><strong>移动位置</strong>：</p><p><img src="/CSS.assets/image-20220408114049335.png" alt="image-20220408114049335"></p><ul><li><p>静态定位：标准流</p></li><li><h3 id="相对定位relative：-x3D-x3D-根据原来的位置-x3D-x3D"><a href="#相对定位relative：-x3D-x3D-根据原来的位置-x3D-x3D" class="headerlink" title="相对定位relative：&#x3D;&#x3D;根据原来的位置&#x3D;&#x3D;"></a>相对定位relative：&#x3D;&#x3D;<strong>根据原来的位置</strong>&#x3D;&#x3D;</h3></li></ul><p><img src="/CSS.assets/image-20220408114309509.png" alt="image-20220408114309509"></p><ul><li>不会影响其他盒子</li></ul><p><img src="/CSS.assets/image-20220408114627020.png" alt="image-20220408114627020"></p><p>（不脱标）</p><ul><li><h3 id="绝对定位-absolute：-x3D-x3D-根据父元素-x3D-x3D"><a href="#绝对定位-absolute：-x3D-x3D-根据父元素-x3D-x3D" class="headerlink" title="绝对定位 absolute：&#x3D;&#x3D;根据父元素&#x3D;&#x3D;"></a>绝对定位 absolute：&#x3D;&#x3D;根据父元素&#x3D;&#x3D;</h3></li></ul><p><strong>&#x3D;&#x3D;没有父元素：&#x3D;&#x3D;</strong></p><ul><li>以浏览器为标准定位</li></ul><p>&#x3D;&#x3D;<strong>有父元素，但父元素没定位：</strong>&#x3D;&#x3D;</p><ul><li>以浏览器为标准定位</li></ul><p><strong>&#x3D;&#x3D;有父元素，父元素有定位：&#x3D;&#x3D;</strong></p><ul><li>以最近的有定位的父元素为标准</li></ul><p>​&#x3D;&#x3D;<em><strong>绝对定位会脱标</strong></em>&#x3D;&#x3D;</p><p><img src="/CSS.assets/image-20220408121017846.png" alt="image-20220408121017846"></p><p><img src="/CSS.assets/image-20220408121303285.png" alt="image-20220408121303285"></p><p>一个图片上面的两个绝对定位：不会影响背景图的展示（脱标了）</p><h2 id="子绝父相"><a href="#子绝父相" class="headerlink" title="子绝父相"></a>子绝父相</h2><p><img src="/CSS.assets/image-20220408121829253.png" alt="image-20220408121829253"></p><p><strong>子元素绝对，不占位置，脱标，就不会影响其他兄弟盒子</strong></p><p>父盒子相对定位：</p><ul><li>绝对定位会导致父盒子也脱标，不能保留原来的位置，但是父盒子一定要保留原来的位置！</li></ul><p><img src="/CSS.assets/image-20220408122032338.png" alt="image-20220408122032338"></p><h2 id="固定定位fiexd"><a href="#固定定位fiexd" class="headerlink" title="固定定位fiexd"></a>固定定位fiexd</h2><p><img src="/CSS.assets/image-20220408122125977.png" alt="image-20220408122125977"></p><p><img src="/CSS.assets/image-20220408122135419.png" alt="image-20220408122135419"></p><p><img src="/CSS.assets/image-20220408122838245.png" alt="image-20220408122838245"></p><ul><li>固定定位不占有原先的位置</li><li>也是脱标的，一种特殊的绝对定位</li></ul><p><img src="/CSS.assets/image-20220408123100980.png" alt="image-20220408123100980"></p><h4 id="固定定位跟着版心对齐"><a href="#固定定位跟着版心对齐" class="headerlink" title="固定定位跟着版心对齐"></a>固定定位跟着版心对齐</h4><p><img src="/CSS.assets/image-20220408123237940.png" alt="image-20220408123237940"></p><ul><li>**&#x3D;&#x3D;left&#x3D;&#x3D;**走50%（如果父盒子在正中间）</li><li>**&#x3D;&#x3D;margin-left：&#x3D;&#x3D;**再多走版心宽度的一半</li></ul><h2 id="绝对定位盒子使得水平垂直居中"><a href="#绝对定位盒子使得水平垂直居中" class="headerlink" title="绝对定位盒子使得水平垂直居中"></a>绝对定位盒子使得水平垂直居中</h2><p><img src="/CSS.assets/image-20220408123757684.png" alt="image-20220408123757684"></p><ul><li>**&#x3D;&#x3D;left&#x3D;&#x3D;**走50%</li><li><strong>&#x3D;&#x3D;marigin-left&#x3D;负值&#x3D;&#x3D;</strong></li></ul><h1 id="媒体查询"><a href="#媒体查询" class="headerlink" title="媒体查询"></a>媒体查询</h1><blockquote><p>为不同尺寸的屏幕设定不同的CSS样式</p></blockquote><p><img src="/CSS.assets/image-20220408095319203.png" alt="image-20220408095319203"></p><p><img src="/CSS.assets/image-20220408095413348.png" alt="image-20220327203747892"></p><p><img src="/CSS.assets/image-20220327204154120.png" alt="image-20220327204154120"></p><h2 id="背景颜色变色"><a href="#背景颜色变色" class="headerlink" title="背景颜色变色"></a>背景颜色变色</h2><p><img src="/CSS.assets/image-20220408101228577.png" alt="image-20220408101228577"></p><ul><li>绿色高度变高</li><li>文字字体变大</li></ul><h2 id="media常用参数"><a href="#media常用参数" class="headerlink" title="@media常用参数"></a>@media常用参数</h2><p><img src="/CSS.assets/image-20220328111520155.png" alt="image-20220328111520155"></p><p>device没有：浏览器</p><p>有device：手机设备</p><h2 id="media引用方式1"><a href="#media引用方式1" class="headerlink" title="@media引用方式1"></a>@media引用方式1</h2><p><img src="/CSS.assets/image-20220329195005424.png" alt="image-20220329195005424"></p><h2 id="media引用方式2"><a href="#media引用方式2" class="headerlink" title="@media引用方式2"></a>@media引用方式2</h2><p><img src="/CSS.assets/image-20220329195035018.png" alt="image-20220329195035018"></p><h3 id="缩略图示例"><a href="#缩略图示例" class="headerlink" title="缩略图示例"></a>缩略图示例</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-comment">/* 媒体查询 */</span><br><span class="hljs-keyword">@media</span>(max-width:600px)&#123;<br>  <span class="hljs-comment">/* 宽度&lt;600时 */</span><br>  <span class="hljs-selector-class">.sub-imgs</span>&#123;<br>    <span class="hljs-comment">/* 子图容器设置为2列 */</span><br>    <span class="hljs-attribute">grid-template-columns</span>: <span class="hljs-number">1</span>fr <span class="hljs-number">1</span>fr;<br>  &#125;<br>&#125; <br></code></pre></td></tr></table></figure><p><img src="/CSS.assets/image-20220331103341806.png" alt="image-20220331103341806"></p><ul><li>其他时候都是4个一行</li></ul><h1 id="flex"><a href="#flex" class="headerlink" title="flex"></a>flex</h1><img src="CSS.assets/image-20220329195332550.png" alt="image-20220329195332550" style="zoom:100%;" /><h2 id="子元素伸缩比"><a href="#子元素伸缩比" class="headerlink" title="子元素伸缩比"></a>子元素伸缩比</h2><p><img src="/CSS.assets/image-20220329201106877.png" alt="image-20220329201106877"></p><h3 id="basis："><a href="#basis：" class="headerlink" title="basis："></a>basis：</h3><p>宽度 覆盖width</p><h3 id="grow："><a href="#grow：" class="headerlink" title="grow："></a>grow：</h3><p>子对象没填满父对象时（有剩余空间时）</p><p>每个子对象按grow比例扩充大小</p><p><img src="/CSS.assets/image-20220329201742387.png" alt="image-20220329201742387"></p><h3 id="shrink"><a href="#shrink" class="headerlink" title="shrink"></a>shrink</h3><p>不允许缩小</p><h3 id="flex-1"><a href="#flex-1" class="headerlink" title="flex"></a>flex</h3><p>扩大 缩小 宽</p><h2 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h2><p><img src="/CSS.assets/image-20220329202346172.png" alt="image-20220329202346172"></p><h1 id="rem单位"><a href="#rem单位" class="headerlink" title="rem单位"></a>rem单位</h1><ul><li>文字大小随屏幕大小变化而变化</li><li>高度不要定死，随屏幕变化等比例缩放</li></ul><p><img src="/CSS.assets/image-20220329203126026.png" alt="image-20220329203126026"></p><p><img src="/CSS.assets/image-20220329204321057.png" alt="image-20220329204321057"></p><ul><li>单位：<ul><li>相对单位 类似em（父元素字体大小）</li><li>em：<img src="/CSS.assets/image-20220408094435210.png" alt="image-20220408094435210"></li><li><img src="/CSS.assets/image-20220408094545844.png" alt="image-20220408094545844"></li></ul></li></ul><h3 id="em"><a href="#em" class="headerlink" title="em"></a>em</h3><p>与父一级字体 </p><ul><li>父元素是10px，<strong>子元素10em&#x3D;10*10&#x3D;100px</strong></li></ul><h3 id="rem"><a href="#rem" class="headerlink" title="rem"></a>rem</h3><p>只用改一个html的文字大小，随着屏幕只用改变一个值，其他值也会跟着变</p><p><strong>优势：</strong></p><ul><li>可以通过<strong>修改html里的文字大小</strong>来改变<strong>页面中元素大小</strong></li></ul><h1 id="响应式布局"><a href="#响应式布局" class="headerlink" title="响应式布局"></a>响应式布局</h1><p><img src="/CSS.assets/image-20220329210536961.png" alt="image-20220329210536961"></p><p><img src="/CSS.assets/image-20220329210552057.png" alt="image-20220329210552057"></p><p><img src="/CSS.assets/image-20220408110803082.png" alt="image-20220408110803082"></p><p><strong>通过media查询改变布局容器的大小，再改变子元素的排列方式和大小</strong></p><h2 id="响应式导航"><a href="#响应式导航" class="headerlink" title="响应式导航"></a>响应式导航</h2><p><img src="/CSS.assets/image-20220408111725777.png" alt="image-20220408111725777"></p><ul><li>大屏幕时：一排li</li><li>小屏幕时：宽度变化100%，li一排只有几个，33.33%</li></ul><h1 id="CSS菜鸟教程"><a href="#CSS菜鸟教程" class="headerlink" title="CSS菜鸟教程"></a>CSS菜鸟教程</h1><h1 id="盒子模型"><a href="#盒子模型" class="headerlink" title="盒子模型"></a>盒子模型</h1><p>完整大小的元素，你还必须添加<strong>内边距</strong>，<strong>边框和外边距</strong>。</p><p><img src="/CSS.assets/image-20220324210307503.png" alt="image-20220324210307503"></p><ul><li>content+padding左右+border&#x3D;内容</li></ul><h2 id="网格布局"><a href="#网格布局" class="headerlink" title="网格布局"></a>网格布局</h2><ul><li>容器添加 <strong>display：grid</strong></li><li>容器设置摆放： <strong>grid-template-columns: 1fr 1fr;  &#x2F;&#x2F;2列</strong><ul><li>​ <strong>grid-template-columns: 1fr 1fr 1fr 1fr;&#x2F;&#x2F;4列</strong></li></ul></li></ul><p><img src="/CSS.assets/image-20220331103505142.png" alt="image-20220331103505142"></p><h2 id="【几种改变】"><a href="#【几种改变】" class="headerlink" title="【几种改变】"></a>【几种改变】</h2><h3 id="改变类："><a href="#改变类：" class="headerlink" title="改变类："></a>改变类：</h3><p>js:</p><ul><li>addClass</li><li>ClassList</li><li>ClassName</li></ul><p>jquery:</p><p>- </p><h3 id="改变属性："><a href="#改变属性：" class="headerlink" title="改变属性："></a>改变属性：</h3><p>js:</p><ul><li>xxx**.src**&#x3D;’’</li><li>xxx.checked&#x3D;’’</li><li>document.getElementById(‘img’)<strong>.src</strong>&#x3D;’..&#x2F;图片切换&#x2F;images&#x2F;2.jpg’</li></ul><p>jquery:</p><ul><li>$(‘#img’)<strong>.attr</strong>(‘src’,’xxxx’)</li></ul><h3 id="改变css："><a href="#改变css：" class="headerlink" title="改变css："></a>改变css：</h3><p>js:</p><ul><li>document.getElementById(“p2”)**.style.**color&#x3D;”blue”;</li></ul><h3 id="绑定事件："><a href="#绑定事件：" class="headerlink" title="绑定事件："></a>绑定事件：</h3><p>js：</p><ul><li>document.getElementById(“myBtn”)<strong>.onclick&#x3D;function()</strong>{<strong>displayDate()</strong>};</li><li>&lt;button **onclick**&#x3D;”**displayDate()**”&gt;</li></ul><p>jquery：</p><ul><li>$(‘#id’).click&#x3D;function(){ }</li><li>$(‘#id’).focus&#x3D;function(){}</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>前端</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JavaScript</title>
    <link href="/2022/04/22/JavaScript/"/>
    <url>/2022/04/22/JavaScript/</url>
    
    <content type="html"><![CDATA[<h1 id="JS"><a href="#JS" class="headerlink" title="JS"></a>JS</h1><h1 id="ajax"><a href="#ajax" class="headerlink" title="ajax"></a>ajax</h1><p><img src="/JavaScript.assets/image-20220303211516427.png" alt="image-20220303211516427"></p><ol><li>创建ajax对象</li><li>配置请求信息<ol><li>请求方式、请求地址、是否异步</li></ol></li><li>注册请求完成事件</li><li>发送请求</li></ol><p><img src="/JavaScript.assets/image-20220303212540356.png" alt="image-20220303212540356"></p><p><img src="/JavaScript.assets/image-20220306095720248.png" alt="image-20220306095720248"></p><p><img src="/JavaScript.assets/image-20220306095703923.png" alt="image-20220306095703923"></p><p><img src="/JavaScript.assets/image-20220403101156168.png" alt="image-20220403101156168"></p><p>**&#x3D;&#x3D;json字符串—》json格式数据&#x3D;&#x3D;**：JSON.parse(xxx)</p><h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><p><img src="/JavaScript.assets/image-20220306095842062.png" alt="image-20220306095842062"></p><p><img src="/JavaScript.assets/image-20220306100120621.png" alt="image-20220306100120621"></p><ul><li>携带参数时：直接写在地址后   &#x3D;&#x3D;<strong>?xxx&#x3D;xxx&amp;xxx2&#x3D;xxx2</strong>&#x3D;&#x3D;</li></ul><h3 id="post"><a href="#post" class="headerlink" title="post"></a>post</h3><p><img src="/JavaScript.assets/image-20220306095901958.png" alt="image-20220306095901958"></p><p><img src="/JavaScript.assets/image-20220306100355868.png" alt="image-20220306100355868"></p><h1 id="jquery-ajax"><a href="#jquery-ajax" class="headerlink" title="jquery ajax"></a>jquery ajax</h1><p><img src="/JavaScript.assets/image-20220318150107518.png" alt="image-20220318150107518"></p><p><img src="/JavaScript.assets/image-20220318150144365.png" alt="image-20220318150144365"></p><p><img src="/JavaScript.assets/image-20220318150502539.png" alt="image-20220318150502539"></p><p><img src="/JavaScript.assets/image-20220318150815935.png" alt="image-20220318150815935"></p><h2 id="登录注册案例"><a href="#登录注册案例" class="headerlink" title="登录注册案例"></a>登录注册案例</h2><p> <img src="/JavaScript.assets/image-20220306111526609.png" alt="image-20220306111526609"></p><p><img src="/JavaScript.assets/image-20220306111637980.png" alt="image-20220306111637980"></p><p><img src="/JavaScript.assets/image-20220306111725582.png" alt="image-20220306111725582"></p><p><img src="/JavaScript.assets/image-20220306111851621.png" alt="image-20220306111851621"></p><p><img src="/JavaScript.assets/image-20220306111913379.png" alt="image-20220306111913379"></p><h1 id="ajax-1"><a href="#ajax-1" class="headerlink" title="$.ajax()"></a>$.ajax()</h1><p><img src="/JavaScript.assets/image-20220318152009104.png" alt="image-20220318152009104"></p><p><img src="/JavaScript.assets/image-20220318152142253.png" alt="image-20220318152142253"></p><p><img src="/JavaScript.assets/image-20220318152227155.png" alt="image-20220318152227155"></p><p><img src="/JavaScript.assets/image-20220318152257598.png" alt="image-20220318152257598"></p><ul><li>$.ajax({ type&#x2F;<strong>url&#x2F;dataType&#x2F;success</strong> })<ul><li>success中k而已使接收到的数据显示在页面上</li><li>&#x3D;&#x3D;创建一个dom：var ul&#x3D;$(“<xxx></xxx>“)&#x3D;&#x3D;</li><li>遍历接收到的数组元素，for(var i&#x3D;0;i&lt;data.length;i++)</li><li>获取每一个元素 var user&#x3D;data[i]</li><li>&#x3D;&#x3D;创建li元素 var li&#x3D;”<li>“+user.userName+”</li>“&#x3D;&#x3D;</li><li>将li元素添加到ul中： <strong>ul.append(li)</strong><ul><li>​<strong>$(“body”).append(ul)</strong></li></ul></li></ul></li></ul><h1 id="get-1"><a href="#get-1" class="headerlink" title="$.get()"></a>$.get()</h1><p><img src="/JavaScript.assets/image-20220318153023408.png" alt="image-20220318153023408"></p><h1 id="post-1"><a href="#post-1" class="headerlink" title="$.post()"></a>$.post()</h1><h1 id="getJson"><a href="#getJson" class="headerlink" title="$.getJson()"></a>$.getJson()</h1><p>只识别json格式的数据</p><p><img src="/JavaScript.assets/image-20220318153207882.png" alt="image-20220318153207882"></p><h1 id="Ajax"><a href="#Ajax" class="headerlink" title="Ajax"></a>Ajax</h1><p><img src="/JavaScript.assets/image-20220306110050949.png" alt="image-20220306110050949"></p><h1 id="JavaScript-表单验证"><a href="#JavaScript-表单验证" class="headerlink" title="JavaScript 表单验证"></a>JavaScript 表单验证</h1><p>在数据被送往服务器前对 HTML 表单中的这些输入数据<strong>进行验证</strong></p><ul><li>是否为空？</li><li>正确的email地址？</li><li>日期是否输入正确？</li><li>输入内容是否为数字型？</li></ul><h3 id="检查表单是否填写"><a href="#检查表单是否填写" class="headerlink" title="检查表单是否填写"></a>检查表单是否填写</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">validateForm</span>(<span class="hljs-params"></span>)<br>&#123;<br>  <span class="hljs-keyword">var</span> x=<span class="hljs-variable language_">document</span>.<span class="hljs-property">forms</span>[<span class="hljs-string">&quot;myForm&quot;</span>][<span class="hljs-string">&quot;fname&quot;</span>].<span class="hljs-property">value</span>;<br>  <span class="hljs-keyword">if</span> (x==<span class="hljs-literal">null</span> || x==<span class="hljs-string">&quot;&quot;</span>)<br>  &#123;<br>    <span class="hljs-title function_">alert</span>(<span class="hljs-string">&quot;姓必须填写&quot;</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ol><li>获取输入框值<ol><li><strong>document.forms[ ][ ].value</strong></li><li>document.getElementById(“id”).value;</li></ol></li><li>if-else判断是否合条件<ol><li>为空：<strong>if (x &#x3D;&#x3D; null || x &#x3D;&#x3D; “”)</strong></li></ol></li></ol><h3 id="E-mail-验证"><a href="#E-mail-验证" class="headerlink" title="E-mail 验证"></a>E-mail 验证</h3><p>检查输入的数据是否符合电子邮件地址的基本语法</p><ul><li>必须包含 @ 符号和点号(.)</li><li>@ 不可以是邮件地址的首字符</li><li>@ 之后需有至少一个点号</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">validateForm</span>(<span class="hljs-params"></span>)&#123;<br>  <span class="hljs-keyword">var</span> x=<span class="hljs-variable language_">document</span>.<span class="hljs-property">forms</span>[<span class="hljs-string">&quot;myForm&quot;</span>][<span class="hljs-string">&quot;email&quot;</span>].<span class="hljs-property">value</span>;<br>  <span class="hljs-keyword">var</span> atpos=x.<span class="hljs-title function_">indexOf</span>(<span class="hljs-string">&quot;@&quot;</span>);<br>  <span class="hljs-keyword">var</span> dotpos=x.<span class="hljs-title function_">lastIndexOf</span>(<span class="hljs-string">&quot;.&quot;</span>);<br>  <span class="hljs-keyword">if</span> (atpos&lt;<span class="hljs-number">1</span> || dotpos&lt;atpos+<span class="hljs-number">2</span> || dotpos+<span class="hljs-number">2</span>&gt;=x.<span class="hljs-property">length</span>)&#123;<br>    <span class="hljs-title function_">alert</span>(<span class="hljs-string">&quot;不是一个有效的 e-mail 地址&quot;</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ol><li>获取某个值<ol><li><strong>document.forms[ ][ ].value</strong></li><li>document.getElementById(“id”).value;</li></ol></li><li>if-else判断是否合条件<ol><li>@不在首位<ol><li><strong>indexof(“@”)&gt;&#x3D;1</strong></li><li>x.indexOf(“@”)&lt;1,0是开头，不满足</li></ol></li><li>.在@之后且不紧跟着@<ol><li><strong>dotpos&gt;&#x3D;atpos+2</strong></li><li>x.lastIndexOf(“.”)&lt;x.indexOf(“@”)+2</li></ol></li><li>.不是最后一位，.后有后缀<ol><li><strong>dotpos+1&lt;&#x3D;x.length</strong></li><li>x.lastIndexOf(“.”)+2&gt;&#x3D;x.length</li></ol></li></ol></li></ol><h1 id="JavaScript-验证-API"><a href="#JavaScript-验证-API" class="headerlink" title="JavaScript 验证 API"></a>JavaScript 验证 API</h1><p>input 元素有个 validationMessage 属性</p><ul><li><p>自定义错误提示信息的方法</p><ul><li><p><strong>使用 setCustomValidity 设置了自定义提示</strong></p><ul><li>&#96;&#96;&#96;javascript<br>setCustomValidity(‘’)<br>setCustomValidity(null)<br>setCustomValidity(undefined)<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><br>- ```<span class="language-javascript">javascript</span><br><span class="language-javascript">  <span class="hljs-keyword">function</span> <span class="hljs-title function_">myFunction</span>(<span class="hljs-params"></span>) &#123;</span><br><span class="language-javascript">      <span class="hljs-keyword">var</span> inpObj = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&quot;id1&quot;</span>);</span><br><span class="language-javascript">      <span class="hljs-keyword">if</span> (inpObj.<span class="hljs-title function_">checkValidity</span>() == <span class="hljs-literal">false</span>) &#123;</span><br><span class="language-javascript">          <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&quot;demo&quot;</span>).<span class="hljs-property">innerHTML</span> = inpObj.<span class="hljs-property">validationMessage</span>;</span><br><span class="language-javascript">      &#125; <span class="hljs-keyword">else</span> &#123;</span><br><span class="language-javascript">          <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&quot;demo&quot;</span>).<span class="hljs-property">innerHTML</span> = <span class="hljs-string">&quot;输入正确&quot;</span>;</span><br><span class="language-javascript">      &#125;</span><br><span class="language-javascript">  &#125;</span><br></code></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p><img src="/JavaScript.assets/image-20220308221354220.png" alt="image-20220308221354220"></p><h2 id="创建正则表达式对象"><a href="#创建正则表达式对象" class="headerlink" title="创建正则表达式对象"></a>创建正则表达式对象</h2><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> xxx=<span class="hljs-keyword">new</span> <span class="hljs-title class_">RegExp</span>(<span class="hljs-string">&quot;正则表达式&quot;</span>,<span class="hljs-string">&quot;匹配模式&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/JavaScript.assets/image-20220308222049389.png" alt="image-20220308222049389"></p><ul><li>这个表达式：是否含有“a”，开头结尾都可以</li><li>大小写严格区分</li></ul><p><img src="/JavaScript.assets/image-20220308222100442.png" alt="image-20220308222100442"></p><p><img src="/JavaScript.assets/image-20220308222339187.png" alt="image-20220308222339187"></p><h2 id="使用字面量的语法"><a href="#使用字面量的语法" class="headerlink" title="使用字面量的语法"></a>使用字面量的语法</h2><p><img src="/JavaScript.assets/image-20220308222528175.png" alt="image-20220308222528175"></p><p><img src="/JavaScript.assets/image-20220308222553637.png" alt="image-20220308222553637"></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">var</span> reg=<span class="hljs-keyword">new</span> <span class="hljs-title class_">RegExp</span>(<span class="hljs-string">&quot;a&quot;</span>,<span class="hljs-string">&quot;i&quot;</span>)<br>reg=<span class="hljs-regexp">/a/i</span><br><br><span class="hljs-keyword">var</span> result=reg.<span class="hljs-title function_">test</span>(<span class="hljs-string">&quot;abc&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="或-x3D-x3D-x3D-x3D-x3D-x3D-x3D-x3D"><a href="#或-x3D-x3D-x3D-x3D-x3D-x3D-x3D-x3D" class="headerlink" title="或 &#x3D;&#x3D;|&#x3D;&#x3D;  &#x3D;&#x3D;[]&#x3D;&#x3D;"></a>或 &#x3D;&#x3D;|&#x3D;&#x3D;  &#x3D;&#x3D;[]&#x3D;&#x3D;</h3><p><img src="/JavaScript.assets/image-20220308222933763.png" alt="image-20220308222933763"></p><p><img src="/JavaScript.assets/image-20220308223006752.png" alt="image-20220308223006752"></p><p><img src="/JavaScript.assets/image-20220308223015911.png" alt="image-20220308223015911"></p><h3 id="任意大-x2F-小写字母"><a href="#任意大-x2F-小写字母" class="headerlink" title="任意大&#x2F;小写字母"></a>任意大&#x2F;小写字母</h3><p><img src="/JavaScript.assets/image-20220308223026875.png" alt="image-20220308223026875"></p><p> <img src="/JavaScript.assets/image-20220308223049887.png" alt="image-20220308223049887"></p><h3 id="任意字母-A-z"><a href="#任意字母-A-z" class="headerlink" title="任意字母 A-z"></a>任意字母 A-z</h3><p><img src="/JavaScript.assets/image-20220308223104439.png" alt="image-20220308223104439"></p><p><img src="/JavaScript.assets/image-20220308223146508.png" alt="image-20220308223146508"></p><h3 id="中间任意选"><a href="#中间任意选" class="headerlink" title="中间任意选"></a>中间任意选</h3><p><img src="/JavaScript.assets/image-20220308223229240.png" alt="image-20220308223229240"></p><h3 id="任意数字"><a href="#任意数字" class="headerlink" title="任意数字"></a>任意数字</h3><p><img src="/JavaScript.assets/image-20220309094150616.png" alt="image-20220309094150616"></p><h3 id="除了xxx"><a href="#除了xxx" class="headerlink" title="除了xxx"></a>除了xxx</h3><p><img src="/JavaScript.assets/image-20220309094115564.png" alt="image-20220309094115564"></p><h1 id="字符换和正则"><a href="#字符换和正则" class="headerlink" title="字符换和正则"></a>字符换和正则</h1><h3 id="split把字符串拆成数组（返回数组）"><a href="#split把字符串拆成数组（返回数组）" class="headerlink" title="split把字符串拆成数组（返回数组）"></a>split把字符串拆成数组（返回数组）</h3><p><img src="/JavaScript.assets/image-20220309094521594.png" alt="image-20220309094521594"></p><p>输出：1a2b3,4d5e6f</p><img src="JavaScript.assets/image-20220309095637166.png" alt="image-20220309095637166" style="zoom:150%;" /><ul><li>使用正则表达式作为拆分规则</li><li>输出：1,2,3,4,5,6,7</li><li><strong>不指定全局匹配，也会全部拆分</strong></li></ul><h3 id="search搜索（返回索引）"><a href="#search搜索（返回索引）" class="headerlink" title="search搜索（返回索引）"></a>search搜索（返回索引）</h3><p><img src="/JavaScript.assets/image-20220309095907382.png" alt="image-20220309095907382"></p><ul><li><strong>只会查找第一个</strong>，即使加了g</li></ul><h3 id="match匹配（提取内容）"><a href="#match匹配（提取内容）" class="headerlink" title="match匹配（提取内容）"></a>match匹配（提取内容）</h3><p>可以根据正则表达式，从一个字符串中将符合条件的内容提取出来</p><p><img src="/JavaScript.assets/image-20220309100045493.png" alt="image-20220309100045493"></p><ul><li>只会输出a</li><li>默认情况match只会找<strong>第一个符合要求的内容</strong>，找到后就停止检索</li><li>会将匹配到的内容封装成<strong>数组</strong>返回</li><li>可以设置全局匹配模式</li></ul><p><img src="/JavaScript.assets/image-20220309100147545.png" alt="image-20220309100147545"></p><h3 id="找出所有字母，不分大小写"><a href="#找出所有字母，不分大小写" class="headerlink" title="找出所有字母，不分大小写"></a>找出所有字母，不分大小写</h3><p><img src="/JavaScript.assets/image-20220309100220539.png" alt="image-20220309100220539"></p><p>+<strong>ig</strong></p><h3 id="replace替换"><a href="#replace替换" class="headerlink" title="replace替换"></a>replace替换</h3><p>可以将字符串中指定内容<strong>替换成新的内容</strong></p><p><img src="/JavaScript.assets/image-20220309100535292.png" alt="image-20220309100535292"></p><ul><li>默认<strong>只会替换第一个</strong></li><li>解决：使用正则表达式的<strong>全局匹配</strong>：</li></ul><p><img src="/JavaScript.assets/image-20220309100610037.png" alt="image-20220309100610037"></p><h3 id="replace删掉字母"><a href="#replace删掉字母" class="headerlink" title="replace删掉字母"></a>replace删掉字母</h3><p><img src="/JavaScript.assets/image-20220309100951017.png" alt="image-20220309100951017"></p><ul><li>用””空串替换a-z所有字母</li></ul><h2 id="量词"><a href="#量词" class="headerlink" title="量词"></a>量词</h2><p>设置一个内容出现的次数</p><p><img src="/JavaScript.assets/image-20220309101649715.png" alt="image-20220309101649715"></p><ul><li>{}只对<strong>前一个内容</strong>起作用</li><li>用（）</li></ul><p><img src="/JavaScript.assets/image-20220309101735946.png" alt="image-20220309101735946"></p><ul><li><h3 id="1-3-1到3次"><a href="#1-3-1到3次" class="headerlink" title="{1,3}1到3次"></a>{1,3}1到3次</h3></li><li><h3 id="3，-gt-x3D-3次"><a href="#3，-gt-x3D-3次" class="headerlink" title="{3，}&gt;&#x3D;3次"></a>{3，}&gt;&#x3D;3次</h3></li><li><p><img src="/JavaScript.assets/image-20220309102002950.png" alt="image-20220309102002950"></p></li></ul><p><img src="/JavaScript.assets/image-20220309102007678.png" alt="image-20220309102007678"></p><h3 id="检查是否以a开头"><a href="#检查是否以a开头" class="headerlink" title="^检查是否以a开头"></a>^检查是否以a开头</h3><p><img src="/JavaScript.assets/image-20220309102217732.png" alt="image-20220309102217732"></p><ul><li>&#x3D;&#x3D;&#x2F;^a&#x2F;以a开头&#x3D;&#x3D;</li><li>&#x3D;&#x3D;[^a]除了a&#x3D;&#x3D;</li></ul><h3 id="结尾"><a href="#结尾" class="headerlink" title="$结尾"></a>$结尾</h3><p><img src="/JavaScript.assets/image-20220309103257281.png" alt="image-20220309103257281"></p><h3 id="以a开头必须马上结尾-x2F-a-x2F"><a href="#以a开头必须马上结尾-x2F-a-x2F" class="headerlink" title="以a开头必须马上结尾&#x2F;^a$&#x2F;"></a>以a开头必须马上结尾&#x2F;^a$&#x2F;</h3><p><img src="/JavaScript.assets/image-20220309103337880.png" alt="image-20220309103337880"></p><ul><li>只能是“a”</li></ul><h3 id="以a开头或以a结尾-x2F-a-a-x2F"><a href="#以a开头或以a结尾-x2F-a-a-x2F" class="headerlink" title="以a开头或以a结尾 &#x2F;^a|a$&#x2F;"></a>以a开头或以a结尾 &#x2F;^a|a$&#x2F;</h3><p><img src="/JavaScript.assets/image-20220309103353696.png" alt="image-20220309103353696"></p><h3 id="检查手机号-x2F-1-3-9-0-9-9-x2F"><a href="#检查手机号-x2F-1-3-9-0-9-9-x2F" class="headerlink" title="检查手机号 &#x2F;^1[3-9][0-9]{9}$&#x2F;"></a>检查手机号 &#x2F;^1[3-9][0-9]{9}$&#x2F;</h3><ol><li>以1开头</li><li>第二位3-9</li><li>三位以后任意数字，一共9个  &#x2F;^1[3-9][0-9]{9}$&#x2F;</li></ol><p><img src="/JavaScript.assets/image-20220309103822531.png" alt="image-20220309103822531"></p><ul><li>1开头</li><li>3-9任意一个第二位</li><li>0-9任意一个重复9次且是结尾</li><li>如果不写^$<ul><li><img src="/JavaScript.assets/image-20220309104006822.png" alt="image-20220309104006822"></li><li>也会通过，不可以！</li></ul></li></ul><h3 id="检查是否含有"><a href="#检查是否含有" class="headerlink" title="检查是否含有.  ."></a>检查是否含有.  .</h3><ul><li>单纯的.表示<strong>任意字符</strong></li></ul><p><img src="/JavaScript.assets/image-20220309104201841.png" alt="image-20220309104201841"></p><ul><li>使用\转义字符</li><li>&#x3D;&#x3D;\ .&#x3D;&#x3D;</li></ul><p><img src="/JavaScript.assets/image-20220309104237670.png" alt="image-20220309104237670"></p><h3 id="检查是否含有-1"><a href="#检查是否含有-1" class="headerlink" title="检查是否含有\  \"></a>检查是否含有\  \</h3><p> \ \表示\</p><p><img src="/JavaScript.assets/image-20220309104538225.png" alt="image-20220309104538225"></p><p><img src="/JavaScript.assets/image-20220309104700637.png" alt="image-20220309104700637"></p><h3 id="单词边界（是否含有单词child）"><a href="#单词边界（是否含有单词child）" class="headerlink" title="单词边界（是否含有单词child）"></a>单词边界（是否含有单词child）</h3><p><img src="/JavaScript.assets/image-20220309110426036.png" alt="image-20220309110426036"></p><h3 id="去掉字符串中的所有空格-replace-x2F-s-x2F-g-’’"><a href="#去掉字符串中的所有空格-replace-x2F-s-x2F-g-’’" class="headerlink" title="去掉字符串中的所有空格  replace(&#x2F;\s&#x2F;g,’’)"></a>去掉字符串中的所有空格  replace(&#x2F;\s&#x2F;g,’’)</h3><p><img src="/JavaScript.assets/image-20220309105353811.png" alt="image-20220309105353811"></p><p>str.replace(&#x2F;<strong>\s</strong>&#x2F;g,””)</p><ul><li><p>会把<strong>前中后的空格都去掉</strong></p></li><li><p>去掉<strong>开头</strong>的空格：  replace(&#x2F;^\s&#x2F;,’’)</p><ul><li>replace(&#x2F;<strong>^\s</strong>&#x2F;,””)</li></ul></li><li><p>去掉开头的多个空格 ：  replace(&#x2F;^\s*&#x2F;,’’)</p><ul><li>replace(&#x2F;<strong>^\s</strong>*&#x2F;,””)</li></ul></li><li><p>去掉结尾的空格：  replace(&#x2F;\s*$&#x2F;,’’)</p><ul><li>replace(&#x2F;<em><em>\s</em>$</em>*&#x2F;,””)</li></ul></li><li><p>去掉<strong>开头或结尾的空格</strong>  replaace(&#x2F;^\s*|\s*$&#x2F;,’’)</p></li><li><p><img src="/JavaScript.assets/image-20220309105936766.png" alt="image-20220309105936766"></p></li></ul><p><img src="/JavaScript.assets/image-20220309105957515.png" alt="image-20220309105957515"></p><p><img src="/JavaScript.assets/image-20220309110147512.png" alt="image-20220309110147512"></p><p><img src="/JavaScript.assets/image-20220309110339516.png" alt="image-20220309110339516"></p><p>任意数字：</p><ul><li><strong>[0-9]</strong></li><li>\d</li><li>[d]</li></ul><p>任意字母：</p><ul><li><strong>[A-z]</strong></li><li><strong>[a-z]&#x2F;i</strong></li></ul><p>任意字母、数字、下划线：</p><ul><li>[A-z0-9_]</li></ul><h3 id="邮件正则"><a href="#邮件正则" class="headerlink" title="邮件正则"></a>邮件正则</h3><ul><li>&#x3D;&#x3D;*：有没有都行&#x3D;&#x3D;</li></ul><p><img src="/JavaScript.assets/image-20220309112016430.png" alt="image-20220309112016430"></p><ol><li><strong>任意字母数字</strong>下划线：<strong>\w</strong>{3,}</li><li>\ .</li><li><strong>\w+ (1次以上)</strong><ol><li>2和3<strong>可有可无</strong>，*</li></ol></li><li>[A-z0-9]+任意字母数字，1位以上</li></ol><p><img src="/JavaScript.assets/image-20220309112430110.png" alt="image-20220309112430110"></p><p>   <img src="/JavaScript.assets/image-20220309112546467.png" alt="image-20220309112546467"></p><ul><li>不要忘了开头和结尾符号</li></ul><h3 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h3><h4 id="dataset验证邮箱时："><a href="#dataset验证邮箱时：" class="headerlink" title="dataset验证邮箱时："></a>dataset验证邮箱时：</h4><p>var emailReg&#x3D;&#x2F;^\w{3,}(.\w+)*@[A-z0-9]+.[A-z]{2,5}**(\ .[A-z]{2,5}) *** $&#x2F;;</p><p>或var emailReg&#x3D;&#x2F;^\w{3,}(.\w+)<em>@[A-z0-9]+(\ .[A-z]{2,5})**{1,2}*</em> $&#x2F;;</p><p>最后的\ .[A-z]{2,5}应该是可有可无！</p><h4 id="验证手机号："><a href="#验证手机号：" class="headerlink" title="验证手机号："></a>验证手机号：</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">// 186  134-139  150-152</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">isPhone</span>(<span class="hljs-params">phoneNumber</span>) &#123;<br>  <span class="hljs-comment">//  var regExp=/^(186|13[4-9]|15[0-2])[0-9]&#123;8&#125;$/;</span><br>   <span class="hljs-keyword">var</span> regExp=<span class="hljs-regexp">/^1(86|3[4-9]|5[0-2])\d&#123;8&#125;$/</span>;<span class="hljs-comment">//记得结尾</span><br>   <span class="hljs-keyword">var</span> flag=regExp.<span class="hljs-title function_">test</span>(phoneNumber);<br>   <span class="hljs-keyword">return</span> flag;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="JS案例"><a href="#JS案例" class="headerlink" title="JS案例"></a>JS案例</h1><h2 id="ifelse简单"><a href="#ifelse简单" class="headerlink" title="ifelse简单"></a>ifelse简单</h2><p><a href="https://www.bilibili.com/video/BV1Sy4y1C7ha?p=58&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV1Sy4y1C7ha?p=58&amp;spm_id_from=pageDriver</a></p><p><img src="/JavaScript.assets/image-20220311103126730.png" alt="image-20220311103126730"></p><p><img src="/JavaScript.assets/image-20220311103211200.png" alt="image-20220311103211200"></p><h2 id="判断闰年"><a href="#判断闰年" class="headerlink" title="判断闰年"></a>判断闰年</h2><p><img src="/JavaScript.assets/image-20220311103341711.png" alt="image-20220311103341711"></p><p> <img src="/JavaScript.assets/image-20220311103609760.png" alt="image-20220311103609760"></p><ul><li>闰年判断：<ul><li>能被4整除且不能整除100</li><li>或能被400整除</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220311103710036.png" alt="image-20220311103710036"></p><h2 id="判断成绩"><a href="#判断成绩" class="headerlink" title="判断成绩"></a>判断成绩</h2><p><img src="/JavaScript.assets/image-20220311103827053.png" alt="image-20220311103827053"></p><p><img src="/JavaScript.assets/image-20220311104017260.png" alt="image-20220311104017260"></p><ul><li>大于等于：从大到小</li><li>小于等于：从小到大</li></ul><h2 id="数字补0"><a href="#数字补0" class="headerlink" title="数字补0"></a>数字补0</h2><p><img src="/JavaScript.assets/image-20220311104306794.png" alt="image-20220311104306794"></p><p><img src="/JavaScript.assets/image-20220311104331678.png" alt="image-20220311104331678"></p><ul><li>&#x3D;&#x3D;ifelse（2个选择）可以用三元表达式代替&#x3D;&#x3D;</li></ul><p><img src="/JavaScript.assets/image-20220311104443512.png" alt="image-20220311104443512"></p><ul><li>三元表达式一定有返回值，用一个<strong>变量承接</strong></li></ul><h2 id="switch注意事项"><a href="#switch注意事项" class="headerlink" title="switch注意事项"></a>switch注意事项</h2><p><img src="/JavaScript.assets/image-20220311104654279.png" alt="image-20220311104654279"></p><h2 id="switch-查询水果"><a href="#switch-查询水果" class="headerlink" title="switch 查询水果"></a>switch 查询水果</h2><p><img src="/JavaScript.assets/image-20220311104744878.png" alt="image-20220311104744878"></p><p><img src="/JavaScript.assets/image-20220311104801322.png" alt="image-20220311104801322"></p><p><img src="/JavaScript.assets/image-20220311104836355.png" alt="image-20220311104836355"></p><h1 id="断点"><a href="#断点" class="headerlink" title="断点"></a>断点</h1><p><img src="/JavaScript.assets/image-20220311104919122.png" alt="image-20220311104919122"></p><p><img src="/JavaScript.assets/image-20220311105052097.png" alt="image-20220311105052097"></p><p><img src="/JavaScript.assets/image-20220311105137465.png" alt="image-20220311105137465"></p><ul><li>watch：观察变量</li></ul><h2 id="for案例"><a href="#for案例" class="headerlink" title="for案例"></a>for案例</h2><p><img src="/JavaScript.assets/image-20220311105228801.png" alt="image-20220311105228801"></p><p><img src="/JavaScript.assets/image-20220311105335834.png" alt="image-20220311105335834"></p><p><img src="/JavaScript.assets/image-20220311105350525.png" alt="image-20220311105350525"></p><p> <img src="/JavaScript.assets/image-20220311110626460.png" alt="image-20220311110626460"></p><h3 id="输入第n次："><a href="#输入第n次：" class="headerlink" title="输入第n次："></a>输入第n次：</h3><p>(‘输入第’+i+’个xxxx’)</p><h2 id="一行n个星星"><a href="#一行n个星星" class="headerlink" title="一行n个星星"></a>一行n个星星</h2><p><img src="/JavaScript.assets/image-20220311110922809.png" alt="image-20220311110922809"></p><p><img src="/JavaScript.assets/image-20220311111140615.png" alt="image-20220311111140615"></p><h1 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h1><ul><li>有return 返回的是<strong>return后面的值</strong></li><li>没有return <strong>返回undefined</strong></li></ul><h1 id="break-continue-return"><a href="#break-continue-return" class="headerlink" title="break continue return"></a>break continue return</h1><p><img src="/JavaScript.assets/image-20220311111337496.png" alt="image-20220311111337496"></p><p><img src="/JavaScript.assets/image-20220311111610471.png" alt="image-20220311111610471"></p><h1 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h1><h2 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h2><p><img src="/JavaScript.assets/image-20220311113721728.png" alt="image-20220311113721728"></p><ul><li>多个属性\方法中间用逗号隔开</li></ul><h2 id="调用属性方法"><a href="#调用属性方法" class="headerlink" title="调用属性方法"></a>调用属性方法</h2><p><img src="/JavaScript.assets/image-20220311113847623.png" alt="image-20220311113847623"></p><h2 id="遍历对象属性"><a href="#遍历对象属性" class="headerlink" title="遍历对象属性"></a>遍历对象属性</h2><p><img src="/JavaScript.assets/image-20220311112425240.png" alt="image-20220311112425240"></p><h1 id="内置对象"><a href="#内置对象" class="headerlink" title="内置对象"></a>内置对象</h1><p><img src="/JavaScript.assets/image-20220311114744825.png" alt="image-20220311114744825"></p><ul><li>一些常见方法、功能<ul><li>&#x3D;&#x3D;Math、Date、Array、String&#x3D;&#x3D;</li></ul></li></ul><h2 id="Math对象"><a href="#Math对象" class="headerlink" title="Math对象"></a>Math对象</h2><p><img src="/JavaScript.assets/image-20220311115437763.png" alt="image-20220311115437763"></p><h3 id="封装自己的函数"><a href="#封装自己的函数" class="headerlink" title="封装自己的函数"></a>封装自己的函数</h3><p><img src="/JavaScript.assets/image-20220311115703954.png" alt="image-20220311115703954"></p><h4 id="x3D-x3D-let和var？？-x3D-x3D"><a href="#x3D-x3D-let和var？？-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;let和var？？&#x3D;&#x3D;"></a>&#x3D;&#x3D;let和var？？&#x3D;&#x3D;</h4><p><img src="/JavaScript.assets/image-20220311115820126.png" alt="image-20220311115820126"></p><h3 id="random（）"><a href="#random（）" class="headerlink" title="random（）"></a>random（）</h3><p><img src="/JavaScript.assets/image-20220311120225054.png" alt="image-20220311120225054"></p><ul><li>Math.random()返回<strong>随机小数</strong></li><li>不需要跟着参数</li></ul><p><img src="/JavaScript.assets/image-20220311120423154.png" alt="image-20220311120423154"></p><h3 id="随机点名"><a href="#随机点名" class="headerlink" title="随机点名"></a>随机点名</h3><p><img src="/JavaScript.assets/image-20220311120517653.png" alt="image-20220311120517653"></p><ul><li>getrandom返回的是数值</li><li>我此处arr后要改变数值使其随机</li><li>&#x3D;&#x3D;&#x3D;》agetrandom返回的数值作为参数</li></ul><p><img src="/JavaScript.assets/image-20220311120635793.png" alt="image-20220311120635793"></p><ul><li>数组长度：arr.length</li></ul><h3 id="猜数字游戏"><a href="#猜数字游戏" class="headerlink" title="猜数字游戏"></a>猜数字游戏</h3><p><img src="/JavaScript.assets/image-20220312142252107.png" alt="image-20220312142252107"></p><ul><li>while(true)死循环，适合于不知道什么时候会停止的循环<ul><li>在函数内部某个条件下使用 break 跳出所有循环</li></ul></li></ul><h2 id="Date对象"><a href="#Date对象" class="headerlink" title="Date对象"></a>Date对象</h2><ul><li>与Math不同</li><li>需要<strong>通过Date构造函数实例化日期对象</strong>（new）</li></ul><p><img src="/JavaScript.assets/image-20220312142716266.png" alt="image-20220312142716266"></p><p><img src="/JavaScript.assets/image-20220312142817834.png" alt="image-20220312142817834"></p><p><img src="/JavaScript.assets/image-20220312143003133.png" alt="image-20220312143003133"></p><p><img src="/JavaScript.assets/image-20220312143016166.png" alt="image-20220312143016166"></p><ul><li>使用字符串**”2019-10-1”<strong>构造函数能得到</strong>准确的时间**</li><li>使用2019，10，1得到不准确</li></ul><p><img src="/JavaScript.assets/image-20220312143458878.png" alt="image-20220312143458878"></p><p><img src="/JavaScript.assets/image-20220312143630027.png" alt="image-20220312143630027"></p><p><img src="/JavaScript.assets/image-20220312143721309.png" alt="image-20220312143721309"></p><ul><li>星期三：不想返回“3”，但是getday只会返回索引：练习数组，返回具体名字</li><li>要注意周日写在数组0号位置上</li></ul><p><img src="/JavaScript.assets/image-20220312144048376.png" alt="image-20220312144048376"></p><ul><li>返回时分秒、不足10的数字写做03、04、05</li></ul><h2 id="总的毫秒数（时间戳）"><a href="#总的毫秒数（时间戳）" class="headerlink" title="总的毫秒数（时间戳）"></a>总的毫秒数（时间戳）</h2><p><img src="/JavaScript.assets/image-20220312154325448.png" alt="image-20220312154325448"></p><h1 id="案例-秒杀系统"><a href="#案例-秒杀系统" class="headerlink" title="案例 秒杀系统"></a>案例 秒杀系统</h1><p><img src="/JavaScript.assets/image-20220312154756439.png" alt="image-20220312154756439"></p><p> <img src="/JavaScript.assets/image-20220312155057270.png" alt="image-20220312155057270"></p><p>   <img src="/JavaScript.assets/image-20220312155332259.png" alt="image-20220312155332259"></p><h1 id="数组对象"><a href="#数组对象" class="headerlink" title="数组对象"></a>数组对象</h1><p><img src="/JavaScript.assets/image-20220312155431361.png" alt="image-20220312155431361"></p><p><img src="/JavaScript.assets/image-20220312155610063.png" alt="image-20220312155610063"></p><ul><li><strong>new Array方式创建</strong>：<strong>只有1个参数相当于长度</strong>，2个参数是<strong>数组中的元素</strong></li></ul><h2 id="反转数组"><a href="#反转数组" class="headerlink" title="反转数组"></a>反转数组</h2><p><img src="/JavaScript.assets/image-20220312155753212.png" alt="image-20220312155753212"></p><h2 id="判断是否为数组"><a href="#判断是否为数组" class="headerlink" title="判断是否为数组"></a>判断是否为数组</h2><p><img src="/JavaScript.assets/image-20220312155959336.png" alt="image-20220312155959336"></p><p><img src="/JavaScript.assets/image-20220312160025430.png" alt="image-20220312160025430"></p><p><img src="/JavaScript.assets/image-20220312160229645.png" alt="image-20220312160229645"></p><ul><li>instanceof</li><li>Array.isArray(params)</li></ul><h2 id="添加数据删除数组"><a href="#添加数据删除数组" class="headerlink" title="添加数据删除数组"></a>添加数据删除数组</h2><h3 id="在后面添加元素-push"><a href="#在后面添加元素-push" class="headerlink" title="在后面添加元素 push"></a>在后面添加元素 push</h3><p><img src="/JavaScript.assets/image-20220312162125281.png" alt="image-20220312162125281"></p><ul><li>数组名.push（x,x,x,…)可以添加不同类型的元素</li><li>数组名 返回数组长度</li></ul><h3 id="开头添加-unshift"><a href="#开头添加-unshift" class="headerlink" title="开头添加 unshift"></a>开头添加 unshift</h3><p><img src="/JavaScript.assets/image-20220312162319176.png" alt="image-20220312162319176"></p><h2 id="删除数组元素-pop-shift"><a href="#删除数组元素-pop-shift" class="headerlink" title="删除数组元素 pop shift"></a>删除数组元素 pop shift</h2><p><img src="/JavaScript.assets/image-20220312162409197.png" alt="image-20220312162409197"></p><p><img src="/JavaScript.assets/image-20220312162444771.png" alt="image-20220312162444771"></p><h2 id="筛选数组"><a href="#筛选数组" class="headerlink" title="筛选数组"></a>筛选数组</h2><p><img src="/JavaScript.assets/image-20220312162626693.png" alt="image-20220312162626693"></p><h2 id="174-数组反转与排序"><a href="#174-数组反转与排序" class="headerlink" title="174 数组反转与排序"></a>174 数组反转与排序</h2><p><img src="/JavaScript.assets/image-20220312162922042.png" alt="image-20220312162922042"></p><ul><li>sort直接排序有问题</li><li>&#x3D;&#x3D;需要a-b或者b-a&#x3D;&#x3D;</li></ul><h2 id="175-indexof-、lastindexof"><a href="#175-indexof-、lastindexof" class="headerlink" title="175 indexof 、lastindexof"></a>175 indexof 、lastindexof</h2><p><img src="/JavaScript.assets/image-20220313153009072.png" alt="image-20220313153009072"></p><p><img src="/JavaScript.assets/image-20220313153020892.png" alt="image-20220313153020892"></p><ul><li>从前往后找</li><li>从后往前找</li></ul><h2 id="175-数组去重"><a href="#175-数组去重" class="headerlink" title="175 数组去重"></a>175 数组去重</h2><ul><li>一个一个存进新数组，如果已存在，就不存</li></ul><p><img src="/JavaScript.assets/image-20220313153240510.png" alt="image-20220313153240510"></p><p><img src="/JavaScript.assets/image-20220313153405139.png" alt="image-20220313153405139"></p><p><img src="/JavaScript.assets/image-20220313153437099.png" alt="image-20220313153437099"></p><ul><li>indexof判断是否存在</li><li>push添加</li></ul><h2 id="177-数组转换为字符串"><a href="#177-数组转换为字符串" class="headerlink" title="177 数组转换为字符串"></a>177 数组转换为字符串</h2><p><img src="/JavaScript.assets/image-20220313153738320.png" alt="image-20220313153738320"></p><p><img src="/JavaScript.assets/image-20220313153925581.png" alt="image-20220313153925581"></p><ul><li>toString（）</li><li>join（‘分隔符’）</li></ul><p><img src="/JavaScript.assets/image-20220313153954240.png" alt="image-20220313153954240"></p><h1 id="178-字符串对象"><a href="#178-字符串对象" class="headerlink" title="178 字符串对象"></a>178 字符串对象</h1><p><img src="/JavaScript.assets/image-20220313154239311.png" alt="image-20220313154239311"></p><p><img src="/JavaScript.assets/image-20220313154257498.png" alt="image-20220313154257498"></p><h2 id="179-字符串的不可变"><a href="#179-字符串的不可变" class="headerlink" title="179 字符串的不可变"></a>179 字符串的不可变</h2><p><img src="/JavaScript.assets/image-20220313154559930.png" alt="image-20220313154559930"></p><h2 id="180-根据字符返回位置"><a href="#180-根据字符返回位置" class="headerlink" title="180 根据字符返回位置"></a>180 根据字符返回位置</h2><p><img src="/JavaScript.assets/image-20220313154737672.png" alt="image-20220313154737672"></p><ul><li>想用indexof查找位于后面的，加个参数</li></ul><p><img src="/JavaScript.assets/image-20220313155020515.png" alt="image-20220313155020515"></p><ul><li>找到多个index：<ul><li>循环，index不为-1（找到了）就继续找，index+1</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220313155117760.png" alt="image-20220313155117760"></p><h2 id="根据位置返回字符"><a href="#根据位置返回字符" class="headerlink" title="根据位置返回字符"></a>根据位置返回字符</h2><p><img src="/JavaScript.assets/image-20220313155207761.png" alt="image-20220313155207761"></p><p><img src="/JavaScript.assets/image-20220313155414393.png" alt="image-20220313155414393"></p><ul><li>charat（）和[index]效果同，一个当作字符串，一个当作数组</li></ul><h2 id="183-统计出现最多的"><a href="#183-统计出现最多的" class="headerlink" title="183 统计出现最多的"></a>183 统计出现最多的</h2><p><img src="/JavaScript.assets/image-20220313161400069.png" alt="image-20220313161400069"></p><p><img src="/JavaScript.assets/image-20220313162456830.png" alt="image-20220313162456830"></p><p><img src="/JavaScript.assets/image-20220313162831249.png" alt="image-20220313162831249"></p><h2 id="185-拼接与截取"><a href="#185-拼接与截取" class="headerlink" title="185 拼接与截取"></a>185 拼接与截取</h2><p><img src="/JavaScript.assets/image-20220313162949660.png" alt="image-20220313162949660"></p><ul><li>concat等价于+连接</li><li><strong>substr截取</strong></li></ul><h2 id="186-replace替换和split分割成数组"><a href="#186-replace替换和split分割成数组" class="headerlink" title="186 replace替换和split分割成数组"></a>186 replace替换和split分割成数组</h2><p><img src="/JavaScript.assets/image-20220313163319079.png" alt="image-20220313163319079"></p><p><img src="/JavaScript.assets/image-20220313163400982.png" alt="image-20220313163400982"></p><ul><li>需要原本字符有个分割</li></ul><h2 id="大小写"><a href="#大小写" class="headerlink" title="大小写"></a>大小写</h2><p><img src="/JavaScript.assets/image-20220313163419757.png" alt="image-20220313163419757"></p><h2 id="小作业"><a href="#小作业" class="headerlink" title="小作业"></a>小作业</h2><p><img src="/JavaScript.assets/image-20220313163434514.png" alt="image-20220313163434514"></p><h1 id="194-DOM"><a href="#194-DOM" class="headerlink" title="194 DOM"></a>194 DOM</h1><p><img src="/JavaScript.assets/image-20220313164107544.png" alt="image-20220313164107544"></p><p><img src="/JavaScript.assets/image-20220313164206510.png" alt="image-20220313164206510"></p><h2 id="196-获取元素-id"><a href="#196-获取元素-id" class="headerlink" title="196 获取元素-id"></a>196 获取元素-id</h2><p><img src="/JavaScript.assets/image-20220313164325979.png" alt="image-20220313164325979"></p><p>&#x3D;&#x3D;documemt.getElementById（）&#x3D;&#x3D;</p><p><img src="/JavaScript.assets/image-20220313164641490.png" alt="image-20220313164641490"></p><ul><li>返回的是元素对象</li></ul><h2 id="197-获取元素-tag"><a href="#197-获取元素-tag" class="headerlink" title="197 获取元素 -tag"></a>197 获取元素 -tag</h2><blockquote><p>获取多个</p></blockquote><h4 id="根据标签名"><a href="#根据标签名" class="headerlink" title="根据标签名"></a>根据标签名</h4><p><img src="/JavaScript.assets/image-20220313164922363.png" alt="image-20220313164922363"></p><p><img src="/JavaScript.assets/image-20220313164937728.png" alt="image-20220313164937728"></p><p><img src="/JavaScript.assets/image-20220313165013788.png" alt="image-20220313165013788"></p><ul><li>返回的是一个对象的<strong>数组</strong></li><li>&#x3D;&#x3D;即使只有1个li，也是<strong>数组</strong>&#x3D;&#x3D;</li><li>没有li，返回[]</li></ul><p><img src="/JavaScript.assets/image-20220313165303004.png" alt="image-20220313165303004"></p><ul><li>想要获取ol下的li<ul><li>把element换成相应的父元素</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220313165403809.png" alt="image-20220313165403809"></p><h2 id="获取类"><a href="#获取类" class="headerlink" title="获取类"></a>获取类</h2><p><img src="/JavaScript.assets/image-20220313165521419.png" alt="image-20220313165521419"></p><ul><li>queryselector只会返回第一个对象</li><li>queryselector需要符号. #</li></ul><p><img src="/JavaScript.assets/image-20220313165636703.png" alt="image-20220313165636703"></p><p><img src="/JavaScript.assets/image-20220313165740324.png" alt="image-20220313165740324"></p><p><img src="/JavaScript.assets/image-20220313165822693.png" alt="image-20220313165822693"></p><h2 id="获取body、html"><a href="#获取body、html" class="headerlink" title="获取body、html"></a>获取body、html</h2><ul><li>两个特殊，需要注意</li></ul><p><img src="/JavaScript.assets/image-20220313165934383.png" alt="image-20220313165934383"></p><p><img src="/JavaScript.assets/image-20220313165942128.png" alt="image-20220313165942128"></p><h2 id="200-事件基础"><a href="#200-事件基础" class="headerlink" title="200 事件基础"></a>200 事件基础</h2><p><img src="/JavaScript.assets/image-20220313170123542.png" alt="image-20220313170123542"></p><ul><li>事件源<ul><li>被触发的那个，需要获得事件源</li></ul></li><li>事件类型<ul><li>js里.&#x3D;&#x3D;onclick&#x3D;&#x3D;</li></ul></li><li>事件处理程序<ul><li>function（）{}</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220313171142940.png" alt="image-20220313171142940"></p><p><img src="/JavaScript.assets/image-20220313171223615.png" alt="image-20220313171223615"></p><p><img src="/JavaScript.assets/image-20220313172741218.png" alt="image-20220313172741218"></p><p><img src="/JavaScript.assets/image-20220313172749663.png" alt="image-20220313172749663"></p><h2 id="改变元素内容-innertext"><a href="#改变元素内容-innertext" class="headerlink" title="改变元素内容 innertext"></a>改变元素内容 innertext</h2><p><img src="/JavaScript.assets/image-20220313172915457.png" alt="image-20220313172915457"></p><p><img src="/JavaScript.assets/image-20220313173533549.png" alt="image-20220313173533549"></p><p>  <img src="/JavaScript.assets/image-20220313173547078.png" alt="image-20220313173547078"></p><h2 id="innerhtml"><a href="#innerhtml" class="headerlink" title="innerhtml"></a>innerhtml</h2><p><img src="/JavaScript.assets/image-20220313173920223.png" alt="image-20220313173920223"></p><p><img src="/JavaScript.assets/image-20220313174038841.png" alt="image-20220313174038841"></p><p><img src="/JavaScript.assets/image-20220313174104316.png" alt="image-20220313174104316"></p><hr><p>  &#x2F;&#x2F; a标签添加字体图标</p><p>  a.innerHTML&#x3D;”<i class='fa fa-items'></i>“</p><p><img src="/JavaScript.assets/image-20220330123147095.png" alt="image-20220330123147095"></p><ul><li>相当于同时设定了<strong>标签名和内容 （整个tag）</strong></li></ul><h2 id="204-元素属性"><a href="#204-元素属性" class="headerlink" title="204 元素属性"></a>204 元素属性</h2><p><img src="/JavaScript.assets/image-20220313174429896.png" alt="image-20220313174429896"></p><p> <img src="/JavaScript.assets/image-20220316200100490.png" alt="image-20220316200100490"></p><p><img src="/JavaScript.assets/image-20220316200127496.png" alt="image-20220316200127496"></p><ul><li>先获取各个元素，ldh，zxy，img</li><li>在各个元素上改变属性</li></ul><p> <img src="/JavaScript.assets/image-20220316200543696.png" alt="image-20220316200543696"></p><p><img src="/JavaScript.assets/image-20220316200550736.png" alt="image-20220316200550736"></p><h1 id="206-表单-修改属性"><a href="#206-表单-修改属性" class="headerlink" title="206 表单 修改属性"></a>206 表单 修改属性</h1><p><img src="/JavaScript.assets/image-20220316200659159.png" alt="image-20220316200659159"></p><h2 id="value-输入框里的值"><a href="#value-输入框里的值" class="headerlink" title="value 输入框里的值"></a>value 输入框里的值</h2><p><img src="/JavaScript.assets/image-20220316200824678.png" alt="image-20220316200824678"></p><h2 id="disabled-按钮禁止点击"><a href="#disabled-按钮禁止点击" class="headerlink" title="disabled 按钮禁止点击"></a>disabled 按钮禁止点击</h2><p><img src="/JavaScript.assets/image-20220316200924628.png" alt="image-20220316200924628"></p><p><img src="/JavaScript.assets/image-20220316201000030.png" alt="image-20220316201000030"></p><h2 id="显示密码"><a href="#显示密码" class="headerlink" title="显示密码"></a>显示密码</h2><p><img src="/JavaScript.assets/image-20220316201056334.png" alt="image-20220316201056334"></p><p><img src="/JavaScript.assets/image-20220316201126644.png" alt="image-20220316201126644"></p><p><img src="/JavaScript.assets/image-20220316201302634.png" alt="image-20220316201302634"></p><p><img src="/JavaScript.assets/image-20220316201819824.png" alt="image-20220316201819824"></p><p><img src="/JavaScript.assets/image-20220316201834553.png" alt="image-20220316201834553"></p><p><img src="/JavaScript.assets/image-20220316201843977.png" alt="image-20220316201843977"></p><p><img src="/JavaScript.assets/image-20220316202012879.png" alt="image-20220316202012879"></p><p><img src="/JavaScript.assets/image-20220316202022266.png" alt="image-20220316202022266"></p><p><img src="/JavaScript.assets/image-20220316202032530.png" alt="image-20220316202032530"></p><p><img src="/JavaScript.assets/image-20220317095225735.png" alt="image-20220317095225735"></p><h1 id="207-样式修改"><a href="#207-样式修改" class="headerlink" title="207 样式修改"></a>207 样式修改</h1><p><img src="/JavaScript.assets/image-20220317095558156.png" alt="image-20220317095558156"></p><h1 id="显示与隐藏"><a href="#显示与隐藏" class="headerlink" title="显示与隐藏"></a>显示与隐藏</h1><p><img src="/JavaScript.assets/image-20220317100122291.png" alt="image-20220317100122291"></p><p><img src="/JavaScript.assets/image-20220317100146033.png" alt="image-20220317100146033"></p><h3 id="带有附属品的布局："><a href="#带有附属品的布局：" class="headerlink" title="带有附属品的布局："></a>带有附属品的布局：</h3><ul><li><p>一个div，放文字和img</p></li><li><p>还有一个小div，通过定位移到左边去</p><p><img src="/JavaScript.assets/image-20220317100349558.png" alt="image-20220317100349558"></p></li><li><p>style.display&#x3D;’none’</p></li></ul><h2 id="循环精灵图"><a href="#循环精灵图" class="headerlink" title="循环精灵图"></a>循环精灵图</h2><p><img src="/JavaScript.assets/image-20220317102447036.png" alt="image-20220317102447036"></p><p>  <img src="/JavaScript.assets/image-20220317104719543.png" alt="image-20220317104719543"></p><p><img src="/JavaScript.assets/image-20220317105214951.png" alt="image-20220317105214951"></p><p><img src="/JavaScript.assets/image-20220317105224474.png" alt="image-20220317105224474"></p><p><img src="/JavaScript.assets/image-20220317111256169.png" alt="image-20220317111256169"></p><ul><li><p>lis[i].style.<strong>backgroundPosition</strong>&#x3D;&#x3D;&#x3D;’0 -‘+index+’px’&#x3D;&#x3D;</p></li><li><p>‘0 -xxpx’使用变量形式：<strong>字符串连接</strong></p></li></ul><h1 id="焦点事件"><a href="#焦点事件" class="headerlink" title="焦点事件"></a>焦点事件</h1><p><img src="/JavaScript.assets/image-20220317111503402.png" alt="image-20220317111503402"></p><ul><li>onfocus获得、onblur失去</li></ul><p><img src="/JavaScript.assets/image-20220317111949007.png" alt="image-20220317111949007"></p><ul><li>value如果不定义onfocus和onblur的话，输入密码时提示文字会一直在那里，直接使用placeholder！</li></ul><h1 id="修改类"><a href="#修改类" class="headerlink" title="修改类"></a>修改类</h1><p><img src="/JavaScript.assets/image-20220317112906939.png" alt="image-20220317112906939"></p><p><img src="/JavaScript.assets/image-20220317112958675.png" alt="image-20220317112958675"></p><ul><li>保留原先类名，增加新的【多类名选择器】</li></ul><p><img src="/JavaScript.assets/image-20220317113137568.png" alt="image-20220317113137568"></p><h1 id="登录信息验证"><a href="#登录信息验证" class="headerlink" title="登录信息验证"></a>登录信息验证</h1><p><img src="/JavaScript.assets/image-20220317113314352.png" alt="image-20220317113314352"></p><p><img src="/JavaScript.assets/image-20220317114415322.png" alt="image-20220317114415322"></p><p><img src="/JavaScript.assets/image-20220317114633130.png" alt="image-20220317114633130"></p><p><img src="/JavaScript.assets/image-20220317114849012.png" alt="image-20220317114849012"></p><h1 id="Jquery"><a href="#Jquery" class="headerlink" title="Jquery"></a>Jquery</h1><p><a href="https://www.bilibili.com/vid">https://www.bilibili.com/vid</a> eo&#x2F;BV1W54y1J7Ed?p&#x3D;71&amp;spm_id_from&#x3D;pageDriver</p><p><img src="/JavaScript.assets/image-20220310105209602.png" alt="image-20220310105209602"></p><h2 id="获取元素：选择器"><a href="#获取元素：选择器" class="headerlink" title="获取元素：选择器"></a>获取元素：选择器</h2><p><img src="/JavaScript.assets/image-20220310105226961.png" alt="image-20220310105226961"></p><p><img src="/JavaScript.assets/image-20220310105249431.png" alt="image-20220310105249431"></p><p><img src="/JavaScript.assets/image-20220310105319484.png" alt="image-20220310105319484"></p><p><img src="/JavaScript.assets/image-20220310105344550.png" alt="image-20220310105344550"></p><ul><li>使用**$(‘ ‘)**获取元素</li></ul><h2 id="筛选器"><a href="#筛选器" class="headerlink" title="筛选器"></a>筛选器</h2><p><img src="/JavaScript.assets/image-20220310105431797.png" alt="image-20220310105431797"></p><p><img src="/JavaScript.assets/image-20220310105457411.png" alt="image-20220310105457411"></p><p><img src="/JavaScript.assets/image-20220310111734636.png" alt="image-20220310111734636"></p><p><img src="/JavaScript.assets/image-20220310111818654.png" alt="image-20220310111818654"></p><ul><li>find要参数，找满足i标签&#x2F;id&#x2F;class的</li></ul><p><img src="/JavaScript.assets/image-20220310111831628.png" alt="image-20220310111831628"></p><h2 id="1-基本选择器"><a href="#1-基本选择器" class="headerlink" title="1.基本选择器"></a>1.基本选择器</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;<span class="hljs-selector-id">#id</span>&quot;)            //ID选择器<br>$(&quot;<span class="hljs-selector-tag">div</span>&quot;)            //元素选择器<br>$(&quot;<span class="hljs-selector-class">.classname</span>&quot;)     //类选择器<br>$(&quot;<span class="hljs-selector-class">.classname</span>,<span class="hljs-selector-class">.classname1</span>,<span class="hljs-selector-id">#id1</span>&quot;)     //组合选择器<br></code></pre></td></tr></table></figure><h2 id="2-层次选择器"><a href="#2-层次选择器" class="headerlink" title="2.层次选择器"></a>2.层次选择器</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;<span class="hljs-selector-id">#id</span>&gt;<span class="hljs-selector-class">.classname</span> &quot;)    //子元素选择器<br>$(&quot;<span class="hljs-selector-id">#id</span> <span class="hljs-selector-class">.classname</span> &quot;)    //后代元素选择器<br>$(&quot;<span class="hljs-selector-id">#id</span> + <span class="hljs-selector-class">.classname</span> &quot;)    //紧邻下一个元素选择器<br>$(&quot;<span class="hljs-selector-id">#id</span> ~ <span class="hljs-selector-class">.classname</span> &quot;)    //兄弟元素选择器<br></code></pre></td></tr></table></figure><h2 id="3-过滤选择器-重点"><a href="#3-过滤选择器-重点" class="headerlink" title="3.过滤选择器(重点)"></a>3.过滤选择器(重点)</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;<span class="hljs-selector-tag">li</span><span class="hljs-selector-pseudo">:first</span>&quot;)    //第一个<span class="hljs-selector-tag">li</span><br>$(&quot;<span class="hljs-selector-tag">li</span>:last<span class="hljs-string">&quot;)     //最后一个li</span><br><span class="hljs-string">$(&quot;</span>li:even<span class="hljs-string">&quot;)     //挑选下标为偶数的li</span><br><span class="hljs-string">$(&quot;</span>li:odd<span class="hljs-string">&quot;)      //挑选下标为奇数的li</span><br><span class="hljs-string">$(&quot;</span>li:<span class="hljs-built_in">eq</span>(<span class="hljs-number">4</span>)<span class="hljs-string">&quot;)    //下标等于 4 的li(第五个 li 元素)</span><br><span class="hljs-string">$(&quot;</span>li:<span class="hljs-built_in">gt</span>(<span class="hljs-number">2</span>)<span class="hljs-string">&quot;)    //下标大于 2 的li</span><br><span class="hljs-string">$(&quot;</span>li:<span class="hljs-built_in">lt</span>(<span class="hljs-number">2</span>)<span class="hljs-string">&quot;)    //下标小于 2 的li</span><br><span class="hljs-string">$(&quot;</span>li:<span class="hljs-built_in">not</span>(#runoob)<span class="hljs-string">&quot;) //挑选除 id=&quot;</span>runoob<span class="hljs-string">&quot; 以外的所有li</span><br></code></pre></td></tr></table></figure><p><strong>3.3可见性过滤选择器</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;<span class="hljs-selector-tag">li</span>:hidden<span class="hljs-string">&quot;)       //匹配所有不可见元素，或type为hidden的元素</span><br><span class="hljs-string">$(&quot;</span>li:visible<span class="hljs-string">&quot;)      //匹配所有可见元素</span><br></code></pre></td></tr></table></figure><p><strong>3.5状态过滤选择器</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;<span class="hljs-selector-tag">input</span><span class="hljs-selector-pseudo">:enabled</span>&quot;)    // 匹配可用的 <span class="hljs-selector-tag">input</span><br>$(&quot;<span class="hljs-selector-tag">input</span><span class="hljs-selector-pseudo">:disabled</span>&quot;)   // 匹配不可用的 <span class="hljs-selector-tag">input</span><br>$(&quot;<span class="hljs-selector-tag">input</span><span class="hljs-selector-pseudo">:checked</span>&quot;)    // 匹配选中的 <span class="hljs-selector-tag">input</span><br>$(&quot;option:selected<span class="hljs-string">&quot;)  // 匹配选中的 option</span><br></code></pre></td></tr></table></figure><h2 id="4-表单选择器"><a href="#4-表单选择器" class="headerlink" title="4.表单选择器"></a>4.表单选择器</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs css">$(&quot;:input<span class="hljs-string">&quot;)      //匹配所有 input, textarea, select 和 button 元素</span><br><span class="hljs-string">$(&quot;</span>:text<span class="hljs-string">&quot;)       //所有的单行文本框，$(&quot;</span>:text<span class="hljs-string">&quot;) 等价于$(&quot;</span>[type=text]<span class="hljs-string">&quot;)，推荐使用$(&quot;</span>input:text<span class="hljs-string">&quot;)效率更高，下同</span><br><span class="hljs-string">$(&quot;</span>:password<span class="hljs-string">&quot;)   //所有密码框</span><br><span class="hljs-string">$(&quot;</span>:radio<span class="hljs-string">&quot;)      //所有单选按钮</span><br><span class="hljs-string">$(&quot;</span>:checkbox<span class="hljs-string">&quot;)   //所有复选框</span><br><span class="hljs-string">$(&quot;</span>:submit<span class="hljs-string">&quot;)     //所有提交按钮</span><br><span class="hljs-string">$(&quot;</span>:reset<span class="hljs-string">&quot;)      //所有重置按钮</span><br><span class="hljs-string">$(&quot;</span>:button<span class="hljs-string">&quot;)     //所有button按钮</span><br><span class="hljs-string">$(&quot;</span>:file<span class="hljs-string">&quot;)       //所有文件域</span><br></code></pre></td></tr></table></figure><h2 id="操作文本内容"><a href="#操作文本内容" class="headerlink" title="操作文本内容"></a>操作文本内容</h2><p><img src="/JavaScript.assets/image-20220310111903424.png" alt="image-20220310111903424"></p><h3 id="html"><a href="#html" class="headerlink" title="html"></a>html</h3><p><img src="/JavaScript.assets/image-20220310112229562.png" alt="image-20220310112229562"></p><p><img src="/JavaScript.assets/image-20220310112246099.png" alt="image-20220310112246099"></p><ul><li>加上html标签，会变成此标签格式</li></ul><h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><p><img src="/JavaScript.assets/image-20220310112909115.png" alt="image-20220310112909115"></p><h3 id="val"><a href="#val" class="headerlink" title="val"></a>val</h3><p><img src="/JavaScript.assets/image-20220310113027717.png" alt="image-20220310113027717"></p><h2 id="操作类名"><a href="#操作类名" class="headerlink" title="操作类名"></a>操作类名</h2><p><img src="/JavaScript.assets/image-20220310113045166.png" alt="image-20220310113045166"></p><p><img src="/JavaScript.assets/image-20220310113315782.png" alt="image-20220310113315782"></p><p><img src="/JavaScript.assets/image-20220310113323398.png" alt="image-20220310113323398"></p><p><img src="/JavaScript.assets/image-20220310113256641.png" alt="image-20220310113256641"></p><ul><li><h3 id="原生js绑定事件："><a href="#原生js绑定事件：" class="headerlink" title="原生js绑定事件："></a>原生js绑定事件：</h3><ul><li>获取元素 document.querySelector(‘button’)</li><li>&#x3D;&#x3D;btn.onclick函数&#x3D;&#x3D;</li><li>改变div，jquery获取元素$(“div”)</li><li><em>是否可以$(“button”).onclick&#x3D;function(){}</em></li></ul></li></ul><h2 id="操作样式"><a href="#操作样式" class="headerlink" title="操作样式"></a>操作样式</h2><p><img src="/JavaScript.assets/image-20220310113520254.png" alt="image-20220310113520254"></p><p><img src="/JavaScript.assets/image-20220310113732182.png" alt="image-20220310113732182"></p><p><img src="/JavaScript.assets/image-20220310113814236.png" alt="image-20220310113814236"></p><p><img src="/JavaScript.assets/image-20220310113843013.png" alt="image-20220310113843013"></p><h2 id="操作元素属性"><a href="#操作元素属性" class="headerlink" title="操作元素属性"></a>操作元素属性</h2><ul><li>标签里的属性：<strong>id、class、style、自定义属性</strong></li></ul><p><img src="/JavaScript.assets/image-20220310114958553.png" alt="image-20220310114958553"></p><p> <img src="/JavaScript.assets/image-20220310114050367.png" alt="image-20220310114050367"></p><p><img src="/JavaScript.assets/image-20220310114108241.png" alt="image-20220310114108241"></p><p><img src="/JavaScript.assets/image-20220310114139363.png" alt="image-20220310114139363"></p><h3 id="prop"><a href="#prop" class="headerlink" title="prop"></a>prop</h3><p><img src="/JavaScript.assets/image-20220310114325716.png" alt="image-20220310114325716"></p><p> <img src="/JavaScript.assets/image-20220310114737275.png" alt="image-20220310114737275"></p><p><img src="/JavaScript.assets/image-20220310114950344.png" alt="image-20220310114950344"></p><h3 id="prop-和attr"><a href="#prop-和attr" class="headerlink" title="prop()和attr()"></a>prop()和attr()</h3><p><strong>prop()函数的结果:</strong></p><p>   1.如果有相应的属性，返回指定属性值。</p><p>   2.如果没有相应的属性，返回值是<strong>空字符串</strong>。</p><p><strong>attr()函数的结果:</strong></p><p>   1.如果有相应的属性，返回指定属性值。</p><p>   2.如果没有相应的属性，返回值是 <strong>undefined</strong>。</p><p><strong>原生固有属性：prop</strong></p><p><strong>自定义属性：attr</strong></p><h2 id="获取元素尺寸"><a href="#获取元素尺寸" class="headerlink" title="获取元素尺寸"></a>获取元素尺寸</h2><p><img src="/JavaScript.assets/image-20220310115040640.png" alt="image-20220310115040640"></p><p><img src="/JavaScript.assets/image-20220310115144518.png" alt="image-20220310115144518"></p><p><img src="/JavaScript.assets/image-20220310115155877.png" alt="image-20220310115155877"></p><p><img src="/JavaScript.assets/image-20220310115337915.png" alt="image-20220310115337915"></p><p><img src="/JavaScript.assets/image-20220310115424032.png" alt="image-20220310115424032"></p><p><img src="/JavaScript.assets/image-20220310115418210.png" alt="image-20220310115418210"></p><p><img src="/JavaScript.assets/image-20220310115445440.png" alt="image-20220310115445440"></p><h2 id="操作元素偏移量"><a href="#操作元素偏移量" class="headerlink" title="操作元素偏移量"></a>操作元素偏移量</h2><p><img src="/JavaScript.assets/image-20220310151347217.png" alt="image-20220310151347217"></p><p><img src="/JavaScript.assets/image-20220310150959979.png" alt="image-20220310150959979"></p><p>p也写上overflow：hidden</p><p><img src="/JavaScript.assets/image-20220310151023635.png" alt="image-20220310151023635"></p><p>  <img src="/JavaScript.assets/image-20220310151103762.png" alt="image-20220310151103762"></p><p><img src="/JavaScript.assets/image-20220310151124648.png" alt="image-20220310151124648"></p><p><img src="/JavaScript.assets/image-20220310151322289.png" alt="image-20220310151322289"></p><h2 id="事件绑定"><a href="#事件绑定" class="headerlink" title="事件绑定"></a>事件绑定</h2><p><img src="/JavaScript.assets/image-20220310151352334.png" alt="image-20220310151352334"></p><p><img src="/JavaScript.assets/image-20220310151455201.png" alt="image-20220310151455201"></p><p> <img src="/JavaScript.assets/image-20220310151535434.png" alt="image-20220310151535434"></p><p><img src="/JavaScript.assets/image-20220310151654038.png" alt="image-20220310151654038"></p><p><img src="/JavaScript.assets/image-20220310151749795.png" alt="image-20220310151749795"></p><ul><li>对<strong>一个元素绑定多个事件</strong></li></ul><p><img src="/JavaScript.assets/image-20220310152041394.png" alt="image-20220310152041394"></p><p><img src="/JavaScript.assets/image-20220310152108934.png" alt="image-20220310152108934"></p><p><img src="/JavaScript.assets/image-20220310152145597.png" alt="image-20220310152145597"></p><p><img src="/JavaScript.assets/image-20220310152221996.png" alt="image-20220310152221996"></p><h2 id="事件解绑和触发"><a href="#事件解绑和触发" class="headerlink" title="事件解绑和触发"></a>事件解绑和触发</h2><p><img src="/JavaScript.assets/image-20220310152815499.png" alt="image-20220310152815499"></p><p><img src="/JavaScript.assets/image-20220310152927558.png" alt="image-20220310152927558"></p><p><img src="/JavaScript.assets/image-20220310153000324.png" alt="image-20220310153000324"></p><ul><li>直接用click绑定的事件需要<strong>点击触发</strong></li><li>off<strong>解绑</strong>，丢掉这个函数</li><li>trigger在设置的条件下<strong>自动触发</strong></li></ul><h2 id="基本动画函数"><a href="#基本动画函数" class="headerlink" title="基本动画函数"></a>基本动画函数</h2><p><img src="/JavaScript.assets/image-20220310153221381.png" alt="image-20220310153221381"></p><p><img src="/JavaScript.assets/image-20220310153256038.png" alt="image-20220310153256038"></p><p><img src="/JavaScript.assets/image-20220310153318699.png" alt="image-20220310153318699"></p><p><img src="/JavaScript.assets/image-20220310153501426.png" alt="image-20220310153501426"></p><ul><li>show显示</li><li>hide隐藏</li><li>toggle切换</li></ul><h2 id="折叠动画"><a href="#折叠动画" class="headerlink" title="折叠动画"></a>折叠动画</h2><p><img src="/JavaScript.assets/image-20220310153535672.png" alt="image-20220310153535672"></p><p><img src="/JavaScript.assets/image-20220310153554607.png" alt="image-20220310153554607"></p><p><img src="/JavaScript.assets/image-20220310153623532.png" alt="image-20220310153623532"></p><h2 id="渐隐渐显动画函数"><a href="#渐隐渐显动画函数" class="headerlink" title="渐隐渐显动画函数"></a>渐隐渐显动画函数</h2><p><img src="/JavaScript.assets/image-20220310153638744.png" alt="image-20220310153638744"></p><p><img src="/JavaScript.assets/image-20220310153651480.png" alt="image-20220310153651480"></p><p><img src="/JavaScript.assets/image-20220310153715579.png" alt="image-20220310153715579"></p><p><img src="/JavaScript.assets/image-20220310153736446.png" alt="image-20220310153736446"></p><p><img src="/JavaScript.assets/image-20220310153758805.png" alt="image-20220310153758805"></p><h2 id="综合动画"><a href="#综合动画" class="headerlink" title="综合动画"></a>综合动画</h2><p><img src="/JavaScript.assets/image-20220310153852546.png" alt="image-20220310153852546"></p><p><img src="/JavaScript.assets/image-20220310153905153.png" alt="image-20220310153905153"></p><p><img src="/JavaScript.assets/image-20220310154033754.png" alt="image-20220310154033754"></p><p><img src="/JavaScript.assets/image-20220310154049883.png" alt="image-20220310154049883"></p><p><img src="/JavaScript.assets/image-20220310154103206.png" alt="image-20220310154103206"></p><p><img src="/JavaScript.assets/image-20220310154123310.png" alt="image-20220310154123310"></p><p><img src="/JavaScript.assets/image-20220310154136003.png" alt="image-20220310154136003"></p><ul><li>先给一个绝对定位</li><li>animate函数使变圆，宽高变化</li></ul><h2 id="运动结束动画函数"><a href="#运动结束动画函数" class="headerlink" title="运动结束动画函数"></a>运动结束动画函数</h2><p><img src="/JavaScript.assets/image-20220310154230943.png" alt="image-20220310154230943"></p><p><img src="/JavaScript.assets/image-20220310154303734.png" alt="image-20220310154303734"></p><p><img src="/JavaScript.assets/image-20220310155758526.png" alt="image-20220310155758526"></p><p><img src="/JavaScript.assets/image-20220310155814806.png" alt="image-20220310155814806"></p><p><img src="/JavaScript.assets/image-20220310155852020.png" alt="image-20220310155852020"></p><p> <img src="/JavaScript.assets/image-20220310155922356.png" alt="image-20220310155922356"></p><p><img src="/JavaScript.assets/image-20220310155948692.png" alt="image-20220310155948692"></p><h2 id="ajax请求"><a href="#ajax请求" class="headerlink" title="ajax请求"></a>ajax请求</h2><p><img src="/JavaScript.assets/image-20220310160002827.png" alt="image-20220310160002827"></p><p><img src="/JavaScript.assets/image-20220310160142038.png" alt="image-20220310160142038"></p><p><img src="/JavaScript.assets/image-20220310160228631.png" alt="image-20220310160228631"></p><p><img src="/JavaScript.assets/image-20220310160405921.png" alt="image-20220310160405921"></p><h1 id="jquery"><a href="#jquery" class="headerlink" title="jquery"></a>jquery</h1><p><a href="https://www.bilibili.com/video/BV1Sy4y1C7ha?p=359&amp;spm_id_from=pageDriver">https://www.bilibili.com/video/BV1Sy4y1C7ha?p=359&amp;spm_id_from=pageDriver</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/JavaScript.assets/image-20220310161021343.png" alt="image-20220310161021343"></p><p><img src="/JavaScript.assets/image-20220310161133211.png" alt="image-20220310161133211"></p><p><img src="/JavaScript.assets/image-20220310161137824.png" alt="image-20220310161137824"></p><h2 id="入口函数"><a href="#入口函数" class="headerlink" title="入口函数"></a>入口函数</h2><p><img src="/JavaScript.assets/image-20220310162044495.png" alt="image-20220310162044495"></p><p><img src="/JavaScript.assets/image-20220310162056685.png" alt="image-20220310162056685"></p><h2 id="DOM对象和jquery对象"><a href="#DOM对象和jquery对象" class="headerlink" title="DOM对象和jquery对象"></a>DOM对象和jquery对象</h2><p><img src="/JavaScript.assets/image-20220310163049669.png" alt="image-20220310163049669"></p><p><img src="/JavaScript.assets/image-20220310163127825.png" alt="image-20220310163127825"></p><p><img src="/JavaScript.assets/image-20220310163230839.png" alt="image-20220310163230839"></p><h4 id="特别注意第三条"><a href="#特别注意第三条" class="headerlink" title="特别注意第三条"></a>特别注意第三条</h4><ul><li>jquery和dom对象<strong>不能使用对方的属性和方法</strong></li><li>jquery只能用jquery的方法<ul><li>**$(‘div’).hide() **    &#x3D;&#x3D;隐藏&#x3D;&#x3D;</li></ul></li><li>dom对象只能使用js方法<ul><li><strong>myDiv.style.display&#x3D;’none’</strong>  隐藏</li></ul></li></ul><h2 id="jquery和dom对象"><a href="#jquery和dom对象" class="headerlink" title="jquery和dom对象"></a>jquery和dom对象</h2><ul><li><p>有的dom方法jquery没有，需要<strong>jquery转换成dom</strong>才能用</p><ul><li>如：视频的play方法</li><li><img src="/JavaScript.assets/image-20220310163819374.png" alt="image-20220310163819374"></li></ul></li><li><p><img src="/JavaScript.assets/image-20220318155431675.png" alt="image-20220318155431675"></p></li></ul><p><img src="/JavaScript.assets/image-20220318155532747.png" alt="image-20220318155532747"></p><p><img src="/JavaScript.assets/image-20220407184555962.png" alt="image-20220407184555962"></p><h3 id="jquery转换成dom"><a href="#jquery转换成dom" class="headerlink" title="jquery转换成dom"></a><strong>jquery转换成dom</strong></h3><ul><li>由于jquery是数组形式存储</li><li>所以可以使用[index]获取数组元素</li></ul><p><img src="/JavaScript.assets/image-20220407184637336.png" alt="image-20220407184637336"></p><h2 id="jquery选择器-“”"><a href="#jquery选择器-“”" class="headerlink" title="jquery选择器  $(“”)"></a>jquery选择器  $(“”)</h2><p><img src="/JavaScript.assets/image-20220407184827904.png" alt="image-20220407184827904"></p><p><img src="/JavaScript.assets/image-20220407185853432.png" alt="image-20220407185853432"></p><blockquote><p>:&gt; 只有儿子</p><p>:空格  孙子</p></blockquote><h3 id="jquery隐式迭代"><a href="#jquery隐式迭代" class="headerlink" title="jquery隐式迭代"></a>jquery隐式迭代</h3><h3 id="设置样式"><a href="#设置样式" class="headerlink" title="设置样式"></a>设置样式</h3><p>.css(‘backgroudn’,’pink’)</p><p><img src="/JavaScript.assets/image-20220407190731110.png" alt="image-20220407190731110"></p><h3 id="筛选选择器"><a href="#筛选选择器" class="headerlink" title="筛选选择器"></a>筛选选择器</h3><p><img src="/JavaScript.assets/image-20220407191202536.png" alt="image-20220407191202536"></p><h3 id="筛选方法-children-“li”"><a href="#筛选方法-children-“li”" class="headerlink" title="筛选方法 children(“li”)"></a>筛选方法 children(“li”)</h3><p><img src="/JavaScript.assets/image-20220407193924235.png" alt="image-20220407193924235"></p><p>parent：亲爸爸</p><p>children：亲儿子 ul&gt;li</p><p>find：后代  ul li</p><h2 id="新浪下拉菜单"><a href="#新浪下拉菜单" class="headerlink" title="新浪下拉菜单"></a>新浪下拉菜单</h2><p><img src="/JavaScript.assets/image-20220407194606234.png" alt="image-20220407194606234"></p><ul><li>ul下4个li</li><li>每个里下一个a，三个ul</li></ul><p><img src="/JavaScript.assets/image-20220407200126494.png" alt="image-20220407200126494"></p><p><strong>给每个元素都绑定了事件</strong>，<strong>同样的函数功能</strong></p><p>（不需要for循环了）</p><p><img src="/JavaScript.assets/image-20220407200211576.png" alt="image-20220407200211576"></p><h3 id="兄弟等其他筛选方法-siblings-‘li’"><a href="#兄弟等其他筛选方法-siblings-‘li’" class="headerlink" title="兄弟等其他筛选方法 siblings(‘li’)"></a>兄弟等其他筛选方法 siblings(‘li’)</h3><p><img src="/JavaScript.assets/image-20220407200841597.png" alt="image-20220407200841597"></p><p><img src="/JavaScript.assets/image-20220407200958621.png" alt="image-20220407200958621"></p><h3 id="排他"><a href="#排他" class="headerlink" title="排他"></a>排他</h3><p><img src="/JavaScript.assets/image-20220407202133455.png" alt="image-20220407202133455"></p><h2 id="淘宝精品服饰"><a href="#淘宝精品服饰" class="headerlink" title="淘宝精品服饰"></a>淘宝精品服饰</h2><p><img src="/JavaScript.assets/image-20220407203027170.png" alt="image-20220407203027170"></p><p> <img src="/JavaScript.assets/image-20220407203112252.png" alt="image-20220407203112252"></p><p><strong>得到某个li的索引号：$(this).index()</strong></p><ul><li>每个li添加mouseover函数</li><li>第几个li就显示#content下的第几个div</li><li>其他的div隐藏</li></ul><p><img src="/JavaScript.assets/image-20220407204706791.png" alt="image-20220407204706791"></p><h3 id="链式编程"><a href="#链式编程" class="headerlink" title="链式编程"></a>链式编程</h3><p><img src="/JavaScript.assets/image-20220407205406952.png" alt="image-20220407205406952"></p><h3 id="jquery修改样式css方法"><a href="#jquery修改样式css方法" class="headerlink" title="jquery修改样式css方法"></a>jquery修改样式css方法</h3><h4 id="原始：-css-‘属性’：’value‘"><a href="#原始：-css-‘属性’：’value‘" class="headerlink" title="原始：.css(‘属性’：’value‘)"></a>原始：.css(‘属性’：’value‘)</h4><p><img src="/JavaScript.assets/image-20220407205810081.png" alt="image-20220407205810081"></p><h4 id="类名操作-添加、移除、切换类（有较多样式时）"><a href="#类名操作-添加、移除、切换类（有较多样式时）" class="headerlink" title="类名操作 添加、移除、切换类（有较多样式时）"></a>类名操作 添加、移除、切换类（有较多样式时）</h4><p><img src="/JavaScript.assets/image-20220407210017792.png" alt="image-20220407210017792"></p><p><strong>&#x3D;&#x3D;类名不需要.&#x3D;&#x3D;</strong></p><h3 id="tab栏切换"><a href="#tab栏切换" class="headerlink" title="tab栏切换"></a>tab栏切换</h3><p><img src="/JavaScript.assets/image-20220407210634058.png" alt="image-20220407210634058"></p><ul><li>每个li都绑定over事件<ul><li>获取li的index</li><li>当前li增加类，兄弟们移除类</li><li>.item显示第index个</li></ul></li></ul><h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><img src="/JavaScript.assets/image-20220407212334541.png" alt="image-20220407212334541"></p><p><img src="/JavaScript.assets/image-20220407212643130.png" alt="image-20220407212643130"></p><h3 id="hover：鼠标经过和离开的组合"><a href="#hover：鼠标经过和离开的组合" class="headerlink" title="hover：鼠标经过和离开的组合"></a>hover：鼠标经过和离开的组合</h3><p><img src="/JavaScript.assets/image-20220407213329202.png" alt="image-20220407213329202"></p><p><strong>如果只有一个函数，鼠标经过和离开都触发那一个</strong></p><p><img src="/JavaScript.assets/image-20220407213712751.png" alt="image-20220407213712751"></p><p>&#x3D;&#x3D;使用toggle&#x3D;&#x3D;</p><h3 id="动画队列-停止排队-stop"><a href="#动画队列-停止排队-stop" class="headerlink" title="动画队列 停止排队  stop()"></a>动画队列 停止排队  stop()</h3><p><strong>&#x3D;&#x3D;必须写在动画前面&#x3D;&#x3D;</strong></p><p><img src="/JavaScript.assets/image-20220407213904248.png" alt="image-20220407213904248"></p><h2 id="王者荣耀手风琴"><a href="#王者荣耀手风琴" class="headerlink" title="王者荣耀手风琴"></a>王者荣耀手风琴</h2><p><img src="/JavaScript.assets/image-20220407222403273.png" alt="image-20220407222403273"></p><p><img src="/JavaScript.assets/image-20220407222416080.png" alt="image-20220407222416080"></p><h3 id="操作属性值"><a href="#操作属性值" class="headerlink" title="操作属性值"></a>操作属性值</h3><p><strong>固有属性</strong>：prop   （&#x3D;&#x3D;<strong>checked、href、value、title等</strong>&#x3D;&#x3D;）</p><hr><p><img src="/JavaScript.assets/image-20220407222702198.png" alt="image-20220407222702198"></p><p><strong>自定属性值：</strong> attr()  </p><p><img src="/JavaScript.assets/image-20220407222944791.png" alt="image-20220407222944791"></p><h1 id="Restful-api"><a href="#Restful-api" class="headerlink" title="Restful api"></a>Restful api</h1><p><img src="/JavaScript.assets/image-20220319140952730.png" alt="image-20220319140952730"></p><p><img src="/JavaScript.assets/image-20220319142932692.png" alt="image-20220319142932692"></p><p><img src="/JavaScript.assets/image-20220319151425878.png" alt="image-20220319151425878"></p><ul><li>&#x3D;&#x3D;每一个<strong>资源</strong>都变成一个<strong>地址</strong>&#x3D;&#x3D;</li><li>对于资源可能的操作：<ul><li>增删改查</li><li>对每一个url都会有不同的谓词GET&#x2F;POST&#x2F;PUT（整体更新）&#x2F;DELETE</li></ul></li></ul><h2 id="jsonserver中每一个对象都是一i个地址"><a href="#jsonserver中每一个对象都是一i个地址" class="headerlink" title="jsonserver中每一个对象都是一i个地址"></a><strong>jsonserver中每一个对象都是一i个地址</strong></h2><p><img src="/JavaScript.assets/image-20220319152022107.png" alt="image-20220319152022107"></p><h1 id="nodejs"><a href="#nodejs" class="headerlink" title="nodejs"></a>nodejs</h1><p><img src="/JavaScript.assets/image-20220319153057266.png" alt="image-20220319153057266"></p><ul><li>response返回数据，request请求</li><li>最后要listen</li></ul><h2 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h2><p><img src="/JavaScript.assets/image-20220319153156729.png" alt="image-20220319153156729"></p><p><img src="/JavaScript.assets/image-20220319153421049.png" alt="image-20220319153421049"></p><h2 id="jquery常用API"><a href="#jquery常用API" class="headerlink" title="jquery常用API"></a>jquery常用API</h2><p><img src="/JavaScript.assets/image-20220310164324058.png" alt="image-20220310164324058"></p><p><img src="/JavaScript.assets/image-20220310165031071.png" alt="image-20220310165031071"></p><ul><li>ul里面的li：”ul li”</li></ul><p><img src="/JavaScript.assets/image-20220310165014101.png" alt="image-20220310165014101"></p><h2 id="jquery修改样式"><a href="#jquery修改样式" class="headerlink" title="jquery修改样式"></a>jquery修改样式</h2><p><img src="/JavaScript.assets/image-20220310165233931.png" alt="image-20220310165233931"></p><p><img src="/JavaScript.assets/image-20220310165252746.png" alt="image-20220310165252746"></p><h2 id="隐式迭代"><a href="#隐式迭代" class="headerlink" title="隐式迭代"></a>隐式迭代</h2><p><img src="/JavaScript.assets/image-20220310165425079.png" alt="image-20220310165425079"></p><p><img src="/JavaScript.assets/image-20220310165405826.png" alt="image-20220310165405826"></p><h2 id="筛选器-1"><a href="#筛选器-1" class="headerlink" title="筛选器"></a>筛选器</h2><p><img src="/JavaScript.assets/image-20220318155701873.png" alt="image-20220318155701873"></p><p><img src="/JavaScript.assets/image-20220310165441757.png" alt="image-20220310165441757"></p><p><img src="/JavaScript.assets/image-20220310165456848.png" alt="image-20220310165456848"></p><p><img src="/JavaScript.assets/image-20220318155823396.png" alt="image-20220318155823396"></p><p> <img src="/JavaScript.assets/image-20220318160343932.png" alt="image-20220318160343932"></p><ul><li>：input和input打印出来的不一样，前者包含所有button、select、textarea</li><li>后者只包含input</li></ul><h2 id="创建元素-添加元素"><a href="#创建元素-添加元素" class="headerlink" title="创建元素+添加元素"></a>创建元素+添加元素</h2><p><img src="/JavaScript.assets/image-20220318161010581.png" alt="image-20220318161010581"></p><ul><li>$(‘&lt;&gt;xxxx&lt;&gt;’)   &#x3D;&#x3D;$加上标签&lt;&gt;即可&#x3D;&#x3D;</li></ul><p><img src="/JavaScript.assets/image-20220318161739525.png" alt="image-20220318161739525"></p><ul><li>没有$()，直接&#x3D;“&lt;&gt;xxx&lt;&gt;”</li></ul><p><img src="/JavaScript.assets/image-20220318161111820.png" alt="image-20220318161111820"></p><ul><li>append、prepend</li></ul><p><img src="/JavaScript.assets/image-20220318161608204.png" alt="image-20220318161608204"></p><p><img src="/JavaScript.assets/image-20220318161625573.png" alt="image-20220318161625573"></p><p> <img src="/JavaScript.assets/image-20220318162417434.png" alt="image-20220318162417434"></p><p><img src="/JavaScript.assets/image-20220318162532517.png" alt="image-20220318162532517"></p><h2 id="ready加载事件"><a href="#ready加载事件" class="headerlink" title="ready加载事件"></a>ready加载事件</h2><p><img src="/JavaScript.assets/image-20220318165551373.png" alt="image-20220318165551373"></p><p><img src="/JavaScript.assets/image-20220318170211061.png" alt="image-20220318170211061"></p><ul><li>类似js中的load事件，当dom写在了script下面时，等加载完dom再执行script</li></ul><h1 id="JS实战项目"><a href="#JS实战项目" class="headerlink" title="JS实战项目"></a>JS实战项目</h1><h2 id="通讯录"><a href="#通讯录" class="headerlink" title="通讯录"></a>通讯录</h2><p><img src="/JavaScript.assets/image-20220312190513390.png" alt="image-20220312190513390"></p><p> <img src="/JavaScript.assets/image-20220312190905278.png" alt="image-20220312190905278"></p><p><img src="/JavaScript.assets/image-20220312192513897.png" alt="image-20220312192513897"></p><p><img src="/JavaScript.assets/image-20220312192455662.png" alt="image-20220312192455662"></p><h2 id="Document-querySelectorAll"><a href="#Document-querySelectorAll" class="headerlink" title="Document.querySelectorAll"></a>Document.querySelectorAll</h2><p>返回与指定的选择器组匹配的文档中的<strong>元素列表</strong></p><p>匹配指定 CSS 选择器的<strong>所有元素</strong></p><h3 id="获取匹配列表"><a href="#获取匹配列表" class="headerlink" title="获取匹配列表"></a><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document/querySelectorAll#%E8%8E%B7%E5%8F%96%E5%8C%B9%E9%85%8D%E5%88%97%E8%A1%A8">获取匹配列表</a></h3><ul><li>要获取文档中所有p元素：</li></ul><p>var matches &#x3D; <strong>document.querySelectorAll(“p”);</strong></p><ul><li>获取所有div元素：</li></ul><p>var matches&#x3D;<strong>document.querySelectorAll(“div”)</strong></p><ul><li>获取所有类名包含note、alert的div元素：</li></ul><p>var matches&#x3D;<strong>document.querySelectorAll(“div.note,div.alert”)</strong></p><ul><li>获取元素下的元素</li></ul><p>var container &#x3D; <strong>document.querySelector</strong>(“#test”); 容器元素</p><p>var matches &#x3D; <strong>container.querySelectorAll</strong>(“div.highlighted &gt; p”); <u>容器</u>里面的类名为highlighted的<u>div</u>的<u>p标签</u></p><h3 id="访问匹配项"><a href="#访问匹配项" class="headerlink" title="访问匹配项"></a><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Document/querySelectorAll#%E8%AE%BF%E9%97%AE%E5%8C%B9%E9%85%8D%E9%A1%B9">访问匹配项</a></h3><ul><li><p>一旦返回匹配元素的<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/NodeList"><code>NodeList</code></a>，就可以像任何数组一样检查它</p></li><li><p>只需使用<strong>标准数组方法</strong>来<strong>访问列表的内容</strong>(使用任何<strong>常见的循环语句</strong>)</p></li><li><p>可以使用 NodeList 对象的 <a href="https://www.runoob.com/jsref/prop-nodelist-length.html">length</a> 属性来获取匹配选择器的元素属性,然后遍历</p></li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs javascript"> <br><span class="hljs-comment">// 设置第一个 &lt;p&gt; 元素的背景颜色</span><br>x[<span class="hljs-number">0</span>].<span class="hljs-property">style</span>.<span class="hljs-property">backgroundColor</span> = <span class="hljs-string">&quot;red&quot;</span>;<br><br><span class="hljs-comment">// 获取文档中所有 class=&quot;example&quot; 的 &lt;p&gt; 元素</span><br><span class="hljs-keyword">var</span> x = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">querySelectorAll</span>(<span class="hljs-string">&quot;p.example&quot;</span>); <br> <br><span class="hljs-comment">// 设置 class=&quot;example&quot; 的第一个 &lt;p&gt; 元素的背景颜色</span><br>x[<span class="hljs-number">0</span>].<span class="hljs-property">style</span>.<span class="hljs-property">backgroundColor</span> = <span class="hljs-string">&quot;red&quot;</span>;<br><br><span class="hljs-comment">//计算文档中 class=&quot;example&quot; 的 &lt;p&gt; 元素的数量（使用 NodeList 对象的 length 属性）:</span><br><span class="hljs-keyword">var</span> x = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">querySelectorAll</span>(<span class="hljs-string">&quot;.example&quot;</span>).<span class="hljs-property">length</span>;<br><br><br><span class="hljs-keyword">var</span> x = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">querySelectorAll</span>(<span class="hljs-string">&quot;p&quot;</span>);<br><span class="hljs-keyword">var</span> i;<br><span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; x.<span class="hljs-property">length</span>; i++) &#123;<br>    x[i].<span class="hljs-property">style</span>.<span class="hljs-property">backgroundColor</span> = <span class="hljs-string">&quot;red&quot;</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>querySelectorAll（）括号里可以写：</p><ul><li><p>标签 p,div,li,ul,h3 </p><ul><li><p>&#96;&#96;&#96;<br>给文档中所有的 <h2>, <div> 和 <span> 元素设置背景颜色：</p><p>var x &#x3D; document.querySelectorAll(“h2, div, span”);<br>var i;<br>for (i &#x3D; 0; i &lt; x.length; i++) {<br>x[i].style.backgroundColor &#x3D; “red”;<br>}</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs dart"><br>- 类名 .name .example<br><br>  - ```<br>    <span class="hljs-comment">//设置文档中所有 class=&quot;example&quot; 元素的背景颜色:</span><br>    <span class="hljs-keyword">var</span> x = <span class="hljs-built_in">document</span>.<span class="hljs-built_in">querySelectorAll</span>(<span class="hljs-string">&quot;.example&quot;</span>);<br>    <span class="hljs-keyword">var</span> i;<br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; x.length; i++) &#123; <span class="hljs-comment">//全部遍历</span><br>        x[i].style.backgroundColor = <span class="hljs-string">&quot;red&quot;</span>;<br>    &#125;<br>    <br>    <span class="hljs-comment">//计算文档中 class=&quot;example&quot; 的 &lt;p&gt; 元素的数量（使用 NodeList 对象的 length 属性）:</span><br>    <span class="hljs-keyword">var</span> x = <span class="hljs-built_in">document</span>.<span class="hljs-built_in">querySelectorAll</span>(<span class="hljs-string">&quot;.example&quot;</span>).length;<br></code></pre></td></tr></table></figure></li></ul></li><li><p>id名 #name #input</p></li><li><p>混合 </p><ul><li><p>p.example  (所有 class&#x3D;”example” 的 <p> 元素)</p><ul><li>&#96;&#96;&#96;<br>&#x2F;&#x2F; 获取文档中所有 class&#x3D;”example” 的 <p> 元素<br>var x &#x3D; document.querySelectorAll(“p.example”); <figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><br>- <span class="hljs-keyword">div</span> &gt; p (<span class="hljs-keyword">div</span>下面的p元素，<span class="hljs-keyword">div</span>为父)<br><br>  - ```<br>    <span class="hljs-built_in">var</span> x = document.querySelectorAll(<span class="hljs-string">&quot;div &gt; p&quot;</span>);<br>    <span class="hljs-built_in">var</span> i;<br>    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; x.length; i++) &#123;<br>        x[i].style.backgroundColor = <span class="hljs-string">&quot;red&quot;</span>;<br>    &#125;<br></code></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>某属性 a[target] （a是标签名，target是属性名）</p><ul><li>&#96;&#96;&#96;<br>var x &#x3D; document.querySelectorAll(“a[target]”);<br>var i;<br>for (i &#x3D; 0; i &lt; x.length; i++) {<br>x[i].style.border &#x3D; “10px solid red”;<br>}<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs clean"><br>    <br><br>## ul添加li节点<br><br>- 获取ul节点<br>  - const ul=document.querySelector(<span class="hljs-string">&#x27;ul&#x27;</span>)<br>- 创建li节点<br>  -   const li=document.createElement(<span class="hljs-string">&#x27;li&#x27;</span>)<br>- li节点添加类名（规范样式）<br>  -   li.className=<span class="hljs-string">&#x27;collection-item&#x27;</span><br>- li添加文本节点，并赋值<br>  -  li.appendChild(document.createTextNode(inputtask))<br>  -  inputtask由#input.value获得<br>  -  此步已经获得&lt;li&gt;xxx&lt;/li&gt; 包含内容的li标签<br>- ul添加li<br>  -   ul.appendChild(li)<br><br>**小结**：<br><br><span class="hljs-number">1.</span> 创建元素<br><span class="hljs-number">2.</span> （添加类）<br><span class="hljs-number">3.</span> 添加文本<br><span class="hljs-number">4.</span> 父元素添加子元素<br><br><br><br>### 输入框清除内容<br><br>  <span class="hljs-comment">// 显示了内容后清除输入框中的内容</span><br><br>  <span class="hljs-comment">// inputbox.innerHTML=&#x27;&#x27;</span><br><br>  inputbox.==value===<span class="hljs-string">&#x27;&#x27;</span><br><br>- innerhtml是文本内容，input中的内容是value<br><br><br><br><br>### 删除某个元素 remove()<br><br>元素结构：ul-&gt;li-&gt;a-&gt;i<br><br>  <span class="hljs-comment">// 删除i的父a的父li</span><br><br>function removeTask(e)&#123; <span class="hljs-comment">//e是删除按钮</span><br><br>  e.target.parentElement.parentElement.**remove()**<br><br>&#125;<br><br>---<br><br>ul.innerHTML=<span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-comment">//只剩下ul标签</span><br><br>![image<span class="hljs-number">-20220330143558908</span>](JavaScript.assets/image<span class="hljs-number">-20220330143558908.</span>png)<br><br>清空innerhtml，相当于去掉了li、a等<br><br><br><br><br><br>### 删除孩子元素 removeChild()<br><br> while(ul.firstChild)&#123;<br><br>​    ul.**removeChild**(ul.firstChild)<br><br> &#125;<br><br><br><br>### 匹配，显示对的，隐藏错误的<br><br>```javascript<br>  for(<span class="hljs-keyword">let</span> i=<span class="hljs-number">0</span>;i&lt;lis.length;i++)&#123;<br>      const li_value=lis[i].textContent<br>      <span class="hljs-keyword">if</span>(li_value.toLowerCase().indexOf(filtercontent)!=<span class="hljs-number">-1</span>)&#123;<br>        lis[i].style.display=<span class="hljs-string">&quot;block&quot;</span><br>      &#125;else&#123;<br>        lis[i].style.display=<span class="hljs-string">&quot;none&quot;</span><br>      &#125;<br></code></pre></td></tr></table></figure></li></ul></li></ul><p>ifelse对每一个显示隐藏即可，不需要隐藏其他全部，显示这一个（此方法使得多个匹配上的不能共存）</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">// 如果找得到某个li下有</span><br><span class="hljs-comment">// if(li_value.indexOf(filtercontent)!=-1)&#123;</span><br><span class="hljs-comment">//   for(let j=0;j&lt;lis.length;j++)&#123;</span><br><span class="hljs-comment">//     // 遍历其他li并隐藏</span><br><span class="hljs-comment">//     lis[j].style.display=&#x27;none&#x27;</span><br><span class="hljs-comment">//     // lis[j].style.visibility=&#x27;hidden&#x27;</span><br><span class="hljs-comment">//   &#125;</span><br><span class="hljs-comment">//   // 此li显示，其他li隐藏</span><br><span class="hljs-comment">//   lis[i].style.display=&quot;block&quot;</span><br><span class="hljs-comment">//   // lis[i].style.visibility=&quot;visible&quot;</span><br><span class="hljs-comment">// &#125;</span><br></code></pre></td></tr></table></figure><p>👆永远只会显示一个</p><h3 id="获取li的文字内容"><a href="#获取li的文字内容" class="headerlink" title="获取li的文字内容"></a>获取li的文字内容</h3><p>lis[i].<strong>&#x3D;&#x3D;textContent&#x3D;&#x3D;</strong></p><p>.value不行（input）</p><p>.innerhtml包含tag</p><h3 id="缩略图"><a href="#缩略图" class="headerlink" title="缩略图"></a>缩略图</h3><ul><li>点击第234个li没有反应的原因：<ul><li><img src="/JavaScript.assets/image-20220331100326845.png" alt="image-20220331100326845"></li><li>类名没有写  “.”</li></ul></li></ul><h3 id="事件监听的两种方式"><a href="#事件监听的两种方式" class="headerlink" title="事件监听的两种方式"></a>事件监听的两种方式</h3><h4 id="加载所有"><a href="#加载所有" class="headerlink" title="加载所有"></a>加载所有</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">// 加载所有事件监听</span><br><span class="hljs-title function_">loadEventListeners</span>()<br><span class="hljs-comment">// 所有事件监听函数</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">loadEventListeners</span>(<span class="hljs-params"></span>)&#123;<br>   <span class="hljs-comment">// 给每个li添加点击函数</span><br>   imgs.<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;click&#x27;</span>,imgClick)<br>&#125;<br><span class="hljs-comment">// img点击函数</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">imgClick</span>(<span class="hljs-params">e</span>)&#123;<br>    <br>&#125;<br></code></pre></td></tr></table></figure><ul><li>当事件较多时【过滤删除任务，添加任务、删除任务、过滤任务】</li></ul><h4 id="给某个元素直接添加监听函数"><a href="#给某个元素直接添加监听函数" class="headerlink" title="给某个元素直接添加监听函数"></a>给某个元素直接添加监听函数</h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">// imgs的添加函数</span><br>imgs.<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;click&#x27;</span>,imgClick)<br><span class="hljs-comment">// img点击函数</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">imgClick</span>(<span class="hljs-params">e</span>)&#123;<br>  <span class="hljs-comment">// 每次点击时，先遍历全部重置缩略图不透明度为1</span><br>  img.<span class="hljs-title function_">forEach</span>(<span class="hljs-function"><span class="hljs-params">img</span>=&gt;</span>(img.<span class="hljs-property">style</span>.<span class="hljs-property">opacity</span>=<span class="hljs-number">1</span>))<br><br>  <span class="hljs-comment">// 改变大图</span><br>  <span class="hljs-comment">// 大图的src=目标图的src</span><br>  current.<span class="hljs-property">src</span>=e.<span class="hljs-property">target</span>.<span class="hljs-property">src</span><br><br>  <span class="hljs-comment">// 每次点击时，将某个img不透明度设置为0.6</span><br>  e.<span class="hljs-property">target</span>.<span class="hljs-property">style</span>.<span class="hljs-property">opacity</span>=<span class="hljs-number">0.6</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>给img容器添加click函数，click函数传参e，使用e.target获得具体某一个img</li></ul><h4 id="点击a时a变，其他都恢复，点击b时其他都恢复"><a href="#点击a时a变，其他都恢复，点击b时其他都恢复" class="headerlink" title="点击a时a变，其他都恢复，点击b时其他都恢复"></a>点击a时a变，其他都恢复，点击b时其他都恢复</h4><ul><li><p>在img<strong>点击函数中先遍历全部归零</strong></p><ul><li>&#96;&#96;&#96;javascript<br>&#x2F;&#x2F; 每次点击时，先遍历全部重置缩略图不透明度为1<br>img.forEach(img&#x3D;&gt;(img.style.opacity&#x3D;1))<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs awk"><br>    <br><br>- 再给某一个img设置xxx为xxx<br><br>  - ```javascript<br>    <span class="hljs-regexp">//</span> 改变大图<br>    <span class="hljs-regexp">//</span> 大图的src=目标图的src<br>    current.src=e.target.src<br>    <br>    <span class="hljs-regexp">//</span> 每次点击时，将某个img不透明度设置为<span class="hljs-number">0.6</span><br>    e.target.style.opacity=<span class="hljs-number">0.6</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h4 id="样式上学到的："><a href="#样式上学到的：" class="headerlink" title="样式上学到的："></a>样式上学到的：</h4><ul><li>网格布局<ul><li>display:grid</li><li>grid-template-columns:1fr 1fr;</li></ul></li><li>margin:0 auto：加给容器<img src="/JavaScript.assets/image-20220331103818108.png" alt="image-20220331103818108"></li></ul><h3 id="函数学到的"><a href="#函数学到的" class="headerlink" title="函数学到的"></a>函数学到的</h3><ul><li>给img的<strong>容器imgs添加点击函数</strong>，传参e，通过&#x3D;&#x3D;e.target&#x3D;&#x3D;获取某一个img</li></ul><p><img src="/JavaScript.assets/image-20220331104109192.png" alt="image-20220331104109192"></p><p><img src="/JavaScript.assets/image-20220331104146305.png" alt="image-20220331104146305"></p><ul><li><p>改变图片url：</p><ul><li><p>&#x2F;&#x2F; 大图的src&#x3D;目标图的src</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">current.<span class="hljs-property">src</span>=e.<span class="hljs-property">target</span>.<span class="hljs-property">src</span><br></code></pre></td></tr></table></figure><ul><li>**&#x3D;&#x3D;直接.src&#x3D;&#x3D;**，不许style.src</li></ul></li></ul></li><li><p>设置透明度</p><ul><li><p>&#x2F;&#x2F; 每次点击时，将<strong>某个img</strong>不透明度设置为0.6</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">e.<span class="hljs-property">target</span>.<span class="hljs-property">style</span>.<span class="hljs-property">opacity</span>=<span class="hljs-number">0.6</span><br></code></pre></td></tr></table></figure><ul><li><strong>&#x3D;&#x3D;style.opcity&#x3D;xxx&#x3D;&#x3D;</strong></li></ul></li></ul></li><li><p>添加某一种效果</p><ul><li>添加类名</li><li><strong>&#x3D;&#x3D;xx.classList.add(‘xxx’)&#x3D;&#x3D;</strong></li><li><strong>&#x3D;&#x3D;xx.classname&#x3D;’xxx xxx’  &#x2F;&#x2F;需要写全部&#x3D;&#x3D;</strong></li></ul></li><li><p>点击其他时效果清除</p><ul><li><p>设置定时器移除类名</p></li><li><p>&#96;&#96;&#96;javascript<br>&#x2F;&#x2F; 大图加上动画效果<br>  current.classList.add(‘fade-in’)<br>  &#x2F;&#x2F; 设置定时器移除类名<br>  setTimeout(()&#x3D;&gt;{current.classList.remove(‘fade-in’),500})</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><br><br><span class="hljs-section">## 图片切换【轮播效果】</span><br><br><span class="hljs-bullet">-</span> <span class="hljs-strong">**$&#123;&#125;**</span>只是个<span class="hljs-strong">**插入变量**</span>的符号<br><span class="hljs-bullet">-</span> es6新增的模板字符串中<span class="hljs-strong">**嵌入变量**</span><br><br><span class="hljs-section">### $&#123;&#125;替换变量</span><br><br>支持$&#123;&#125;替换的字符串不是用引号的，是用`符号，就是键盘上1键左边的那个<br><br><span class="hljs-code">```javascript</span><br><span class="hljs-code">var c = `你好， $&#123;a&#125; ，$&#123;b&#125;！`;</span><br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="两种字符串-变量的拼接方法"><a href="#两种字符串-变量的拼接方法" class="headerlink" title="两种字符串+变量的拼接方法"></a>两种字符串+变量的拼接方法</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json">img.setAttribute(&#x27;src&#x27;<span class="hljs-punctuation">,</span>`./images/$<span class="hljs-punctuation">&#123;</span>current<span class="hljs-punctuation">&#125;</span>.jpg`)<br>img.setAttribute(&#x27;src&#x27;<span class="hljs-punctuation">,</span>&#x27;images/&#x27;+current+&#x27;.jpg&#x27;)<br></code></pre></td></tr></table></figure><h3 id="判断最后一张"><a href="#判断最后一张" class="headerlink" title="判断最后一张"></a>判断最后一张</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">if</span>(current==max)&#123;<br>    <span class="hljs-comment">// 已经到最后一张，赋值第一章</span><br>    current=min;<br>  &#125;<span class="hljs-keyword">else</span>&#123;<br>    current++;<br>  &#125;<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;注意双等号！！！不是&#x3D;max&#x3D;&#x3D;</p><h2 id="显示隐藏【切换效果】"><a href="#显示隐藏【切换效果】" class="headerlink" title="显示隐藏【切换效果】"></a>显示隐藏【切换效果】</h2><h3 id="根据按钮上的文字判断状态："><a href="#根据按钮上的文字判断状态：" class="headerlink" title="根据按钮上的文字判断状态："></a>根据按钮上的文字判断状态：</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs js">btn.<span class="hljs-property">onclick</span>=<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;<br>  <span class="hljs-keyword">if</span>(btn.<span class="hljs-property">innerHTML</span>==<span class="hljs-string">&#x27;隐藏&#x27;</span>)&#123;<br>    btn.<span class="hljs-property">innerHTML</span>=<span class="hljs-string">&#x27;显示&#x27;</span><br>    img.<span class="hljs-property">style</span>.<span class="hljs-property">visibility</span>=<span class="hljs-string">&quot;hidden&quot;</span><br>  &#125;<span class="hljs-keyword">else</span>&#123;<br>    btn.<span class="hljs-property">innerHTML</span>=<span class="hljs-string">&#x27;隐藏&#x27;</span><br>    img.<span class="hljs-property">style</span>.<span class="hljs-property">visibility</span>=<span class="hljs-string">&#x27;visible&#x27;</span><br>  &#125;<br>&#125; <br></code></pre></td></tr></table></figure><h3 id="根据bool判断状态"><a href="#根据bool判断状态" class="headerlink" title="根据bool判断状态"></a>根据bool判断状态</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> isShow=<span class="hljs-literal">true</span><br>btn.<span class="hljs-property">onclick</span>=<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;<br>  isShow=!isShow<br>  <span class="hljs-keyword">if</span>(isShow==<span class="hljs-literal">false</span>)&#123;<br>    btn.<span class="hljs-property">innerHTML</span>=<span class="hljs-string">&#x27;显示&#x27;</span><br>    img.<span class="hljs-property">style</span>.<span class="hljs-property">visibility</span>=<span class="hljs-string">&quot;hidden&quot;</span><br>  &#125;<span class="hljs-keyword">else</span>&#123;<br>    btn.<span class="hljs-property">innerHTML</span>=<span class="hljs-string">&#x27;隐藏&#x27;</span><br>    img.<span class="hljs-property">style</span>.<span class="hljs-property">visibility</span>=<span class="hljs-string">&#x27;visible&#x27;</span><br>  &#125;<br>&#125; <br></code></pre></td></tr></table></figure><h2 id="关闭小广告"><a href="#关闭小广告" class="headerlink" title="关闭小广告"></a>关闭小广告</h2><p><img src="/JavaScript.assets/image-20220402114319510.png" alt="image-20220402114319510"></p><ul><li>大盒子：relative</li><li>左右小盒子：absolute<ul><li>左：absolute+left&#x3D;0</li><li>👉：absolute+right&#x3D;0</li></ul></li></ul><h2 id="JavaScript-中的相等性判断"><a href="#JavaScript-中的相等性判断" class="headerlink" title="JavaScript 中的相等性判断"></a>JavaScript 中的相等性判断</h2><h3 id="严格相等"><a href="#严格相等" class="headerlink" title="严格相等 ==="></a><a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Equality_comparisons_and_sameness#%E4%B8%A5%E6%A0%BC%E7%9B%B8%E7%AD%89">严格相等 <code>===</code></a></h3><p>如果两个被比较的值<strong>具有不同的类型</strong>，这两个值是<strong>不全等</strong>的</p><p>如果两个被比较的值<strong>类型相同，值也相同</strong>，并且<strong>都不是 number 类型</strong>时，两个值全等</p><p>如果两个值<strong>都是 number 类型</strong>，当两个<strong>都不是 NaN</strong>，并且<strong>数值相同</strong>，或是两个值分别为 <strong>+0 和 -0</strong> 时，两个值被认为是全等的</p><h2 id="js显示"><a href="#js显示" class="headerlink" title="js显示"></a>js显示</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">startTime</span>(<span class="hljs-params"></span>)&#123;<br>  <span class="hljs-keyword">var</span> today=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>()<br>  <span class="hljs-keyword">var</span> h=today.<span class="hljs-title function_">getHours</span>()<br>  <span class="hljs-keyword">var</span> m=today.<span class="hljs-title function_">getMinutes</span>()<br>  <span class="hljs-keyword">var</span> s=today.<span class="hljs-title function_">getSeconds</span>()<br>  m=<span class="hljs-title function_">checkTime</span>(m)<br>  s=<span class="hljs-title function_">checkTime</span>(s)<br>  <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;txt&#x27;</span>).<span class="hljs-property">innerHTML</span>=h+<span class="hljs-string">&#x27;:&#x27;</span>+m+<span class="hljs-string">&#x27;:&#x27;</span>+s<br>  t=<span class="hljs-built_in">setTimeout</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;<span class="hljs-title function_">startTime</span>()&#125;,<span class="hljs-number">500</span>)<br>&#125;<br><span class="hljs-keyword">function</span> <span class="hljs-title function_">checkTime</span>(<span class="hljs-params">i</span>)&#123;<br>  <span class="hljs-keyword">if</span>(i&lt;<span class="hljs-number">10</span>)&#123;<br>    i=<span class="hljs-string">&#x27;0&#x27;</span>+i<br>  &#125;<br>  <span class="hljs-keyword">return</span> i<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="动态生成表格"><a href="#动态生成表格" class="headerlink" title="动态生成表格"></a>动态生成表格</h2><p><img src="/JavaScript.assets/image-20220406154402321.png" alt="image-20220406154402321"></p><p><img src="/JavaScript.assets/image-20220406154406291.png" alt="image-20220406154406291"></p><p><img src="/JavaScript.assets/image-20220406154444009.png" alt="image-20220406154444009"></p><p><img src="/JavaScript.assets/image-20220406154632679.png" alt="image-20220406154632679"></p><p><img src="/JavaScript.assets/image-20220406155402011.png" alt="image-20220406155402011"></p><p><img src="/JavaScript.assets/image-20220406155508792.png" alt="image-20220406155508792"></p><p><img src="/JavaScript.assets/image-20220406155606842.png" alt="image-20220406155606842"></p><p>遍历对象：</p><p>for (key in obj)  </p><p>obj[key]得到属性值</p><p>key是属性</p><p><img src="/JavaScript.assets/image-20220406155804259.png" alt="image-20220406155804259"></p><p><img src="/JavaScript.assets/image-20220406155808851.png" alt="image-20220406155808851"></p><p><img src="/JavaScript.assets/image-20220406160146215.png" alt="image-20220406160146215"></p><p>删除：removeChild，需要找到父元素</p><p>当前a标签的父td的父tr的父tbody</p><p>tbody删除child（a的父td的父tr）</p><h2 id="动态创建元素"><a href="#动态创建元素" class="headerlink" title="动态创建元素"></a>动态创建元素</h2><p><img src="/JavaScript.assets/image-20220406160326933.png" alt="image-20220406160326933"></p><ol><li>document.write<ol><li>创建了一个div等标签</li></ol></li><li>innerHTML<br>1. </li><li>createElement</li></ol><p><img src="/JavaScript.assets/image-20220406191145017.png" alt="image-20220406191145017"></p><p><img src="/JavaScript.assets/image-20220406191852174.png" alt="image-20220406191852174"></p><h2 id="动态增加元素"><a href="#动态增加元素" class="headerlink" title="动态增加元素"></a>动态增加元素</h2><p><img src="/JavaScript.assets/image-20220406192737653.png" alt="image-20220406192737653"></p><ol><li>appendChild</li><li>insertBefore</li></ol><h2 id="动态删除元素"><a href="#动态删除元素" class="headerlink" title="动态删除元素"></a>动态删除元素</h2><ol><li>removeChild</li></ol><h2 id="动态修改元素"><a href="#动态修改元素" class="headerlink" title="动态修改元素"></a>动态修改元素</h2><p><img src="/JavaScript.assets/image-20220406192928074.png" alt="image-20220406192928074"></p><ol><li>修改属性<ol><li>直接获得<strong>src\href\title</strong></li><li><strong>自定义属性</strong>：<ol><li>setAttributes设置</li><li>getAttribute得到</li><li>removeAttribute移除</li></ol></li></ol></li><li>修改<strong>内容</strong><ol><li>innerHTML</li><li>innerText</li></ol></li><li>修改<strong>表单</strong><ol><li>.value</li><li>.type</li><li>.disabled</li></ol></li><li>修改<strong>样式</strong><ol><li>style.</li><li>className&#x3D;’’</li></ol></li></ol><table><thead><tr><th>操作</th><th>js</th><th>jquery</th></tr></thead><tbody><tr><td>修改属性</td><td>.src&#x3D;&#x2F;.href&#x3D;&#x2F;.checked&#x3D;&#x2F;.disabled&#x3D;&#x2F;.setgetAttributes(‘’,’’)[自定义]</td><td>.prop()&#x2F;.attr()&#x2F;.removeprop()&#x2F;.removeattr()[自定义]</td></tr><tr><td>修改<strong>内容</strong></td><td>.innerHTML&#x3D;&#x2F;.innerText&#x3D;</td><td>.text()&#x2F;.val()&#x2F;.html()</td></tr><tr><td>修改<strong>表单</strong></td><td>.value&#x3D;&#x2F;</td><td></td></tr><tr><td>修改<strong>样式</strong></td><td>.style.display&#x3D;&#x2F;.className&#x3D;’’</td><td>$().css()&#x2F;$().addClass()&#x2F;.removeClass()</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h2 id="事件操作"><a href="#事件操作" class="headerlink" title="事件操作"></a>事件操作</h2><ul><li>onclick</li><li>onfocus</li><li>onblur</li><li>onmouseup</li><li>onmousedown</li></ul><h2 id="dom总结"><a href="#dom总结" class="headerlink" title="dom总结"></a>dom总结</h2><p><img src="/JavaScript.assets/image-20220406194036756.png" alt="image-20220406194036756"></p><h2 id="事件高级"><a href="#事件高级" class="headerlink" title="事件高级"></a>事件高级</h2><h3 id="传统注册事件方式"><a href="#传统注册事件方式" class="headerlink" title="传统注册事件方式"></a>传统注册事件方式</h3><p><img src="/JavaScript.assets/image-20220406194227777.png" alt="image-20220406194227777"></p><ul><li>在html中：<ul><li>&lt;button onclick&#x2F;其他事件类型&gt;</li></ul></li><li>在js中：<ul><li>xxx.onclick&#x3D;function(){}</li></ul></li></ul><p> 【唯一性】：</p><ul><li>一个xxx只能有一个onclick事件，后面的会覆盖前面旧的</li><li>只会执行一次</li></ul><h3 id="方法监听注册方式"><a href="#方法监听注册方式" class="headerlink" title="方法监听注册方式"></a>方法监听注册方式</h3><p><img src="/JavaScript.assets/image-20220406194554026.png" alt="image-20220406194554026"></p><p><img src="/JavaScript.assets/image-20220406194656534.png" alt="image-20220406194656534"></p><p><img src="/JavaScript.assets/image-20220406194749554.png" alt="image-20220406194749554"></p><ul><li>给同一个元素，同一个click事件，<strong>添加多个function</strong></li><li><img src="/JavaScript.assets/image-20220406195254038.png" alt="image-20220406195254038"></li></ul><h2 id="删除事件"><a href="#删除事件" class="headerlink" title="删除事件"></a>删除事件</h2><p>解绑一个事件：</p><p><img src="/JavaScript.assets/image-20220406195917969.png" alt="image-20220406195917969"></p><p><img src="/JavaScript.assets/image-20220406200015343.png" alt="image-20220406200015343"></p><ul><li>eventTarget.onclick&#x3D;null</li><li>eventTarget.removeEventListener()</li></ul><p><img src="/JavaScript.assets/image-20220406200153193.png" alt="image-20220406200153193"></p><p><img src="/JavaScript.assets/image-20220406200328942.png" alt="image-20220406200328942"></p><h2 id="事件对象-event"><a href="#事件对象-event" class="headerlink" title="事件对象 event"></a>事件对象 event</h2><p><img src="/JavaScript.assets/image-20220406201315580.png" alt="image-20220406201315580"></p><p>输出一个mouseEvent</p><ul><li>包含type、&#x3D;&#x3D;target（触发的事件源）&#x3D;&#x3D;</li><li>可以自己命名为e、evt</li></ul><p><img src="/JavaScript.assets/image-20220406201440006.png" alt="image-20220406201440006"></p><p><img src="/JavaScript.assets/image-20220406201607040.png" alt="image-20220406201607040"></p><h2 id="常见事件对象的属性和方法"><a href="#常见事件对象的属性和方法" class="headerlink" title="常见事件对象的属性和方法"></a>常见事件对象的属性和方法</h2><p><img src="/JavaScript.assets/image-20220406201740903.png" alt="image-20220406201740903"></p><p><img src="/JavaScript.assets/image-20220406202028825.png" alt="image-20220406202028825"></p><h3 id="this和e-target的区别"><a href="#this和e-target的区别" class="headerlink" title="this和e.target的区别"></a>this和e.target的区别</h3><ul><li>target是触发的（被点击的）</li><li>this是绑定的<ul><li>绑定ul，点击了li，会触发，target返回li，this返回ul</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220406202847047.png" alt="image-20220406202847047"></p><h2 id="禁止复制文字"><a href="#禁止复制文字" class="headerlink" title="禁止复制文字"></a>禁止复制文字</h2><h3 id="禁止鼠标右键-contextmenu-e-preventDefault"><a href="#禁止鼠标右键-contextmenu-e-preventDefault" class="headerlink" title="禁止鼠标右键 contextmenu e.preventDefault()"></a>禁止鼠标右键 contextmenu e.preventDefault()</h3><p><img src="/JavaScript.assets/image-20220406202959345.png" alt="image-20220406202959345"></p><p><img src="/JavaScript.assets/image-20220406203017034.png" alt="image-20220406203017034"></p><h3 id="禁止鼠标选中-selectstart"><a href="#禁止鼠标选中-selectstart" class="headerlink" title="禁止鼠标选中 selectstart"></a>禁止鼠标选中 selectstart</h3><p><img src="/JavaScript.assets/image-20220406203117108.png" alt="image-20220406203117108"></p><h2 id="常用鼠标事件"><a href="#常用鼠标事件" class="headerlink" title="常用鼠标事件"></a>常用鼠标事件</h2><p><img src="/JavaScript.assets/image-20220406205628919.png" alt="image-20220406205628919"></p><ul><li>event.clientX</li><li>event.cpageX</li></ul><p>获得鼠标点击位置距离最上方的距离</p><h2 id="跟随鼠标的天使"><a href="#跟随鼠标的天使" class="headerlink" title="跟随鼠标的天使"></a>跟随鼠标的天使</h2><ul><li>鼠标移动事件：mousemove</li><li>页面中的事件：事件源document</li><li>图片要到处移动：<strong>绝对定位</strong></li><li>核心：每次鼠标移动，都会获得最新的鼠标坐标，把这个x和y坐标作为图片的<strong>top和left值</strong>可以移动图片</li><li></li></ul><h2 id="常用键盘事件"><a href="#常用键盘事件" class="headerlink" title="常用键盘事件"></a>常用键盘事件</h2><h3 id=""><a href="#" class="headerlink" title=""></a><img src="/JavaScript.assets/image-20220406211456717.png" alt="image-20220406211456717"></h3><ul><li><p>onkeydown：</p><ul><li>按下时触发</li><li>可以识别快捷键</li></ul></li><li><p>onkeypress：</p><ul><li>按下时触发</li><li>识别不了功能键，退格时，不会知道是在keypress</li></ul></li><li><p><strong>&#x3D;&#x3D;三个事件的执行顺序：keydown-keypress-keyup&#x3D;&#x3D;</strong></p></li><li><p><img src="/JavaScript.assets/image-20220406211648483.png" alt="image-20220406211648483"></p></li></ul><h2 id="快捷键事件"><a href="#快捷键事件" class="headerlink" title="快捷键事件"></a>快捷键事件</h2><p><img src="/JavaScript.assets/image-20220406212042680.png" alt="image-20220406212042680"></p><p><img src="/JavaScript.assets/image-20220406212155257.png" alt="image-20220406212155257"></p><h2 id="京东输入案例"><a href="#京东输入案例" class="headerlink" title="京东输入案例"></a>京东输入案例</h2><p><img src="/JavaScript.assets/image-20220407002432487.png" alt="image-20220407002432487"></p><p><img src="/JavaScript.assets/image-20220407002537197.png" alt="image-20220407002537197"></p><p><img src="/JavaScript.assets/image-20220407090320565.png" alt="image-20220407090320565"></p><ul><li>keyup事件，传入参数e事件</li><li>判断e.keyCode是否&#x3D;&#x3D;83</li><li>输入框聚焦方法：.focus（）</li></ul><h2 id="京东快递单号查询"><a href="#京东快递单号查询" class="headerlink" title="京东快递单号查询"></a>京东快递单号查询</h2><p><img src="/JavaScript.assets/image-20220407090531550.png" alt="image-20220407090531550"></p><p><img src="/JavaScript.assets/image-20220407090743524.png" alt="image-20220407090743524"></p><p>【小三角】</p><p><img src="/JavaScript.assets/image-20220407093055471.png" alt="image-20220407093055471"></p><p>注意是blur 不是onblur </p><p><img src="/JavaScript.assets/image-20220407093221342.png" alt="image-20220407093221342"></p><h2 id="BOM概述"><a href="#BOM概述" class="headerlink" title="BOM概述"></a>BOM概述</h2><p><img src="/JavaScript.assets/image-20220407093416550.png" alt="image-20220407093416550"></p><p><img src="/JavaScript.assets/image-20220407093500842.png" alt="image-20220407093500842"></p><ul><li>BOM&gt;DOM</li></ul><p><img src="/JavaScript.assets/image-20220407093605085.png" alt="image-20220407093605085"></p><p><img src="/JavaScript.assets/image-20220407093609896.png" alt="image-20220407093609896"></p><p><img src="/JavaScript.assets/image-20220407094027288.png" alt="image-20220407094027288"></p><p><strong>定义变量时不要用name！</strong></p><h2 id="window对象常见事件"><a href="#window对象常见事件" class="headerlink" title="window对象常见事件"></a>window对象常见事件</h2><h3 id="窗口加载事件-window-onload"><a href="#窗口加载事件-window-onload" class="headerlink" title="窗口加载事件 window.onload"></a>窗口加载事件 window.onload</h3><p><img src="/JavaScript.assets/image-20220407095236478.png" alt="image-20220407095236478"></p><ul><li>当script写在了html前面时，但是需要先等html完全加载完</li></ul><p><img src="/JavaScript.assets/image-20220407101311779.png" alt="image-20220407101311779"></p><h3 id="DOMContentLoaded"><a href="#DOMContentLoaded" class="headerlink" title="DOMContentLoaded"></a>DOMContentLoaded</h3><p><img src="/JavaScript.assets/image-20220407101502023.png" alt="image-20220407101502023"></p><ul><li>不需要等图片、样式表加载完</li><li>dom加载完就行</li><li>比load（加载完全部元素） 更快</li></ul><h2 id="窗口大小事件resize、onresize"><a href="#窗口大小事件resize、onresize" class="headerlink" title="窗口大小事件resize、onresize"></a>窗口大小事件resize、onresize</h2><p><img src="/JavaScript.assets/image-20220407101833906.png" alt="image-20220407101833906"></p><p><strong>窗口大小：</strong></p><ul><li>innerWidth</li><li>innerHeight</li></ul><p><img src="/JavaScript.assets/image-20220407102512659.png" alt="image-20220407102512659"></p><h2 id="定时器setTimeout"><a href="#定时器setTimeout" class="headerlink" title="定时器setTimeout()"></a>定时器setTimeout()</h2><p><img src="/JavaScript.assets/image-20220407102538756.png" alt="image-20220407102538756"></p><h3 id="setTimeout-function-time"><a href="#setTimeout-function-time" class="headerlink" title="setTimeout(function(),time)"></a>setTimeout(function(),time)</h3><p><img src="/JavaScript.assets/image-20220407102603436.png" alt="image-20220407102603436"></p><p><img src="/JavaScript.assets/image-20220407103909207.png" alt="image-20220407103909207"></p><p><img src="/JavaScript.assets/image-20220407104005810.png" alt="image-20220407104005810"></p><p><img src="/JavaScript.assets/image-20220407104120796.png" alt="image-20220407104120796"></p><p><img src="/JavaScript.assets/image-20220407104221219.png" alt="image-20220407104221219"></p><h3 id="回调函数："><a href="#回调函数：" class="headerlink" title="回调函数："></a>回调函数：</h3><p>时间到了才执行</p><p><img src="/JavaScript.assets/image-20220407104246726.png" alt="image-20220407104246726"></p><h2 id="自动关闭广告"><a href="#自动关闭广告" class="headerlink" title="自动关闭广告"></a>自动关闭广告</h2><p><img src="/JavaScript.assets/image-20220407104523301.png" alt="image-20220407104523301"></p><h2 id="停止定时器clearTimeout"><a href="#停止定时器clearTimeout" class="headerlink" title="停止定时器clearTimeout"></a>停止定时器clearTimeout</h2><p><img src="/JavaScript.assets/image-20220407104948176.png" alt="image-20220407104948176"></p><p><img src="/JavaScript.assets/image-20220407105455608.png" alt="image-20220407105455608"></p><h2 id="setInterval-多次调用"><a href="#setInterval-多次调用" class="headerlink" title="setInterval 多次调用"></a>setInterval 多次调用</h2><p><img src="/JavaScript.assets/image-20220407110459902.png" alt="image-20220407110459902"></p><p><img src="/JavaScript.assets/image-20220407110641860.png" alt="image-20220407110641860"></p><p>使用<strong>setInterval</strong></p><h2 id="倒计时效果"><a href="#倒计时效果" class="headerlink" title="倒计时效果"></a>倒计时效果</h2><p><img src="/JavaScript.assets/image-20220407111633965.png" alt="image-20220407111633965"></p><p><img src="/JavaScript.assets/image-20220407114026247.png" alt="image-20220407114026247"></p><p><img src="/JavaScript.assets/image-20220407114246191.png" alt="image-20220407114246191"></p><p><img src="/JavaScript.assets/image-20220407115241969.png" alt="image-20220407115241969"></p><h2 id="停止计时器"><a href="#停止计时器" class="headerlink" title="停止计时器"></a>停止计时器</h2><p><img src="/JavaScript.assets/image-20220407115611949.png" alt="image-20220407115611949"></p><p>重点：</p><ul><li>在function内部声明的var <strong>只在function内部有效</strong></li><li>如果想在其他function内使用↑，应在所有func外面声明<strong>var xxx&#x3D;&#x3D;&#x3D;null&#x3D;&#x3D;</strong></li><li>在func1中：xxx&#x3D;setInterval(…)</li></ul><h2 id="案例：发送短信"><a href="#案例：发送短信" class="headerlink" title="案例：发送短信"></a>案例：发送短信</h2><p><img src="/JavaScript.assets/image-20220407135753366.png" alt="image-20220407135753366"></p><ul><li>输入框输入信息</li><li>点击发送后<ul><li>按钮变灰，不可点</li><li>设置3s后 恢复，可以点击  文本随时间变化</li></ul></li></ul><p><img src="/JavaScript.assets/image-20220407141443299.png" alt="image-20220407141443299"></p><p><img src="/JavaScript.assets/image-20220407142107609.png" alt="image-20220407142107609"></p><h2 id="this指向问题"><a href="#this指向问题" class="headerlink" title="this指向问题"></a>this指向问题</h2><p>谁调用函数，只想谁</p><p><img src="/JavaScript.assets/image-20220407143925071.png" alt="image-20220407143925071"></p><p><img src="/JavaScript.assets/image-20220407143933966.png" alt="image-20220407143933966"></p><h2 id="js执行机制"><a href="#js执行机制" class="headerlink" title="js执行机制"></a>js执行机制</h2><p><img src="/JavaScript.assets/image-20220407144059941.png" alt="image-20220407144059941"></p><p><img src="/JavaScript.assets/image-20220407144256603.png" alt="image-20220407144256603"></p><p><strong>允许多个任务同时进行</strong></p><p>打印顺序：123（因为3耗时太长了）</p><h2 id="异步任务"><a href="#异步任务" class="headerlink" title="异步任务"></a>异步任务</h2><p><img src="/JavaScript.assets/image-20220407144600009.png" alt="image-20220407144600009"></p><p><img src="/JavaScript.assets/image-20220407144719574.png" alt="image-20220407144719574"></p><p><strong>先执行完同步任务，再执行异步任务</strong></p><p>顺序：1\2\settimeout</p><h2 id="location对象"><a href="#location对象" class="headerlink" title="location对象"></a>location对象</h2><p><img src="/JavaScript.assets/image-20220407145152533.png" alt="image-20220407145152533"></p><p><img src="/JavaScript.assets/image-20220407145229148.png" alt="image-20220407145229148"></p><p><img src="/JavaScript.assets/image-20220407150153992.png" alt="image-20220407150153992"></p><ul><li>location.<strong>href</strong>  整个url</li><li>location.host</li><li>location**.search**  参数</li></ul><h3 id="跳转页面-location-href"><a href="#跳转页面-location-href" class="headerlink" title="跳转页面  location.href"></a>跳转页面  location.href</h3><p><img src="/JavaScript.assets/image-20220407150511990.png" alt="image-20220407150511990"></p><h2 id="自动跳转页面"><a href="#自动跳转页面" class="headerlink" title="自动跳转页面"></a>自动跳转页面</h2><p><img src="/JavaScript.assets/image-20220407151645030.png" alt="image-20220407151645030"></p><p><img src="/JavaScript.assets/image-20220407151649502.png" alt="image-20220407151649502"></p><h2 id="获取url参数"><a href="#获取url参数" class="headerlink" title="获取url参数"></a>获取url参数</h2><p><img src="/JavaScript.assets/image-20220407180403219.png" alt="image-20220407180403219"></p><p><img src="/JavaScript.assets/image-20220407180451978.png" alt="image-20220407180451978"></p><ul><li>提交表单后，参数会提交到新页面</li><li>在新页面：获取当前url的参数：location.search()</li></ul><p><img src="/JavaScript.assets/image-20220407180615369.png" alt="image-20220407180615369"></p><p><img src="/JavaScript.assets/image-20220407181323175.png" alt="image-20220407181323175"></p><h2 id="284-location常见方法"><a href="#284-location常见方法" class="headerlink" title="284 location常见方法"></a>284 location常见方法</h2>]]></content>
    
    
    
    <tags>
      
      <tag>前端</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络-李沐</title>
    <link href="/2022/04/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90/"/>
    <url>/2022/04/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90/</url>
    
    <content type="html"><![CDATA[<h1 id="图神经网络-李沐"><a href="#图神经网络-李沐" class="headerlink" title="图神经网络-李沐"></a>图神经网络-李沐</h1><p>信息：attribute</p><p>embedding：向量</p><p>无向图：朋友</p><p>有向图：粉丝</p><hr><p>图片-》图：</p><p>每个像素：顶点</p><p>像素与像素之间：边</p><p>邻接矩阵：像素与像素之间的关系（是否邻接）</p><hr><p>文本-》有向图：</p><p>词：顶点</p><p>词到词：边</p><hr><h2 id="图层面："><a href="#图层面：" class="headerlink" title="图层面："></a>图层面：</h2><p>图中是否包含某个结构</p><h2 id="节点层："><a href="#节点层：" class="headerlink" title="节点层："></a>节点层：</h2><p>节点分类</p><h2 id="边级："><a href="#边级：" class="headerlink" title="边级："></a>边级：</h2><p>人物之间的关系</p><h2 id="图包含四种信息"><a href="#图包含四种信息" class="headerlink" title="图包含四种信息"></a>图包含四种信息</h2><p>向量：</p><ul><li><p>顶点</p></li><li><p>边</p></li><li><p>图</p></li><li><p>连接性：</p><ul><li>邻接矩阵：太稀疏</li><li></li></ul></li></ul><p><img src="/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90.assets/image-20220418212519633.png" alt="image-20220418212519633"></p><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><ul><li>信息传递</li><li>graph in graph out</li><li>改变向量，不改变连接性（边链接哪些顶点）</li><li><img src="/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90.assets/image-20220418212735321.png" alt="image-20220418212735321"><ul><li>全局、顶点、边<strong>向量</strong>各自对应一个<strong>MLP</strong>，输出新的属性，进行更新</li><li>图结构没有发生变化</li></ul></li></ul><h3 id="1-预测顶点"><a href="#1-预测顶点" class="headerlink" title="1 预测顶点"></a>1 预测顶点</h3><ul><li>二分类</li><li>输出图后，顶点进入全连接层，得到输出</li><li>对顶点做分类</li><li>只有<strong>一个全连接层</strong>，所有顶点共享一个全连接层里面的参数</li></ul><p><img src="/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90.assets/image-20220418214329420.png" alt="image-20220418214329420"></p><ul><li>每个属性进入自己的MLP</li></ul><h3 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h3><p><img src="/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%8E%E6%B2%90.assets/image-20220418214458657.png" alt="image-20220418214458657"></p><ul><li><p>进入MLP之前，汇聚点和它的2个邻居节点</p></li><li><p>如果是图片：</p><ul><li>汇聚过程类似卷积</li><li>但是“卷积核”权重都为1了</li></ul></li><li></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二叉树</title>
    <link href="/2022/04/21/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <url>/2022/04/21/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h1><p>[TOC]</p><h2 id="二叉树的种类"><a href="#二叉树的种类" class="headerlink" title="二叉树的种类"></a>二叉树的种类</h2><ul><li><h3 id="满二叉树"><a href="#满二叉树" class="headerlink" title="满二叉树"></a>满二叉树</h3><ul><li>只有度为0的结点和度为2的结点(2个孩子、叶子)</li><li>度为0的结点（叶子）在同一层上（最后一层）<img src="/../../themes/fluid/source/img/content/image-20220304211324489.png" alt="image-20220304211324489"></li><li></li><li>深度为k（一共4层）</li><li>2^k-1个节点（2+4+8+16…等比数列）</li></ul></li><li><h3 id="完全二叉树"><a href="#完全二叉树" class="headerlink" title="完全二叉树"></a>完全二叉树</h3><ul><li>除了<strong>最底层节点</strong>可能没填满外，其余每层节点数都达到最大值</li><li>最下面一层的节点都集中在该层最左边</li></ul></li></ul><p><img src="/../../themes/fluid/source/img/content/image-20220304211722019.png" alt="image-20220304211722019"></p><h3 id="队列-堆-二叉树"><a href="#队列-堆-二叉树" class="headerlink" title="队列 堆 二叉树"></a>队列 堆 二叉树</h3><blockquote><ul><li>优先级队列&#x3D;堆&#x3D;完全二叉树</li><li>保证父子节点的顺序关系（？？？）</li></ul></blockquote><ul><li><h3 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h3><ul><li><strong>二叉搜索树是一个有序树&#x2F;二叉排序树</strong></li><li>左边&lt;中间根&lt;右边</li><li><img src="../../themes/fluid/source/img/content/image-20220304212048526.png" alt="image-20220304212048526" style="zoom:100%;" /></li></ul></li><li><h3 id="平衡二叉搜索树"><a href="#平衡二叉搜索树" class="headerlink" title="平衡二叉搜索树"></a>平衡二叉搜索树</h3><ul><li><p>AVL（Adelson-Velsky and Landis）树</p></li><li><p>是一棵空树</p></li><li><p>它的左右两个子树的高度差的绝对值不超过1<img src="/../../themes/fluid/source/img/content/image-20220304212257151.png" alt="image-20220304212257151"></p></li><li><p><strong>C++中map、set、multimap，multiset的底层实现都是平衡二叉搜索树</strong></p></li><li><p>map、set的增删操作时间时间复杂度是logn</p></li></ul></li><li><h3 id="二叉树的存储方式"><a href="#二叉树的存储方式" class="headerlink" title="二叉树的存储方式"></a>二叉树的存储方式</h3><ul><li><p><strong>可以链式存储，也可以顺序存储</strong></p></li><li><h4 id="链式存储方式就用指针"><a href="#链式存储方式就用指针" class="headerlink" title="链式存储方式就用指针"></a><strong>链式存储</strong>方式就用指针</h4><ul><li>通过指针把分布在散落在各个地址的<strong>节点串联</strong>一起<img src="/../../themes/fluid/source/img/content/image-20220304212747715.png" alt="image-20220304212747715"></li></ul></li><li><h4 id="顺序存储的方式就是用数组"><a href="#顺序存储的方式就是用数组" class="headerlink" title="顺序存储的方式就是用数组"></a><strong>顺序存储</strong>的方式就是用数组</h4><ul><li>顺序存储的元素在内存是<strong>连续分布</strong>的<img src="/../../themes/fluid/source/img/content/image-20220304212801542.png" alt="image-20220304212801542"></li><li><strong>左孩子就是 i * 2 + 1，右孩子就是 i * 2 + 2</strong></li></ul></li></ul></li></ul><h2 id="二叉树的遍历方式"><a href="#二叉树的遍历方式" class="headerlink" title="二叉树的遍历方式"></a>二叉树的遍历方式</h2><ol><li>深度优先遍历：先往深走，<strong>遇到叶子节点再往回走</strong>。使用<strong>递归</strong>是比较方便的<ol><li>前序遍历：中左右</li><li>中序遍历：左中右</li><li>后序遍历：左右中</li></ol></li><li>广度优先遍历：<strong>一层一层</strong>的去遍历。用<strong>队列</strong>来实现，需要<strong>先进先出</strong>的结构，才能一层一层的来遍历二叉树</li></ol><h2 id="二叉树的定义"><a href="#二叉树的定义" class="headerlink" title="二叉树的定义"></a>二叉树的定义</h2><ul><li>值、左右指针、构造函数</li></ul><blockquote><p>链表定义：值、next指针、构造函数</p></blockquote><p>相对于链表 ，二叉树的节点里多了一个指针</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">TreeNode</span> &#123;<br>    <span class="hljs-type">int</span> val;<br>    TreeNode *left;<br>    TreeNode *right;<br>    <span class="hljs-built_in">TreeNode</span>(<span class="hljs-type">int</span> x) : <span class="hljs-built_in">val</span>(x), <span class="hljs-built_in">left</span>(<span class="hljs-literal">NULL</span>), <span class="hljs-built_in">right</span>(<span class="hljs-literal">NULL</span>) &#123;&#125;<br>&#125;;<br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TreeNode</span> &#123;<br>    <span class="hljs-type">int</span> val;<br>  TreeNode left;<br>  TreeNode right;<br>  TreeNode() &#123;&#125;<br>  TreeNode(<span class="hljs-type">int</span> val) &#123; <span class="hljs-built_in">this</span>.val = val; &#125;<br>  TreeNode(<span class="hljs-type">int</span> val, TreeNode left, TreeNode right) &#123;<br>    <span class="hljs-built_in">this</span>.val = val;<br>    <span class="hljs-built_in">this</span>.left = left;<br>    <span class="hljs-built_in">this</span>.right = right;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TreeNode</span>: <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, value</span>):<br>        self.value = value<br>        self.left = <span class="hljs-literal">None</span><br>        self.right = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><h1 id="二叉树递归遍历框架-traverse函数"><a href="#二叉树递归遍历框架-traverse函数" class="headerlink" title="二叉树递归遍历框架(traverse函数)"></a>二叉树递归遍历框架(traverse函数)</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>    <span class="hljs-keyword">if</span> (root == <span class="hljs-literal">null</span>) &#123;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">// 前序位置</span><br>    traverse(root.left);<br>    <span class="hljs-comment">// 中序位置</span><br>    traverse(root.right);<br>    <span class="hljs-comment">// 后序位置</span><br>&#125;<br><br></code></pre></td></tr></table></figure><h1 id="二叉树的递归遍历"><a href="#二叉树的递归遍历" class="headerlink" title="二叉树的递归遍历"></a>二叉树的递归遍历</h1><p><strong>如何写递归：</strong></p><ol><li><p><strong>确定递归函数的参数和返回值：</strong></p><ol><li>哪些参数是递归的过程中<u>需要处理的</u>，那么就在递归函数里加上这个参数</li><li>每次递归的<u>返回值是什么</u></li></ol></li><li><p><strong>确定终止条件：</strong></p><ol><li>写错了&#x2F;没写会导致栈溢出</li><li>（操作系统存储使用栈结构，无终止递归会使内存栈溢出）</li></ol></li><li><p><strong>确定单层递归的逻辑：</strong></p><ol><li>每一层递归<u>需要处理的信息</u></li></ol></li></ol><h1 id="二叉树的迭代遍历"><a href="#二叉树的迭代遍历" class="headerlink" title="二叉树的迭代遍历"></a>二叉树的迭代遍历</h1><p>用<strong>栈</strong>也可以是实现二叉树的前后中序遍历</p><h2 id="前序遍历（迭代法-x2F-栈处理）"><a href="#前序遍历（迭代法-x2F-栈处理）" class="headerlink" title="前序遍历（迭代法&#x2F;栈处理）"></a>前序遍历（迭代法&#x2F;栈处理）</h2><ul><li><p><strong>中右左</strong>入栈—-&gt;<strong>中左右</strong>出栈</p></li><li><p>一个栈</p></li><li><p>一个数组存放输出&#x2F;返回的结果 return</p></li><li><p>先判断是否为空树</p></li><li><p>根入栈</p></li><li><p>循环：</p><ul><li>根出栈，根放进结果数组中</li><li>如果有右节点，入栈右</li><li>如果有左节点，入栈左</li></ul></li></ul><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * public class TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode left;</span><br><span class="hljs-comment"> *     TreeNode right;</span><br><span class="hljs-comment"> *     TreeNode() &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int val) &#123; this.val = val; &#125;</span><br><span class="hljs-comment"> *     TreeNode(int val, TreeNode left, TreeNode right) &#123;</span><br><span class="hljs-comment"> *         this.val = val;</span><br><span class="hljs-comment"> *         this.left = left;</span><br><span class="hljs-comment"> *         this.right = right;</span><br><span class="hljs-comment"> *     &#125;</span><br><span class="hljs-comment"> * &#125;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">public</span> List&lt;Integer&gt; <span class="hljs-title function_">preorderTraversal</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>        <span class="hljs-comment">// Stack&lt;Integer&gt;st=new Stack&lt;Integer&gt;();//初始化栈</span><br>        Stack&lt;TreeNode&gt;st=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Stack</span>();<span class="hljs-comment">//初始化栈</span><br>        List&lt;Integer&gt;result=<span class="hljs-keyword">new</span> <span class="hljs-title class_">LinkedList</span>&lt;Integer&gt;();<span class="hljs-comment">//存放结果</span><br><br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">null</span>)<span class="hljs-keyword">return</span> result;<br>        <span class="hljs-comment">//根元素的值入栈</span><br>        st.push(root);<br>        <span class="hljs-comment">//栈不为空，就可以处理栈中元素</span><br>        <span class="hljs-keyword">while</span>(!st.isEmpty())&#123;<br>            <span class="hljs-comment">// 获取当前栈顶值（新的父节点）</span><br>            TreeNode node=st.peek();<br>            <span class="hljs-comment">//栈顶元素存入result</span><br>            result.add(node.val);<br>            <span class="hljs-comment">//处理完了弹出</span><br>            st.pop();<br>            <span class="hljs-keyword">if</span>(node.right!=<span class="hljs-literal">null</span>)st.push(node.right);<span class="hljs-comment">//右不为空，入栈</span><br>            <span class="hljs-keyword">if</span>(node.left!=<span class="hljs-literal">null</span>)st.push(node.left);<span class="hljs-comment">//左不为空，入栈</span><br>        &#125;<br>        <span class="hljs-keyword">return</span> result;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>Stack声明为Treenode类型，因为push的是treenode元素，如果声明为int，会报错</li><li>判断左右节点是否为空：<ul><li><strong>if(node.right!&#x3D;null)</strong></li><li>不能直接写：<strong>if(node.right)</strong></li></ul></li></ul><h1 id="二叉树的统一迭代法"><a href="#二叉树的统一迭代法" class="headerlink" title="二叉树的统一迭代法"></a>二叉树的统一迭代法</h1><h2 id="标记法"><a href="#标记法" class="headerlink" title="标记法"></a>标记法</h2><p>中序遍历：</p><ul><li><p><strong>无法同时解决访问节点（遍历节点）和处理节点（将元素放进结果集）不一致的情况</strong></p></li><li><p><strong>就将访问的节点放入栈中，把要&#x3D;&#x3D;处理的节点也放入栈中但是要做标记&#x3D;&#x3D;</strong></p></li></ul><blockquote><p><strong>要处理的节点放入栈之后，紧接着放入一个&#x3D;&#x3D;空指针作为标记&#x3D;&#x3D;</strong></p></blockquote><h1 id="104-二叉树的最大深度"><a href="#104-二叉树的最大深度" class="headerlink" title="104. 二叉树的最大深度"></a><a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/">104. 二叉树的最大深度</a></h1><h2 id="遍历一遍二叉树"><a href="#遍历一遍二叉树" class="headerlink" title="遍历一遍二叉树"></a>遍历一遍二叉树</h2><p>用一个<strong>外部变量</strong>记录<strong>每个节点</strong>所在的深度，取最大值</p><p>要对每个节点做什么：</p><ul><li>进入时+1</li><li>离开时-1</li></ul><p>因为到达最后叶子节点下一个（roo&#x3D;&#x3D;null)时，已经判断了depth和max，且return结束遍历，所以不会到-1，depth仍然保留</p><hr><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * public class TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode left;</span><br><span class="hljs-comment"> *     TreeNode right;</span><br><span class="hljs-comment"> *     TreeNode() &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int val) &#123; this.val = val; &#125;</span><br><span class="hljs-comment"> *     TreeNode(int val, TreeNode left, TreeNode right) &#123;</span><br><span class="hljs-comment"> *         this.val = val;</span><br><span class="hljs-comment"> *         this.left = left;</span><br><span class="hljs-comment"> *         this.right = right;</span><br><span class="hljs-comment"> *     &#125;</span><br><span class="hljs-comment"> * &#125;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-comment">// 当前深度</span><br>    <span class="hljs-type">int</span> depth=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">// 最大深度</span><br>    <span class="hljs-type">int</span> maxDepth=<span class="hljs-number">0</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxDepth</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>        traverse(root);<span class="hljs-comment">//此函数改变了maxDepth的值</span><br>        <span class="hljs-keyword">return</span> maxDepth;<br><br>    &#125;<br>    <span class="hljs-comment">// 二叉树遍历函数</span><br>    <span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(TreeNode root)</span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">null</span>)&#123;<br>            <span class="hljs-comment">// 到达叶子节点，更新最大参数</span><br>            maxDepth=Math.max(maxDepth,depth);<span class="hljs-comment">//取两个值中最大值</span><br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-comment">// 要对每一条路记录深度，到一个节点+1，离开一个节点-1</span><br>        <span class="hljs-comment">// 前序位置</span><br>        depth++;<br>        traverse(root.left);<br>        traverse(root.right);<br>        <span class="hljs-comment">// 后序位置</span><br>        depth--;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>两个全局变量</li><li>主函数：遍历根（所有），返回最大深度</li><li>二叉树遍历函数：前序位置++，后序位置–</li></ul><h2 id="c-遍历"><a href="#c-遍历" class="headerlink" title="c++遍历"></a>c++遍历</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-comment">// 子函数traverse递归</span><br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">int</span> depth=<span class="hljs-number">0</span>;<span class="hljs-comment">//当前深度</span><br>    <span class="hljs-type">int</span> maxdepth=<span class="hljs-number">0</span>;<span class="hljs-comment">//最大深度</span><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">maxDepth</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <span class="hljs-comment">// 调用traverse</span><br>        <span class="hljs-built_in">traverse</span>(root);<span class="hljs-comment">//</span><br>        <span class="hljs-keyword">return</span> maxdepth;<br>    &#125;<br>    <span class="hljs-comment">// 遍历函数</span><br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">traverse</span><span class="hljs-params">(TreeNode *root)</span></span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">nullptr</span>)&#123;<br>            maxdepth=<span class="hljs-built_in">max</span>(maxdepth,depth);<span class="hljs-comment">//跟新最大深度</span><br>            <span class="hljs-keyword">return</span>;<span class="hljs-comment">//结束</span><br>        &#125;<br>        <span class="hljs-comment">// 遍历</span><br>        depth++;<br>        <span class="hljs-built_in">traverse</span>(root-&gt;left);<br>        <span class="hljs-built_in">traverse</span>(root-&gt;right);<br>        depth--;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h2 id="分解法"><a href="#分解法" class="headerlink" title="分解法"></a>分解法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * public class TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode left;</span><br><span class="hljs-comment"> *     TreeNode right;</span><br><span class="hljs-comment"> *     TreeNode() &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int val) &#123; this.val = val; &#125;</span><br><span class="hljs-comment"> *     TreeNode(int val, TreeNode left, TreeNode right) &#123;</span><br><span class="hljs-comment"> *         this.val = val;</span><br><span class="hljs-comment"> *         this.left = left;</span><br><span class="hljs-comment"> *         this.right = right;</span><br><span class="hljs-comment"> *     &#125;</span><br><span class="hljs-comment"> * &#125;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-comment">//  分解法</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">maxDepth</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>        <span class="hljs-comment">// 要有一个遍历到叶子的终止函数</span><br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">null</span>)&#123;<span class="hljs-comment">//到叶子就返回</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        &#125;<br>        <span class="hljs-type">int</span> leftDepth=maxDepth(root.left);<br>        <span class="hljs-type">int</span> rightDepth=maxDepth(root.right);<br>        <span class="hljs-keyword">return</span> Math.max(leftDepth,rightDepth)+<span class="hljs-number">1</span>;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="144-二叉树的前序遍历"><a href="#144-二叉树的前序遍历" class="headerlink" title="144. 二叉树的前序遍历"></a><a href="https://leetcode-cn.com/problems/binary-tree-preorder-traversal/">144. 二叉树的前序遍历</a></h1><h2 id="迭代法"><a href="#迭代法" class="headerlink" title="迭代法"></a>迭代法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * public class TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode left;</span><br><span class="hljs-comment"> *     TreeNode right;</span><br><span class="hljs-comment"> *     TreeNode() &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int val) &#123; this.val = val; &#125;</span><br><span class="hljs-comment"> *     TreeNode(int val, TreeNode left, TreeNode right) &#123;</span><br><span class="hljs-comment"> *         this.val = val;</span><br><span class="hljs-comment"> *         this.left = left;</span><br><span class="hljs-comment"> *         this.right = right;</span><br><span class="hljs-comment"> *     &#125;</span><br><span class="hljs-comment"> * &#125;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-comment">// 存放结果</span><br>    List&lt;Integer&gt;list=<span class="hljs-keyword">new</span> <span class="hljs-title class_">LinkedList</span>&lt;&gt;();<br>    <span class="hljs-keyword">public</span> List&lt;Integer&gt; <span class="hljs-title function_">preorderTraversal</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>        <span class="hljs-comment">// 调用函数</span><br>        traverse(root);<span class="hljs-comment">//改变了list</span><br>        <span class="hljs-keyword">return</span> list;<br><br>    &#125;<br><br>    <span class="hljs-comment">// 二叉树遍历函数</span><br>    <span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(TreeNode root)</span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">null</span>)&#123;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        list.add(root.val);<br>        traverse(root.left);<br>        traverse(root.right);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>全局变量存放结果list</li><li>调用遍历函数改变list</li><li>主函数返回list，调用函数改变list</li><li>赋值函数<strong>在前序位置操作节点</strong>  <strong>add(root.val)</strong></li></ul><h2 id="C-迭代"><a href="#C-迭代" class="headerlink" title="C++迭代"></a>C++迭代</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-comment">// 迭代法</span><br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-comment">// result</span><br>    vector&lt;<span class="hljs-type">int</span>&gt;res;<span class="hljs-comment">//主函数中返回，子函数中赋值</span><br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">preorderTraversal</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <span class="hljs-comment">// 调用遍历函数</span><br>        <span class="hljs-built_in">traverse</span>(root);<br>        <span class="hljs-keyword">return</span> res;<br>    &#125;<br><br>    <span class="hljs-comment">// 二叉树遍历函数</span><br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">traverse</span><span class="hljs-params">(TreeNode*root)</span></span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">nullptr</span>)&#123;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-comment">// res加根</span><br>        res.<span class="hljs-built_in">push_back</span>(root-&gt;val);<br>        <span class="hljs-comment">// 遍历左子树</span><br>        <span class="hljs-built_in">traverse</span>(root-&gt;left);<br>        <span class="hljs-comment">// 遍历右子树</span><br>        <span class="hljs-built_in">traverse</span>(root-&gt;right);<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><ul><li><strong>全局存放结果</strong><ul><li>因为要在调用函数中改变，在主函数中返回</li><li><img src="/../../themes/fluid/source/img/content/image-20220420110738648.png" alt="image-20220420110738648"></li><li>如果用传参改变，每一棵树的res都是新的</li><li><strong>多次遍历共同维护一个res，所以全局变量</strong></li></ul></li><li><strong>遍历函数：</strong><ul><li>赋值操作+遍历左+遍历右</li></ul></li><li><strong>主函数：</strong><ul><li>遍历根</li><li>返回结果</li></ul></li></ul><h2 id="分解法-1"><a href="#分解法-1" class="headerlink" title="分解法"></a>分解法</h2><blockquote><p>不要用像 <code>traverse</code> 这样的辅助函数和任何外部变量，单纯用题目给的 <code>preorderTraverse</code> 函数递归解题</p></blockquote><ul><li><strong>一棵二叉树的前序遍历分解成了&#x3D;&#x3D;根节点&#x3D;&#x3D;和&#x3D;&#x3D;左右子树的前序遍历结果&#x3D;&#x3D;</strong></li></ul><p><img src="/../../themes/fluid/source/img/content/3.jpeg" alt="img"></p><ul><li>中   左   右</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//  分解法</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">public</span> List&lt;Integer&gt; <span class="hljs-title function_">preorderTraversal</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>        List&lt;Integer&gt;list=<span class="hljs-keyword">new</span> <span class="hljs-title class_">LinkedList</span>&lt;&gt;();<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">null</span>)&#123;<span class="hljs-comment">//全部遍历完/根为空</span><br>            <span class="hljs-comment">// return 0;</span><br>            <span class="hljs-keyword">return</span> list;<span class="hljs-comment">//返回已经好的列表</span><br>        &#125;<br>        <span class="hljs-comment">// 根</span><br>        list.add(root.val);<br>        <span class="hljs-comment">// 左子树的前序遍历</span><br>        list.addAll(preorderTraversal(root.left));<br>        <span class="hljs-comment">// 右子树的前序遍历</span><br>        list.addAll(preorderTraversal(root.right));<br>        <span class="hljs-keyword">return</span> list;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>结果list写在了函数里面</li><li>&#x3D;&#x3D;addAll&#x3D;&#x3D;加上全部的结果</li><li>先加根</li><li>加遍历的全部左子树</li><li>加遍历的全部右子树</li></ul><h3 id="Java-ArrayList-addAll-方法"><a href="#Java-ArrayList-addAll-方法" class="headerlink" title="Java ArrayList addAll() 方法"></a>Java ArrayList addAll() 方法</h3><p>addAll() 方法将<strong>给定集合中的所有元素</strong>添加到 arraylist 中。</p><p>无论 ArrayList 还是 LinkedList，<code>addAll</code> 方法的复杂度都是 O(N)</p><h1 id="二叉树题目通用思考过程"><a href="#二叉树题目通用思考过程" class="headerlink" title="二叉树题目通用思考过程"></a>二叉树题目通用思考过程</h1><ul><li><strong>通过遍历一遍二叉树得到答案？</strong>【遍历法】<ul><li><u><code>traverse</code> 函数配合外部变量</u></li></ul></li><li><strong>定义一个递归函数，通过子问题（子树）的答案推导出原问题的答案</strong>？【分解法】<ul><li><u>递归函数的定义+利用函数返回值</u></li></ul></li></ul><h1 id="后序位置的特殊之处"><a href="#后序位置的特殊之处" class="headerlink" title="后序位置的特殊之处"></a>后序位置的特殊之处</h1><p>中序位置主要用在 BST 场景中</p><ul><li>可以把BST 的中序遍历认为是<strong>遍历有序数组</strong></li></ul><p>&#x3D;&#x3D;前序位置的代码执行是自顶向下的&#x3D;&#x3D;</p><p>&#x3D;&#x3D;后序位置的代码执行是自底向上的&#x3D;&#x3D;</p><p><img src="/../../themes/fluid/source/img/content/image-20220321103727052.png" alt="image-20220321103727052"></p><p>前序位置是刚刚进入节点的时刻，后序位置是即将离开节点的时刻</p><h1 id="543-二叉树的直径"><a href="#543-二叉树的直径" class="headerlink" title="543. 二叉树的直径"></a><a href="https://leetcode-cn.com/problems/diameter-of-binary-tree/">543. 二叉树的直径</a></h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * struct TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode *left;</span><br><span class="hljs-comment"> *     TreeNode *right;</span><br><span class="hljs-comment"> *     TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125;</span><br><span class="hljs-comment"> * &#125;;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-comment">// 全局变量，记录最大长度</span><br>    <span class="hljs-type">int</span> maxlength=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">// 某条树的最大深度</span><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">maxDepth</span><span class="hljs-params">(TreeNode* root)</span></span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">nullptr</span>)&#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        &#125;<br>        <span class="hljs-type">int</span> leftDepth=<span class="hljs-built_in">maxDepth</span>(root-&gt;left);<span class="hljs-comment">//左子树最大深度</span><br>        <span class="hljs-type">int</span> rightDepth=<span class="hljs-built_in">maxDepth</span>(root-&gt;right);<span class="hljs-comment">//右子树最大深度</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(leftDepth,rightDepth)+<span class="hljs-number">1</span>;<span class="hljs-comment">//返回某一条树的最深子树（加了根）</span><br>    &#125;<br>    <span class="hljs-comment">// 某个节点的情况</span><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">diameterOfBinaryTree</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>       <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">nullptr</span>)&#123;<br>           <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>       &#125;<br>       <span class="hljs-type">int</span> left=<span class="hljs-built_in">maxDepth</span>(root-&gt;left);<span class="hljs-comment">//左边路径长度</span><br>       <span class="hljs-type">int</span> right=<span class="hljs-built_in">maxDepth</span>(root-&gt;right);<span class="hljs-comment">//右边路径长度</span><br>       <span class="hljs-keyword">if</span>(left+right&gt;maxlength)&#123;<span class="hljs-comment">//如果直径最大</span><br>            maxlength=left+right;<span class="hljs-comment">//更新</span><br>       &#125;<br>        <span class="hljs-comment">//遍历每个节点</span><br>        <span class="hljs-built_in">diameterOfBinaryTree</span>(root-&gt;left);<br>        <span class="hljs-built_in">diameterOfBinaryTree</span>(root-&gt;right);<br>        <span class="hljs-keyword">return</span> maxlength;<br>    &#125;<br>   <br>&#125;;<br></code></pre></td></tr></table></figure><p><code>这条路径可能穿过也可能不穿过根结点：</code></p><ul><li>不是从根节点开始<ul><li>左边很少，右边很多，右边某个节点下有很多左右节点时，最大直径不会经过根节点</li></ul></li><li>max全局变量：<ul><li>主函数对某个root调用左右子函数求两边的深度</li><li>主函数将左右深度相加并判断是否为最大，并更新</li><li>主函数递归其他root</li></ul></li></ul><h2 id="优化版【后序】"><a href="#优化版【后序】" class="headerlink" title="优化版【后序】"></a>优化版【后序】</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 记录最大直径的长度</span><br><span class="hljs-type">int</span> <span class="hljs-variable">maxDiameter</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">diameterOfBinaryTree</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>    maxDepth(root);<br>    <span class="hljs-keyword">return</span> maxDiameter;<br>&#125;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">maxDepth</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>    <span class="hljs-keyword">if</span> (root == <span class="hljs-literal">null</span>) &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-type">int</span> <span class="hljs-variable">leftMax</span> <span class="hljs-operator">=</span> maxDepth(root.left);<br>    <span class="hljs-type">int</span> <span class="hljs-variable">rightMax</span> <span class="hljs-operator">=</span> maxDepth(root.right);<br>    <span class="hljs-comment">// 后序位置，顺便计算最大直径</span><br>    <span class="hljs-type">int</span> <span class="hljs-variable">myDiameter</span> <span class="hljs-operator">=</span> leftMax + rightMax;<br>    maxDiameter = Math.max(maxDiameter, myDiameter);<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> + Math.max(leftMax, rightMax);<br>&#125;<br><br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">int</span> maxDiameter=<span class="hljs-number">0</span>;<span class="hljs-comment">//最大直径=0</span><br>     <span class="hljs-comment">// 计算最大深度，顺便计算直径</span><br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">maxDepth</span><span class="hljs-params">(TreeNode* root)</span></span>&#123;<br>        <span class="hljs-keyword">if</span>(root==<span class="hljs-literal">nullptr</span>)&#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        &#125;<br>        <span class="hljs-type">int</span> leftMax=<span class="hljs-built_in">maxDepth</span>(root-&gt;left);<br>        <span class="hljs-type">int</span> rightMax=<span class="hljs-built_in">maxDepth</span>(root-&gt;right);<br>        <span class="hljs-comment">//后序位置，已知各深度</span><br>        <span class="hljs-type">int</span> myDiameter=leftMax+rightMax;<span class="hljs-comment">//本节点的直径</span><br>        maxDiameter=<span class="hljs-built_in">max</span>(myDiameter,maxDiameter);<span class="hljs-comment">//记录最大直径</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(leftMax,rightMax)+<span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">diameterOfBinaryTree</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <span class="hljs-comment">// 遍历每个节点</span><br>        <span class="hljs-built_in">maxDepth</span>(root);<br>        <span class="hljs-keyword">return</span> maxDiameter;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p>给函数设置返回值，然后在后序位置做文章</p><h1 id="二叉树的重要性"><a href="#二叉树的重要性" class="headerlink" title="二叉树的重要性"></a>二叉树的重要性</h1><blockquote><p><strong>快速排序就是个二叉树的前序遍历，归并排序就是个二叉树的后序遍历</strong></p><p>？？？</p></blockquote><h2 id="快排nums-lo-hi"><a href="#快排nums-lo-hi" class="headerlink" title="快排nums[lo..hi]"></a>快排nums[lo..hi]</h2><ul><li>先找一个分界点p</li><li>交换p的左右元素使得 左&lt;p&lt;右</li><li>递归处理左和右</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">void</span> <span class="hljs-title function_">sort</span><span class="hljs-params">(<span class="hljs-type">int</span>[] nums, <span class="hljs-type">int</span> lo, <span class="hljs-type">int</span> hi)</span> &#123;<br>    <span class="hljs-comment">/****** 前序遍历位置 ******/</span><br>    <span class="hljs-comment">// 通过交换元素构建分界点 p</span><br>    <span class="hljs-type">int</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> partition(nums, lo, hi);<br>    <span class="hljs-comment">/************************/</span><br><br>    sort(nums, lo, p - <span class="hljs-number">1</span>);<br>    sort(nums, p + <span class="hljs-number">1</span>, hi);<br>&#125;<br></code></pre></td></tr></table></figure><p>分界点—》左—-》右</p><p>类似：</p><p>根—》左—》右【前序】</p><h2 id="归并排序nums-lo-hi"><a href="#归并排序nums-lo-hi" class="headerlink" title="归并排序nums[lo..hi]"></a>归并排序nums[lo..hi]</h2><ul><li><p>找到中界mid</p></li><li><p>先对 <code>nums[lo..mid]</code> 排序</p></li><li><p>再对 <code>nums[mid+1..hi]</code> 排序</p></li><li><p>把这两个有序的子数组<strong>合并</strong></p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 定义：排序 nums[lo..hi]</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">sort</span><span class="hljs-params">(<span class="hljs-type">int</span>[] nums, <span class="hljs-type">int</span> lo, <span class="hljs-type">int</span> hi)</span> &#123;<br>    <span class="hljs-type">int</span> <span class="hljs-variable">mid</span> <span class="hljs-operator">=</span> (lo + hi) / <span class="hljs-number">2</span>;<br>    <span class="hljs-comment">// 排序 nums[lo..mid]</span><br>    sort(nums, lo, mid);<br>    <span class="hljs-comment">// 排序 nums[mid+1..hi]</span><br>    sort(nums, mid + <span class="hljs-number">1</span>, hi);<br><br>    <span class="hljs-comment">/****** 后序位置 ******/</span><br>    <span class="hljs-comment">// 合并 nums[lo..mid] 和 nums[mid+1..hi]</span><br>    merge(nums, lo, mid, hi);<br>    <span class="hljs-comment">/*********************/</span><br>&#125;<br></code></pre></td></tr></table></figure><p>左—》右—》合并</p><p>类似：</p><p>左—》右—》中【后序】</p><h1 id="深入理解前中后序"><a href="#深入理解前中后序" class="headerlink" title="深入理解前中后序"></a>深入理解前中后序</h1><h2 id="后序遍历有什么特殊之处？"><a href="#后序遍历有什么特殊之处？" class="headerlink" title="后序遍历有什么特殊之处？"></a>后序遍历有什么特殊之处？</h2><h2 id="为什么多叉树没有中序遍历？"><a href="#为什么多叉树没有中序遍历？" class="headerlink" title="为什么多叉树没有中序遍历？"></a>为什么多叉树没有中序遍历？</h2><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="迭代-x2F-递归遍历数组与链表"><a href="#迭代-x2F-递归遍历数组与链表" class="headerlink" title="迭代&#x2F;递归遍历数组与链表"></a>迭代&#x2F;递归遍历数组与链表</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/* 迭代遍历数组 */</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(<span class="hljs-type">int</span>[] arr)</span> &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; arr.length; i++) &#123;<br><br>    &#125;<br>&#125;<br><br><span class="hljs-comment">/* 递归遍历数组 */</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(<span class="hljs-type">int</span>[] arr, <span class="hljs-type">int</span> i)</span> &#123;<br>    <span class="hljs-keyword">if</span> (i == arr.length) &#123;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">// 前序位置</span><br>    traverse(arr, i + <span class="hljs-number">1</span>);<br>    <span class="hljs-comment">// 后序位置</span><br>&#125;<br><br><span class="hljs-comment">/* 迭代遍历单链表 */</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(ListNode head)</span> &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">ListNode</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> head; p != <span class="hljs-literal">null</span>; p = p.next) &#123;<br><br>    &#125;<br>&#125;<br><br><span class="hljs-comment">/* 递归遍历单链表 */</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(ListNode head)</span> &#123;<br>    <span class="hljs-keyword">if</span> (head == <span class="hljs-literal">null</span>) &#123;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">// 前序位置</span><br>    traverse(head.next);<br>    <span class="hljs-comment">// 后序位置</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="【前序】打印每一个节点所在的层数"><a href="#【前序】打印每一个节点所在的层数" class="headerlink" title="【前序】打印每一个节点所在的层数"></a>【前序】打印每一个节点所在的层数</h1><ul><li>traverse传参level</li><li>前序位置打印level</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 二叉树遍历函数</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">traverse</span><span class="hljs-params">(TreeNode root, <span class="hljs-type">int</span> level)</span> &#123;<br>    <span class="hljs-keyword">if</span> (root == <span class="hljs-literal">null</span>) &#123;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">// 前序位置</span><br>    printf(<span class="hljs-string">&quot;节点 %s 在第 %d 层&quot;</span>, root, level);<br>    traverse(root.left, level + <span class="hljs-number">1</span>);<br>    traverse(root.right, level + <span class="hljs-number">1</span>);<br>&#125;<br><br><span class="hljs-comment">// 这样调用</span><br>traverse(root, <span class="hljs-number">1</span>);<br><br></code></pre></td></tr></table></figure><p><strong>从根节点遍历过来的过程就能顺带记录</strong>–》前序</p><h1 id="【后序】每个节点的左右子树各有多少节点"><a href="#【后序】每个节点的左右子树各有多少节点" class="headerlink" title="【后序】每个节点的左右子树各有多少节点"></a>【后序】每个节点的左右子树各有多少节点</h1><ul><li>count分别遍历左右子树并赋值</li><li>后序位置打印上面的赋值</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 定义：输入一棵二叉树，返回这棵二叉树的节点总数</span><br><span class="hljs-type">int</span> <span class="hljs-title function_">count</span><span class="hljs-params">(TreeNode root)</span> &#123;<br>    <span class="hljs-keyword">if</span> (root == <span class="hljs-literal">null</span>) &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    &#125;<br>    <span class="hljs-type">int</span> <span class="hljs-variable">leftCount</span> <span class="hljs-operator">=</span> count(root.left);<br>    <span class="hljs-type">int</span> <span class="hljs-variable">rightCount</span> <span class="hljs-operator">=</span> count(root.right);<br>    <span class="hljs-comment">// 后序位置</span><br>    printf(<span class="hljs-string">&quot;节点 %s 的左子树有 %d 个节点，右子树有 %d 个节点&quot;</span>,<br>            root, leftCount, rightCount);<br><br>    <span class="hljs-keyword">return</span> leftCount + rightCount + <span class="hljs-number">1</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><p><strong>遍历完子树之后才能数清楚</strong>—》后序（只有后序位置才能<u>通过返回值</u><u>获取子树的信息</u>）</p><h1 id="【后序】"><a href="#【后序】" class="headerlink" title="【后序】"></a>【后序】</h1><p><strong>一旦你发现题目<u>和子树有关</u>，那大概率要给函数<u>设置合理的定义和返回值</u>，在<u>后序位置</u>写代码了</strong></p><p><img src="/../../themes/fluid/source/img/content/image-20220421153657541.png" alt="image-20220421153657541"></p>]]></content>
    
    
    
    <tags>
      
      <tag>leetcode刷题记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自我介绍</title>
    <link href="/2022/04/16/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/"/>
    <url>/2022/04/16/%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p class="note note-primary">标签</p><p>12333</p><div class="note note-success">            <p>文字 或者 <code>markdown</code> 均可</p>          </div>            <input type="checkbox" disabled checked="checked">text          ]]></content>
    
    
    
    <tags>
      
      <tag>profile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/04/16/hello-world/"/>
    <url>/2022/04/16/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
